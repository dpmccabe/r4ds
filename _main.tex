\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={R for Data Science},
            pdfauthor={Garrett Grolemund; Hadley Wickham},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{R for Data Science}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Garrett Grolemund \\ Hadley Wickham}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \date{}
  \predate{}\postdate{}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Welcome}\label{welcome}
\addcontentsline{toc}{chapter}{Welcome}

This is the website for \textbf{``R for Data Science''}. This book will
teach you how to do data science with R: You'll learn how to get your
data into R, get it into the most useful structure, transform it,
visualise it and model it. In this book, you will find a practicum of
skills for data science. Just as a chemist learns how to clean test
tubes and stock a lab, you'll learn how to clean data and draw
plots---and many other things besides. These are the skills that allow
data science to happen, and here you will find the best practices for
doing each of these things with R. You'll learn how to use the grammar
of graphics, literate programming, and reproducible research to save
time. You'll also learn how to manage cognitive resources to facilitate
discoveries when wrangling, visualising, and exploring data.

To be published by O'Reilly in late 2016. Pre-order from
\href{http://amzn.to/2aHLAQ1}{amazon}.

(R for Data Science was formerly called Data Science with R in Hands-On
Programming with R)

This work is licensed under the
\href{http://creativecommons.org/licenses/by-nc-nd/3.0/us/}{Creative
Commons Attribution-NonCommercial-NoDerivs 3.0} United States License.

\chapter{Introduction}\label{introduction}

Data science is an exciting discipline that allows you to turn raw data
into understanding, insight, and knowledge. The goal of ``R for Data
Science'' is to help you learn the most important tools in R that will
allow you to do data science. After reading this book, you'll have the
tools to tackle a wide variety of data science challenges, using the
best parts of R.

\section{What you will learn}\label{what-you-will-learn}

Data science is a huge field, and there's no way you can master it by
reading a single book. The goal of this book is to give you a solid
foundation in the most important tools. Our model of the tools needed in
a typical data science project looks something like this:

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/data-science} \end{center}

First you must \textbf{import} your data into R. This typically means
that you take data stored in a file, database, or web API, and load it
into a data frame in R. If you can't get your data into R, you can't do
data science on it!

Once you've imported your data, it is a good idea to \textbf{tidy} it.
Tidying your data means storing it in a consistent form that matches the
semantics of the dataset with the way it is stored. In brief, when your
data is tidy, each column is a variable, and each row is an observation.
Tidy data is important because the consistent structure lets you focus
your struggle on questions about the data, not fighting to get the data
into the right form for different functions.

Once you have tidy data, a common first step is to \textbf{transform}
it. Transformation includes narrowing in on observations of interest
(like all people in one city, or all data from the last year), creating
new variables that are functions of existing variables (like computing
velocity from speed and time), and calculating a set of summary
statistics (like counts or means). Together, tidying and transforming
are called \textbf{wrangling}, because getting your data in a form
that's natural to work with often feels like a fight!

Once you have tidy data with the variables you need, there are two main
engines of knowledge generation: visualisation and modelling. These have
complementary strengths and weaknesses so any real analysis will iterate
between them many times.

\textbf{Visualisation} is a fundamentally human activity. A good
visualisation will show you things that you did not expect, or raise new
questions about the data. A good visualisation might also hint that
you're asking the wrong question, or you need to collect different data.
Visualisations can surprise you, but don't scale particularly well
because they require a human to interpret them.

\textbf{Models} are complementary tools to visualisation. Once you have
made your questions sufficiently precise, you can use a model to answer
them. Models are a fundamentally mathematical or computational tool, so
they generally scale well. Even when they don't, it's usually cheaper to
buy more computers than it is to buy more brains! But every model makes
assumptions, and by its very nature a model cannot question its own
assumptions. That means a model cannot fundamentally surprise you.

The last step of data science is \textbf{communication}, an absolutely
critical part of any data analysis project. It doesn't matter how well
your models and visualisation have led you to understand the data unless
you can also communicate your results to others.

Surrounding all these tools is \textbf{programming}. Programming is a
cross-cutting tool that you use in every part of the project. You don't
need to be an expert programmer to be a data scientist, but learning
more about programming pays off because becoming a better programmer
allows you to automate common tasks, and solve new problems with greater
ease.

You'll use these tools in every data science project, but for most
projects they're not enough. There's a rough 80-20 rule at play; you can
tackle about 80\% of every project using the tools that you'll learn in
this book, but you'll need other tools to tackle the remaining 20\%.
Throughout this book we'll point you to resources where you can learn
more.

\section{How this book is organised}\label{how-this-book-is-organised}

The previous description of the tools of data science is organised
roughly according to the order in which you use them in an analysis
(although of course you'll iterate through them multiple times). In our
experience, however, this is not the best way to learn them:

\begin{itemize}
\item
  Starting with data ingest and tidying is sub-optimal because 80\% of
  the time it's routine and boring, and the other 20\% of the time it's
  weird and frustrating. That's a bad place to start learning a new
  subject! Instead, we'll start with visualisation and transformation of
  data that's already been imported and tidied. That way, when you
  ingest and tidy your own data, your motivation will stay high because
  you know the pain is worth it.
\item
  Some topics are best explained with other tools. For example, we
  believe that it's easier to understand how models work if you already
  know about visualisation, tidy data, and programming.
\item
  Programming tools are not necessarily interesting in their own right,
  but do allow you to tackle considerably more challenging problems.
  We'll give you a selection of programming tools in the middle of the
  book, and then you'll see they can combine with the data science tools
  to tackle interesting modelling problems.
\end{itemize}

Within each chapter, we try and stick to a similar pattern: start with
some motivating examples so you can see the bigger picture, and then
dive into the details. Each section of the book is paired with exercises
to help you practice what you've learned. While it's tempting to skip
the exercises, there's no better way to learn than practicing on real
problems.

\section{What you won't learn}\label{what-you-wont-learn}

There are some important topics that this book doesn't cover. We believe
it's important to stay ruthlessly focused on the essentials so you can
get up and running as quickly as possible. That means this book can't
cover every important topic.

\subsection{Big data}\label{big-data}

This book proudly focuses on small, in-memory datasets. This is the
right place to start because you can't tackle big data unless you have
experience with small data. The tools you learn in this book will easily
handle hundreds of megabytes of data, and with a little care you can
typically use them to work with 1-2 Gb of data. If you're routinely
working with larger data (10-100 Gb, say), you should learn more about
\href{https://github.com/Rdatatable/data.table}{data.table}. This book
doesn't teach data.table because it has a very concise interface which
makes it harder to learn since it offers fewer linguistic cues. But if
you're working with large data, the performance payoff is worth the
extra effort required to learn it.

If your data is bigger than this, carefully consider if your big data
problem might actually be a small data problem in disguise. While the
complete data might be big, often the data needed to answer a specific
question is small. You might be able to find a subset, subsample, or
summary that fits in memory and still allows you to answer the question
that you're interested in. The challenge here is finding the right small
data, which often requires a lot of iteration.

Another possibility is that your big data problem is actually a large
number of small data problems. Each individual problem might fit in
memory, but you have millions of them. For example, you might want to
fit a model to each person in your dataset. That would be trivial if you
had just 10 or 100 people, but instead you have a million. Fortunately
each problem is independent of the others (a setup that is sometimes
called embarrassingly parallel), so you just need a system (like Hadoop
or Spark) that allows you to send different datasets to different
computers for processing. Once you've figured out how to answer the
question for a single subset using the tools described in this book, you
learn new tools like sparklyr, rhipe, and ddr to solve it for the full
dataset.

\subsection{Python, Julia, and friends}\label{python-julia-and-friends}

In this book, you won't learn anything about Python, Julia, or any other
programming language useful for data science. This isn't because we
think these tools are bad. They're not! And in practice, most data
science teams use a mix of languages, often at least R and Python.

However, we strongly believe that it's best to master one tool at a
time. You will get better faster if you dive deep, rather than spreading
yourself thinly over many topics. This doesn't mean you should only know
one thing, just that you'll generally learn faster if you stick to one
thing at a time. You should strive to learn new things throughout your
career, but make sure your understanding is solid before you move on to
the next interesting thing.

We think R is a great place to start your data science journey because
it is an environment designed from the ground up to support data
science. R is not just a programming language, but it is also an
interactive environment for doing data science. To support interaction,
R is a much more flexible language than many of its peers. This
flexibility comes with its downsides, but the the big upside is how easy
it is to evolve tailored grammars for specific parts of the data science
process. These mini languages help you think about problems as a data
scientist, while supporting fluent interaction between your brain and
the computer.

\subsection{Non-rectangular data}\label{non-rectangular-data}

This book focuses exclusively on rectangular data: collections of values
that are each associated with a variable and an observation. There are
lots of datasets that do not naturally fit in this paradigm: including
images, sounds, trees, and text. But rectangular data frames are
extremely common in science and industry, and we believe that they're a
great place to start your data science journey.

\subsection{Hypothesis confirmation}\label{hypothesis-confirmation}

It's possible to divide data analysis into two camps: hypothesis
generation and hypothesis confirmation (sometimes called confirmatory
analysis). The focus of this book is unabashedly on hypothesis
generation, or data exploration. Here you'll look deeply at the data
and, in combination with your subject knowledge, generate many
interesting hypotheses to help explain why the data behaves the way it
does. You evaluate the hypotheses informally, using your scepticism to
challenge the data in multiple ways.

The complement of hypothesis generation is hypothesis confirmation.
Hypothesis confirmation is hard for two reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You need a precise mathematical model in order to generate falsifiable
  predictions. This often requires considerable statistical
  sophistication.
\item
  You can only use an observation once to confirm a hypothesis. As soon
  as you use it more than once you're back to doing exploratory
  analysis. This means to do hypothesis confirmation you need to
  ``preregister'' (write out in advance) your analysis plan, and not
  deviate from it even when you have seen the data. We'll talk a little
  about some strategies you can use to make this easier in
  \protect\hyperlink{model-intro}{modelling}.
\end{enumerate}

It's common to think about modelling as a tool for hypothesis
confirmation, and visualisation as a tool for hypothesis generation. But
that's a false dichotomy: models are often used for exploration, and
with a little care you can use visualisation for confirmation. The key
difference is how often do you look at each observation: if you look
only once, it's confirmation; if you look more than once, it's
exploration.

\section{Prerequisites}\label{prerequisites}

We've made a few assumptions about what you already know in order to get
the most out of this book. You should be generally numerically literate,
and it's helpful if you have some programming experience already. If
you've never programmed before, you might find
\href{http://amzn.com/1449359019}{Hands on Programming with R} by
Garrett to be a useful adjunct to this book.

There are four things you need to run the code in this book: R, RStudio,
a collection of R packages called the \textbf{tidyverse}, and a handful
of other packages. Packages are the fundamental units of reproducible R
code. They include reusable functions, the documentation that describes
how to use them, and sample data.

\subsection{R}\label{r}

To download R, go to CRAN, the \textbf{c}omprehensive \textbf{R}
\textbf{a}rchive \textbf{n}etwork. CRAN is composed of a set of mirror
servers distributed around the world and is used to distribute R and R
packages. Don't try and pick a mirror that's close to you: instead use
the cloud mirror, \url{https://cloud.r-project.org}, which automatically
figures it out for you.

A new major version of R comes out once a year, and there are 2-3 minor
releases each year. It's a good idea to update regularly. Upgrading can
be a bit of a hassle, especially for major versions, which require you
to reinstall all your packages, but putting it off only make it worse.

\subsection{RStudio}\label{rstudio}

RStudio is an integrated development environment, or IDE, for R
programming. Download and install it from
\url{http://www.rstudio.com/download}. RStudio is updated a couple of
times a year. When a new version is available, RStudio will let you
know. It's a good idea to upgrade regularly so you can take advantage of
the latest and greatest features. For this book, make sure you have
RStudio 1.0.0.

When you start RStudio, you'll see two key regions in the interface:

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/rstudio-console} \end{center}

For now, all you need to know is that you type R code in the console
pane, and press enter to run it. You'll learn more as we go along!

\subsection{The tidyverse}\label{the-tidyverse}

You'll also need to install some R packages. An R \emph{package} is a
collection of functions, data, and documentation that extends the
capabilities of base R. Using packages is key to the successful use of
R. The majority of the packages that you will learn in this book are
part of the so-called tidyverse. The packages in the tidyverse share a
common philosophy of data and R programming, and are designed to work
together naturally.

You can install the complete tidyverse with a single line of code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

On your own computer, type that line of code in the console, and then
press enter to run it. R will download the packages from CRAN and
install them on to your computer. If you have problems installing, make
sure that you are connected to the internet, and that
\url{https://cloud.r-project.org/} isn't blocked by your firewall or
proxy.

You will not be able to use the functions, objects, and help files in a
package until you load it with \texttt{library()}. Once you have
installed a package, you can load it with the \texttt{library()}
function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{#> Loading tidyverse: ggplot2}
\CommentTok{#> Loading tidyverse: tibble}
\CommentTok{#> Loading tidyverse: tidyr}
\CommentTok{#> Loading tidyverse: readr}
\CommentTok{#> Loading tidyverse: purrr}
\CommentTok{#> Loading tidyverse: dplyr}
\CommentTok{#> Conflicts with tidy packages ----------------------------------------------}
\CommentTok{#> filter(): dplyr, stats}
\CommentTok{#> lag():    dplyr, stats}
\end{Highlighting}
\end{Shaded}

This tells you that tidyverse is loading the ggplot2, tibble, tidyr,
readr, purrr, and dplyr packages. These are considered to be the
\textbf{core} of the tidyverse because you'll use them in almost every
analysis.

Packages in the tidyverse change fairly frequently. You can see if
updates are available, and optionally install them, by running
\texttt{tidyverse\_update()}.

\subsection{Other packages}\label{other-packages}

There are many other excellent packages that are not part of the
tidyverse, because they solve problems in a different domain, or are
designed with a different set of underlying principles. This doesn't
make them better or worse, just different. In other words, the
complement to the tidyverse is not the messyverse, but many other
universes of interrelated packages. As you tackle more data science
projects with R, you'll learn new packages and new ways of thinking
about data.

In this book we'll use three data packages from outside the tidyverse:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"nycflights13"}\NormalTok{, }\StringTok{"gapminder"}\NormalTok{, }\StringTok{"Lahman"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

These packages provide data on airline flights, world development, and
baseball that we'll use to illustrate key data science ideas.

\section{Running R code}\label{running-r-code}

The previous section showed you a couple of examples of running R code.
Code in the book looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \NormalTok{+}\StringTok{ }\DecValTok{2}
\CommentTok{#> [1] 3}
\CommentTok{#> [1] 3}
\end{Highlighting}
\end{Shaded}

If you run the same code in your local console, it will look like this:

\begin{verbatim}
> 1 + 2
[1] 3
\end{verbatim}

There are two main differences. In your console, you type after the
\texttt{\textgreater{}}, called the \textbf{prompt}; we don't show the
prompt in the book. In the book, output is commented out with
\texttt{\#\textgreater{}}; in your console it appears directly after
your code. These two differences mean that if you're working with an
electronic version of the book, you can easily copy code out of the book
and into the console.

Throughout the book we use a consistent set of conventions to refer to
code:

\begin{itemize}
\item
  Functions are in a code font and followed by parentheses, like
  \texttt{sum()}, or \texttt{mean()}.
\item
  Other R objects (like data or function arguments) are in a code font,
  without parentheses, like \texttt{flights} or \texttt{x}.
\item
  If we want to make it clear what package an object comes from, we'll
  use the package name followed by two colons, like
  \texttt{dplyr::mutate()}, or\\
  \texttt{nycflights13::flights}. This is also valid R code.
\end{itemize}

\section{Getting help and learning
more}\label{getting-help-and-learning-more}

This book is not an island; there is no single resource that will allow
you to master R. As you start to apply the techniques described in this
book to your own data you will soon find questions that I do not answer.
This section describes a few tips on how to get help, and to help you
keep learning.

If you get stuck, start with Google. Typically adding ``R'' to a query
is enough to restrict it to relevant results: if the search isn't
useful, it often means that there aren't any R-specific results
available. Google is particularly useful for error messages. If you get
an error message and you have no idea what it means, try googling it!
Chances are that someone else has been confused by it in the past, and
there will be help somewhere on the web. (If the error message isn't in
English, run \texttt{Sys.setenv(LANGUAGE\ =\ "en")} and re-run the code;
you're more likely to find help for English error messages.)

If Google doesn't help, try
\href{http://stackoverflow.com}{stackoverflow}. Start by spending a
little time searching for an existing answer, including \texttt{{[}R{]}}
restrict your search to questions and answers that use R. If you don't
find anything useful, prepare a minimal reproducible example or
\textbf{reprex}. A good reprex makes it easier for other people to help
you, and often you'll figure out the problem yourself in the course of
making it.

There are three things you need to include to make your example
reproducible: required packages, data, and code.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Packages} should be loaded at the top of the script, so it's
  easy to see which ones the example needs. This is a good time to check
  that you're using the latest version of each package; it's possible
  you've discovered a bug that's been fixed since you installed the
  package. For packages in the tidyverse, the easiest way to check is to
  run \texttt{tidyverse\_update()}.
\item
  The easiest way to include \textbf{data} in a question is to use
  \texttt{dput()} to generate the R code to recreate it. For example, to
  recreate the \texttt{mtcars} dataset in R, I'd perform the following
  steps:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Run \texttt{dput(mtcars)} in R
  \item
    Copy the output
  \item
    In my reproducible script, type \texttt{mtcars\ \textless{}-} then
    paste.
  \end{enumerate}

  Try and find the smallest subset of your data that still reveals the
  problem.
\item
  Spend a little bit of time ensuring that your \textbf{code} is easy
  for others to read:

  \begin{itemize}
  \item
    Make sure you've used spaces and your variable names are concise,
    yet informative.
  \item
    Use comments to indicate where your problem lies.
  \item
    Do your best to remove everything that is not related to the
    problem.\\
    The shorter your code is, the easier it is to understand, and the
    easier it is to fix.
  \end{itemize}
\end{enumerate}

Finish by checking that you have actually made a reproducible example by
starting a fresh R session and copying and pasting your script in.

You should also spend some time preparing yourself to solve problems
before they occur. Investing a little time in learning R each day will
pay off handsomely in the long run. One way is to follow what Hadley,
Garrett, and everyone else at RStudio are doing on the
\href{https://blog.rstudio.org}{RStudio blog}. This is where we post
announcements about new packages, new IDE features, and in-person
courses. You might also want to follow Hadley
(\href{https://twitter.com/hadleywickham}{@hadleywickham}) or Garrett
(\href{https://twitter.com/statgarrett}{@statgarrett}) on Twitter, or
follow \href{https://twitter.com/rstudiotips}{@rstudiotips} to keep up
with new features in the IDE.

To keep up with the R community more broadly, we recommend reading
\url{http://www.r-bloggers.com}: it aggregates over 500 blogs about R
from around the world. If you're an active Twitter user, follow the
\texttt{\#rstats} hashtag. Twitter is one of the key tools that Hadley
uses to keep up with new developments in the community.

\section{Acknowledgements}\label{acknowledgements}

This book isn't just the product of Hadley and Garrett, but is the
result of many conversations (in person and online) that we've had with
the many people in the R community. There are a few people we'd like to
thank in particular, because they have spent many hours answering our
dumb questions and helping us to better think about data science:

\begin{itemize}
\item
  Jenny Bryan and Lionel Henry for many helpful discussions around
  working with lists and list-columns.
\item
  The three chapters on workflow were adapted (with permission), from
  \url{http://stat545.com/block002_hello-r-workspace-wd-project.html} by
  Jenny Bryan.
\item
  Genevera Allen for discussions about models, modelling, the
  statistical learning perspective, and the difference between
  hypothesis generation and hypothesis confirmation.
\item
  Yihui Xie for his work on the
  \href{https://github.com/rstudio/bookdown}{bookdown} package, and for
  tirelessly responding to my feature requests.
\item
  Bill Behrman for his thoughtful reading of the entire book, and for
  trying it out with his data science class at Stanford.
\item
  The \#rstats twitter community who reviewed all of the draft chapters
  and provided tons of useful feedback.
\item
  Tal Galili for augmenting his dendextend package to support a section
  on clustering that did not make it into the final draft.
\end{itemize}

This book was written in the open, and many people contributed pull
requests to fix minor problems. Special thanks goes to everyone who
contributed via GitHub:

Thanks go to all contributers in alphabetical order: adi pradhan, Ahmed
ElGabbas, Ajay Deonarine, @Alex, Andrew Landgraf, @batpigandme,
@behrman, Ben Marwick, Bill Behrman, Brandon Greenwell, Brett Klamer,
Christian G. Warden, Christian Mongeau, Colin Gillespie, Cooper Morris,
Curtis Alexander, Daniel Gromer, David Clark, Derwin McGeary, Devin
Pastoor, Dylan Cashman, Earl Brown, Eric Watt, Etienne B. Racine,
Flemming Villalona, Gregory Jefferis, @harrismcgehee, Hengni Cai, Ian
Lyttle, Ian Sealy, Jakub Nowosad, Jennifer (Jenny) Bryan, @jennybc,
Jeroen Janssens, Jim Hester, @jjchern, Joanne Jang, John Sears, Jon
Calder, Jonathan Page, @jonathanflint, Julia Stewart Lowndes, Julian
During, Justinas Petuchovas, Kara Woo, @kdpsingh, Kenny Darrell, Kirill
Sevastyanenko, @koalabearski, @KyleHumphrey, Lawrence Wu, Matthew
Sedaghatfar, Mine Cetinkaya-Rundel, @MJMarshall, Mustafa Ascha,
@nate-d-olson, Nelson Areal, Nick Clark, @nickelas, @nwaff, @OaCantona,
Patrick Kennedy, Peter Hurford, Rademeyer Vermaak, Radu Grosu,
@rlzijdeman, Robert Schuessler, @robinlovelace, @robinsones, S'busiso
Mkhondwane, @seamus-mckinsey, @seanpwilliams, Shannon Ellis, @shoili,
@sibusiso16, @spirgel, Steve Mortimer, @svenski, Terence Teo, Thomas
Klebel, TJ Mahr, Tom Prior, Will Beasley, Yihui Xie.

\section{Colophon}\label{colophon}

An online version of this book is available at
\url{http://r4ds.had.co.nz}. It will continue to evolve in between
reprints of the physical book. The source of the book is available at
\url{https://github.com/hadley/r4ds}. The book is powered by
\url{https://bookdown.org} which makes it easy to turn R markdown files
into HTML, PDF, and EPUB.

This book was built with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools::}\KeywordTok{session_info}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{))}
\CommentTok{#> Session info --------------------------------------------------------------}
\CommentTok{#>  setting  value                       }
\CommentTok{#>  version  R version 3.3.1 (2016-06-21)}
\CommentTok{#>  system   x86_64, darwin13.4.0        }
\CommentTok{#>  ui       X11                         }
\CommentTok{#>  language (EN)                        }
\CommentTok{#>  collate  en_US.UTF-8                 }
\CommentTok{#>  tz       America/New_York            }
\CommentTok{#>  date     2016-11-09}
\CommentTok{#> Packages ------------------------------------------------------------------}
\CommentTok{#>  package      * version  date       source        }
\CommentTok{#>  assertthat     0.1      2013-12-06 CRAN (R 3.3.0)}
\CommentTok{#>  BH             1.60.0-2 2016-05-07 CRAN (R 3.3.0)}
\CommentTok{#>  broom          0.4.1    2016-06-24 CRAN (R 3.3.0)}
\CommentTok{#>  colorspace     1.2-7    2016-10-11 cran (@1.2-7) }
\CommentTok{#>  curl           2.2      2016-10-21 CRAN (R 3.3.0)}
\CommentTok{#>  DBI            0.5-1    2016-09-10 CRAN (R 3.3.0)}
\CommentTok{#>  dichromat      2.0-0    2013-01-24 CRAN (R 3.3.0)}
\CommentTok{#>  digest         0.6.10   2016-08-02 cran (@0.6.10)}
\CommentTok{#>  dplyr        * 0.5.0    2016-06-24 CRAN (R 3.3.0)}
\CommentTok{#>  forcats        0.1.1    2016-09-16 CRAN (R 3.3.0)}
\CommentTok{#>  foreign        0.8-67   2016-09-13 CRAN (R 3.3.0)}
\CommentTok{#>  ggplot2      * 2.1.0    2016-03-01 CRAN (R 3.3.0)}
\CommentTok{#>  gtable         0.2.0    2016-02-26 CRAN (R 3.3.0)}
\CommentTok{#>  haven          1.0.0    2016-09-23 CRAN (R 3.3.0)}
\CommentTok{#>  hms            0.2      2016-06-17 CRAN (R 3.3.0)}
\CommentTok{#>  httr           1.2.1    2016-07-03 CRAN (R 3.3.0)}
\CommentTok{#>  jsonlite       1.1      2016-09-14 cran (@1.1)   }
\CommentTok{#>  labeling       0.3      2014-08-23 CRAN (R 3.3.0)}
\CommentTok{#>  lattice        0.20-33  2015-07-14 CRAN (R 3.3.1)}
\CommentTok{#>  lazyeval       0.2.0    2016-06-12 CRAN (R 3.3.0)}
\CommentTok{#>  lubridate      1.6.0    2016-09-13 CRAN (R 3.3.0)}
\CommentTok{#>  magrittr       1.5      2014-11-22 CRAN (R 3.3.0)}
\CommentTok{#>  MASS           7.3-45   2016-04-21 CRAN (R 3.3.1)}
\CommentTok{#>  mime           0.5      2016-07-07 CRAN (R 3.3.0)}
\CommentTok{#>  mnormt         1.5-5    2016-10-15 cran (@1.5-5) }
\CommentTok{#>  modelr         0.1.0    2016-08-31 CRAN (R 3.3.0)}
\CommentTok{#>  munsell        0.4.3    2016-02-13 CRAN (R 3.3.0)}
\CommentTok{#>  nlme           3.1-128  2016-05-10 CRAN (R 3.3.0)}
\CommentTok{#>  openssl        0.9.5    2016-10-28 CRAN (R 3.3.0)}
\CommentTok{#>  plyr           1.8.4    2016-06-08 CRAN (R 3.3.0)}
\CommentTok{#>  psych          1.6.9    2016-09-17 CRAN (R 3.3.0)}
\CommentTok{#>  purrr        * 0.2.2    2016-06-18 CRAN (R 3.3.0)}
\CommentTok{#>  R6             2.2.0    2016-10-05 cran (@2.2.0) }
\CommentTok{#>  RColorBrewer   1.1-2    2014-12-07 CRAN (R 3.3.0)}
\CommentTok{#>  Rcpp           0.12.7   2016-09-05 cran (@0.12.7)}
\CommentTok{#>  readr        * 1.0.0    2016-08-03 CRAN (R 3.3.0)}
\CommentTok{#>  readxl         0.1.1    2016-03-28 CRAN (R 3.3.0)}
\CommentTok{#>  reshape2       1.4.2    2016-10-22 cran (@1.4.2) }
\CommentTok{#>  rvest          0.3.2    2016-06-17 CRAN (R 3.3.0)}
\CommentTok{#>  scales         0.4.0    2016-02-26 CRAN (R 3.3.0)}
\CommentTok{#>  selectr        0.3-0    2016-08-30 CRAN (R 3.3.0)}
\CommentTok{#>  stringi        1.1.2    2016-10-01 cran (@1.1.2) }
\CommentTok{#>  stringr        1.1.0    2016-08-19 cran (@1.1.0) }
\CommentTok{#>  tibble       * 1.2      2016-08-26 CRAN (R 3.3.0)}
\CommentTok{#>  tidyr        * 0.6.0    2016-08-12 CRAN (R 3.3.0)}
\CommentTok{#>  tidyverse    * 1.0.0    2016-09-09 CRAN (R 3.3.0)}
\CommentTok{#>  xml2           1.0.0    2016-06-24 CRAN (R 3.3.0)}
\end{Highlighting}
\end{Shaded}

\part{Explore}\label{part-explore}


\chapter{Introduction}\label{explore-intro}

The goal of the first part of this book is to get you up to speed with
the basic tools of \textbf{data exploration} as quickly as possible.
Data exploration is the art of looking at your data, rapidly generating
hypotheses, quickly testing them, then repeating again and again and
again. The goal of data exploration is to generate many promising leads
that you can later explore in more depth.

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/data-science-explore} \end{center}

In this part of the book you will learn some useful tools that have an
immediate payoff:

\begin{itemize}
\item
  Visualisation is a great place to start with R programming, because
  the payoff is so clear: you get to make elegant and informative plots
  that help you understand data. In
  \protect\hyperlink{data-visualisation}{data visualisation} you'll dive
  into visualisation, learning the basic structure of a ggplot2 plot,
  and powerful techniques for turning data into plots.
\item
  Visualisation alone is typically not enough, so in
  \protect\hyperlink{transform}{data transformation} you'll learn the
  key verbs that allow you to select important variables, filter out key
  observations, create new variables, and compute summaries.
\item
  Finally, in \protect\hyperlink{exploratory-data-analysis}{exploratory
  data analysis}, you'll combine visualisation and transformation with
  your curiosity and scepticism to ask and answer interesting questions
  about data.
\end{itemize}

Modelling is an important part of the exploratory process, but you don't
have the skills to effectively learn or apply it yet. We'll come back to
it in \protect\hyperlink{model-intro}{modelling}, once you're better
equipped with more data wrangling and programming tools.

Nestled among these three chapters that teach you the tools of
exploration are three chapters that focus on your R workflow. In
\protect\hyperlink{workflow-basics}{workflow: basics},
\protect\hyperlink{workflow-scripts}{workflow: scripts}, and
\protect\hyperlink{workflow-projects}{workflow: projects} you'll learn
good practices for writing and organising your R code. These will set
you up for success in the long run, as they'll give you the tools to
stay organised when you tackle real projects.

\hypertarget{data-visualisation}{\chapter{Data
visualisation}\label{data-visualisation}}

\section{Introduction}\label{introduction-1}

\begin{quote}
``The simple graph has brought more information to the data analyst's
mind than any other device.'' --- John Tukey
\end{quote}

This chapter will teach you how to visualise your data using ggplot2. R
has several systems for making graphs, but ggplot2 is one of the most
elegant and most versatile. ggplot2 implements the \textbf{grammar of
graphics}, a coherent system for describing and building graphs. With
ggplot2, you can do more faster by learning one system and applying it
in many places.

If you'd like to learn more about the theoretical underpinnings of
ggplot2 before you start, I'd recommend reading ``The Layered Grammar of
Graphics'', \url{http://vita.had.co.nz/papers/layered-grammar.pdf}.

\subsection{Prerequisites}\label{prerequisites-1}

This chapter focusses on ggplot2, one of the core members of the
tidyverse. To access the datasets, help pages, and functions that we
will use in this chapter, load the tidyverse by running this code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{#> Loading tidyverse: ggplot2}
\CommentTok{#> Loading tidyverse: tibble}
\CommentTok{#> Loading tidyverse: tidyr}
\CommentTok{#> Loading tidyverse: readr}
\CommentTok{#> Loading tidyverse: purrr}
\CommentTok{#> Loading tidyverse: dplyr}
\CommentTok{#> Conflicts with tidy packages ----------------------------------------------}
\CommentTok{#> filter(): dplyr, stats}
\CommentTok{#> lag():    dplyr, stats}
\end{Highlighting}
\end{Shaded}

That one line of code loads the core tidyverse; packages which you will
use in almost every data analysis. It also tells you which functions
from the tidyverse conflict with functions in base R (or from other
packages you might have loaded).

If you run this code and get the error message ``there is no package
called `tidyverse'\,'', you'll need to first install it, then run
\texttt{library()} once again.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

You only need to install a package once, but you need to reload it every
time you start a new session.

If we need to be explicit about where a function (or dataset) comes
from, we'll use the special form \texttt{package::function()}. For
example, \texttt{ggplot2::ggplot()} tells you explicitly that we're
using the \texttt{ggplot()} function from the ggplot2 package.

\section{First steps}\label{first-steps}

Let's use our first graph to answer a question: Do cars with big engines
use more fuel than cars with small engines? You probably already have an
answer, but try to make your answer precise. What does the relationship
between engine size and fuel efficiency look like? Is it positive?
Negative? Linear? Nonlinear?

\subsection{\texorpdfstring{The \texttt{mpg} data
frame}{The mpg data frame}}\label{the-mpg-data-frame}

You can test your answer with the \texttt{mpg} \textbf{data frame} found
in ggplot2 (aka \texttt{ggplot2::mpg}). A data frame is a rectangular
collection of variables (in the columns) and observations (in the rows).
\texttt{mpg} contains observations collected by the US Environment
Protection Agency on 38 models of cars.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mpg}
\CommentTok{#> # A tibble: 234 × 11}
\CommentTok{#>   manufacturer model displ  year   cyl      trans   drv   cty   hwy    fl}
\CommentTok{#>          <chr> <chr> <dbl> <int> <int>      <chr> <chr> <int> <int> <chr>}
\CommentTok{#> 1         audi    a4   1.8  1999     4   auto(l5)     f    18    29     p}
\CommentTok{#> 2         audi    a4   1.8  1999     4 manual(m5)     f    21    29     p}
\CommentTok{#> 3         audi    a4   2.0  2008     4 manual(m6)     f    20    31     p}
\CommentTok{#> 4         audi    a4   2.0  2008     4   auto(av)     f    21    30     p}
\CommentTok{#> 5         audi    a4   2.8  1999     6   auto(l5)     f    16    26     p}
\CommentTok{#> 6         audi    a4   2.8  1999     6 manual(m5)     f    18    26     p}
\CommentTok{#> # ... with 228 more rows, and 1 more variables: class <chr>}
\end{Highlighting}
\end{Shaded}

Among the variables in \texttt{mpg} are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{displ}, a car's engine size, in litres.
\item
  \texttt{hwy}, a car's fuel efficiency on the highway, in miles per
  gallon (mpg). A car with a low fuel efficiency consumes more fuel than
  a car with a high fuel efficiency when they travel the same distance.
\end{enumerate}

To learn more about \texttt{mpg}, open its help page by running
\texttt{?mpg}.

\subsection{Creating a ggplot}\label{creating-a-ggplot}

To plot \texttt{mpg}, run this code to put \texttt{displ} on the x-axis
and \texttt{hwy} on the y-axis:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-4-1} \end{center}

The plot shows a negative relationship between engine size
(\texttt{displ}) and fuel efficiency (\texttt{hwy}). In other words,
cars with big engines use more fuel. Does this confirm or refute your
hypothesis about fuel efficiency and engine size?

With ggplot2, you begin a plot with the function \texttt{ggplot()}.
\texttt{ggplot()} creates a coordinate system that you can add layers
to. The first argument of \texttt{ggplot()} is the dataset to use in the
graph. So \texttt{ggplot(data\ =\ mpg)} creates an empty graph, but it's
not very interesting so I'm not going to show it here.

You complete your graph by adding one or more layers to
\texttt{ggplot()}. The function \texttt{geom\_point()} adds a layer of
points to your plot, which creates a scatterplot. ggplot2 comes with
many geom functions that each add a different type of layer to a plot.
You'll learn a whole bunch of them throughout this chapter.

Each geom function in ggplot2 takes a \texttt{mapping} argument. This
defines how variables in your dataset are mapped to visual properties.
The \texttt{mapping} argument is always paired with \texttt{aes()}, and
the \texttt{x} and \texttt{y} arguments of \texttt{aes()} specify which
variables to map to the x and y axes. ggplot2 looks for the mapped
variable in the \texttt{data} argument, in this case, \texttt{mpg}.

\subsection{A graphing template}\label{a-graphing-template}

Let's turn this code into a reusable template for making graphs with
ggplot2. To make a graph, replace the bracketed sections in the code
below with a dataset, a geom function, or a collection of mappings.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{<DATA>) +}\StringTok{ }
\StringTok{  }\ErrorTok{<}\NormalTok{GEOM_FUNCTION>(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(<MAPPINGS>))}
\end{Highlighting}
\end{Shaded}

The rest of this chapter will show you how to complete and extend this
template to make different types of graphs. We will begin with the
\texttt{\textless{}MAPPINGS\textgreater{}} component.

\subsection{Exercises}\label{exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Run \texttt{ggplot(data\ =\ mpg)} what do you see?
\item
  How many rows are in \texttt{mtcars}? How many columns?
\item
  What does the \texttt{drv} variable describe? Read the help for
  \texttt{?mpg} to find out.
\item
  Make a scatterplot of \texttt{hwy} vs \texttt{cyl}.
\item
  What happens if you make a scatterplot of \texttt{class} vs
  \texttt{drv}. Why is the plot not useful?
\end{enumerate}

\section{Aesthetic mappings}\label{aesthetic-mappings}

\begin{quote}
``The greatest value of a picture is when it forces us to notice what we
never expected to see.'' --- John Tukey
\end{quote}

In the plot below, one group of points (highlighted in red) seems to
fall outside of the linear trend. These cars have a higher mileage than
you might expect. How can you explain these cars?

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-6-1} \end{center}

Let's hypothesize that the cars are hybrids. One way to test this
hypothesis is to look at the \texttt{class} value for each car. The
\texttt{class} variable of the \texttt{mpg} dataset classifies cars into
groups such as compact, midsize, and SUV. If the outlying points are
hybrids, they should be classified as compact cars or, perhaps,
subcompact cars (keep in mind that this data was collected before hybrid
trucks and SUVs became popular).

You can add a third variable, like \texttt{class}, to a two dimensional
scatterplot by mapping it to an \textbf{aesthetic}. An aesthetic is a
visual property of the objects in your plot. Aesthetics include things
like the size, the shape, or the color of your points. You can display a
point (like the one below) in different ways by changing the values of
its aesthetic properties. Since we already use the word ``value'' to
describe data, let's use the word ``level'' to describe aesthetic
properties. Here we change the levels of a point's size, shape, and
color to make the point small, triangular, or blue:

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-7-1} \end{center}

You can convey information about your data by mapping the aesthetics in
your plot to the variables in your dataset. For example, you can map the
colors of your points to the \texttt{class} variable to reveal the class
of each car.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{color =} \NormalTok{class))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-8-1} \end{center}

(If you prefer British English, like Hadley, you can use \texttt{colour}
instead of \texttt{color}.)

To map an aesthetic to a variable, associate the name of the aesthetic
to the name of the variable inside \texttt{aes()}. ggplot2 will
automatically assign a unique level of the aesthetic (here a unique
color) to each unique value of the variable, a process known as
\textbf{scaling}. ggplot2 will also add a legend that explains which
levels correspond to which values.

The colors reveal that many of the unusual points are two-seater cars.
These cars don't seem like hybrids, and are, in fact, sports cars!
Sports cars have large engines like SUVs and pickup trucks, but small
bodies like midsize and compact cars, which improves their gas mileage.
In hindsight, these cars were unlikely to be hybrids since they have
large engines.

In the above example, we mapped \texttt{class} to the color aesthetic,
but we could have mapped \texttt{class} to the size aesthetic in the
same way. In this case, the exact size of each point would reveal its
class affiliation. We get a \emph{warning} here, because mapping an
unordered variable (\texttt{class}) to an ordered aesthetic
(\texttt{size}) is not a good idea.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{size =} \NormalTok{class))}
\CommentTok{#> Warning: Using size for a discrete variable is not advised.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-9-1} \end{center}

Or we could have mapped \texttt{class} to the \emph{alpha} aesthetic,
which controls the transparency of the points, or the shape of the
points.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Left}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{alpha =} \NormalTok{class))}

\CommentTok{# Right}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{shape =} \NormalTok{class))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-10-1} \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-10-2} \end{figure}

What happened to the SUVs? ggplot2 will only use six shapes at a time.
By default, additional groups will go unplotted when you use the shape
aesthetic.

For each aesthetic, you use \texttt{aes()} to associate the name of the
aesthetic with a variable to display. The \texttt{aes()} function
gathers together each of the aesthetic mappings used by a layer and
passes them to the layer's mapping argument. The syntax highlights a
useful insight about \texttt{x} and \texttt{y}: the x and y locations of
a point are themselves aesthetics, visual properties that you can map to
variables to display information about the data.

Once you map an aesthetic, ggplot2 takes care of the rest. It selects a
reasonable scale to use with the aesthetic, and it constructs a legend
that explains the mapping between levels and values. For x and y
aesthetics, ggplot2 does not create a legend, but it creates an axis
line with tick marks and a label. The axis line acts as a legend; it
explains the mapping between locations and values.

You can also \emph{set} the aesthetic properties of your geom manually.
For example, we can make all of the points in our plot blue:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-11-1} \end{center}

Here, the color doesn't convey information about a variable, but only
changes the appearance of the plot. To set an aesthetic manually, set
the aesthetic by name as an argument of your geom function; i.e.~it goes
\emph{outside} of \texttt{aes()}. You'll need to pick a value that makes
sense for that aesthetic:

\begin{itemize}
\item
  The name of a color as a character string.
\item
  The size of a point in mm.
\item
  The shape of a point as a number, as shown in Figure \ref{fig:shapes}.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{_bookdown_files/visualize_files/figure-latex/shapes-1} 

}

\caption{R has 25 built in shapes that are identified by numbers. There are some seeming duplicates: for example, 0, 15, and 22 are all squares. The difference comes from the interaction of the `colour` and `fill` aesthetics. The hollow shapes (0--14) have a border determined by `colour`; the solid shapes (15--18) are filled with `colour`; the filled shapes (21--24) have a border of `colour` and are filled with `fill`.}\label{fig:shapes}
\end{figure}

\subsection{Exercises}\label{exercises-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What's gone wrong with this code? Why are the points not blue?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-12-1} \end{center}
\item
  Which variables in \texttt{mpg} are categorical? Which variables are
  continuous? (Hint: type \texttt{?mpg} to read the documentation for
  the dataset). How can you see this information when you run
  \texttt{mpg}?
\item
  Map a continuous variable to \texttt{color}, \texttt{size}, and
  \texttt{shape}. How do these aesthetics behave differently for
  categorical vs.~continuous variables?
\item
  What happens if you map the same variable to multiple aesthetics?
\item
  What does the \texttt{stroke} aesthetic do? What shapes does it work
  with? (Hint: use \texttt{?geom\_point})
\item
  What happens if you map an aesthetic to something other than a
  variable name, like \texttt{aes(colour\ =\ displ\ \textless{}\ 5)}?
\end{enumerate}

\section{Common problems}\label{common-problems}

As you start to run R code, you're likely to run into problems. Don't
worry --- it happens to everyone. I have been writing R code for years,
and every day I still write code that doesn't work!

Start by carefully comparing the code that you're running to the code in
the book. R is extremely picky, and a misplaced character can make all
the difference. Make sure that every \texttt{(} is matched with a
\texttt{)} and every \texttt{"} is paired with another \texttt{"}.
Sometimes you'll run the code and nothing happens. Check the left-hand
of your console: if it's a \texttt{+}, it means that R doesn't think
you've typed a complete expression and it's waiting for you to finish
it. In this case, it's usually easy to start from scratch again by
pressing ESCAPE to abort processing the current command.

One common problem when creating ggplot2 graphics is to put the
\texttt{+} in the wrong place: it has to come at the end of the line,
not the start. In other words, make sure you haven't accidentally
written code like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) }
\NormalTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}
\end{Highlighting}
\end{Shaded}

If you're still stuck, try the help. You can get help about any R
function by running \texttt{?function\_name} in the console, or
selecting the function name and pressing F1 in RStudio. Don't worry if
the help doesn't seem that helpful - instead skip down to the examples
and look for code that matches what you're trying to do.

If that doesn't help, carefully read the error message. Sometimes the
answer will be buried there! But when you're new to R, the answer might
be in the error message but you don't yet know how to understand it.
Another great tool is Google: trying googling the error message, as it's
likely someone else has had the same problem, and has gotten help
online.

\section{Facets}\label{facets}

One way to add additional variables is with aesthetics. Another way,
particularly useful for categorical variables, is to split your plot
into \textbf{facets}, subplots that each display one subset of the data.

To facet your plot by a single variable, use \texttt{facet\_wrap()}. The
first argument of \texttt{facet\_wrap()} should be a formula, which you
create with \texttt{\textasciitilde{}} followed by a variable name (here
``formula'' is the name of a data structure in R, not a synonym for
``equation''). The variable that you pass to \texttt{facet\_wrap()}
should be discrete.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{class, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-13-1} \end{center}

To facet your plot on the combination of two variables, add
\texttt{facet\_grid()} to your plot call. The first argument of
\texttt{facet\_grid()} is also a formula. This time the formula should
contain two variable names separated by a \texttt{\textasciitilde{}}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(drv ~}\StringTok{ }\NormalTok{cyl)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-14-1} \end{center}

If you prefer to not facet in the rows or columns dimension, use a
\texttt{.} instead of a variable name, e.g.
\texttt{+\ facet\_grid(.\ \textasciitilde{}\ cyl)}.

\subsection{Exercises}\label{exercises-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What happens if you facet on a continuous variable?
\item
  What do the empty cells in plot with
  \texttt{facet\_grid(drv\ \textasciitilde{}\ cyl)} mean? How do they
  relate to this plot?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{drv, }\DataTypeTok{y =} \NormalTok{cyl))}
\end{Highlighting}
\end{Shaded}
\item
  What plots does the following code make? What does \texttt{.} do?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(drv ~}\StringTok{ }\NormalTok{.)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(. ~}\StringTok{ }\NormalTok{cyl)}
\end{Highlighting}
\end{Shaded}
\item
  Take the first faceted plot in this section:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{class, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  What are the advantages to using faceting instead of the colour
  aesthetic? What are the disadvantages? How might the balance change if
  you had a larger dataset?
\item
  Read \texttt{?facet\_wrap}. What does \texttt{nrow} do? What does
  \texttt{ncol} do? What other options control the layout of the
  individual panels? Why doesn't \texttt{facet\_grid()} have
  \texttt{nrow} and \texttt{ncol} variables?
\item
  When using \texttt{facet\_grid()} you should usually put the variable
  with more unique levels in the columns. Why?
\end{enumerate}

\section{Geometric objects}\label{geometric-objects}

How are these two plots similar?

\includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-18-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-18-2}

Both plots contain the same x variable, the same y variable, and both
describe the same data. But the plots are not identical. Each plot uses
a different visual object to represent the data. In ggplot2 syntax, we
say that they use different \textbf{geoms}.

A \textbf{geom} is the geometrical object that a plot uses to represent
data. People often describe plots by the type of geom that the plot
uses. For example, bar charts use bar geoms, line charts use line geoms,
boxplots use boxplot geoms, and so on. Scatterplots break the trend;
they use the point geom. As we see above, you can use different geoms to
plot the same data. The plot on the left uses the point geom, and the
plot on the right uses the smooth geom, a smooth line fitted to the
data.

To change the geom in your plot, change the geom function that you add
to \texttt{ggplot()}. For instance, to make the plots above, you can use
this code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# left}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}

\CommentTok{# right}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}
\end{Highlighting}
\end{Shaded}

Every geom function in ggplot2 takes a \texttt{mapping} argument.
However, not every aesthetic works with every geom. You could set the
shape of a point, but you couldn't set the ``shape'' of a line. On the
other hand, you \emph{could} set the linetype of a line.
\texttt{geom\_smooth()} will draw a different line, with a different
linetype, for each unique value of the variable that you map to
linetype.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{linetype =} \NormalTok{drv))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-20-1} \end{center}

Here \texttt{geom\_smooth()} separates the cars into three lines based
on their \texttt{drv} value, which describes a car's drivetrain. One
line describes all of the points with a \texttt{4} value, one line
describes all of the points with an \texttt{f} value, and one line
describes all of the points with an \texttt{r} value. Here, \texttt{4}
stands for four-wheel drive, \texttt{f} for front-wheel drive, and
\texttt{r} for rear-wheel drive.

If this sounds strange, we can make it more clear by overlaying the
lines on top of the raw data and then coloring everything according to
\texttt{drv}.

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-21-1} \end{center}

Notice that this plot contains two geoms in the same graph! If this
makes you excited, buckle up. In the next section, we will learn how to
place multiple geoms in the same plot.

ggplot2 provides over 30 geoms, and extension packages provide even more
(see \url{https://www.ggplot2-exts.org} for a sampling). The best way to
get a comprehensive overview is the ggplot2 cheatsheet, which you can
find at \url{http://rstudio.com/cheatsheets}. To learn more about any
single geom, use help: \texttt{?geom\_smooth}.

Many geoms, like \texttt{geom\_smooth()}, use a single geometric object
to display multiple rows of data. For these geoms, you can set the
\texttt{group} aesthetic to a categorical variable to draw multiple
objects. ggplot2 will draw a separate object for each unique value of
the grouping variable. In practice, ggplot2 will automatically group the
data for these geoms whenever you map an aesthetic to a discrete
variable (as in the \texttt{linetype} example). It is convenient to rely
on this feature because the group aesthetic by itself does not add a
legend or distinguishing features to the geoms.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}
              
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{group =} \NormalTok{drv))}
    
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}
    \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{group =} \NormalTok{drv)}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.33\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-22-1}
\includegraphics[width=0.33\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-22-2}
\includegraphics[width=0.33\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-22-3}

To display multiple geoms in the same plot, add multiple geom functions
to \texttt{ggplot()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-23-1} \end{center}

This, however, introduces some duplication in our code. Imagine if you
wanted to change the y-axis to display \texttt{cty} instead of
\texttt{hwy}. You'd need to change the variable in two places, and you
might forget to update one. You can avoid this type of repetition by
passing a set of mappings to \texttt{ggplot()}. ggplot2 will treat these
mappings as global mappings that apply to each geom in the graph. In
other words, this code will produce the same plot as the previous code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

If you place mappings in a geom function, ggplot2 will treat them as
local mappings for the layer. It will use these mappings to extend or
overwrite the global mappings \emph{for that layer only}. This makes it
possible to display different aesthetics in different layers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{class)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-25-1} \end{center}

You can use the same idea to specify different \texttt{data} for each
layer. Here, our smooth line displays just a subset of the \texttt{mpg}
dataset, the subcompact cars. The local data argument in
\texttt{geom\_smooth()} overrides the global data argument in
\texttt{ggplot()} for that layer only.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{class)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(mpg, class ==}\StringTok{ "subcompact"}\NormalTok{), }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-26-1} \end{center}

(You'll learn how \texttt{filter()} works in the next chapter: for now,
just know that this command selects only the subcompact cars.)

\subsection{Exercises}\label{exercises-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What geom would you use to draw a line chart? A boxplot? A histogram?
  An area chart?
\item
  Run this code in your head and predict what the output will look like.
  Then, run the code in R and check your predictions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy, }\DataTypeTok{color =} \NormalTok{drv)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  What does \texttt{show.legend\ =\ FALSE} do? What happens if you
  remove it?\\
  Why do you think I used it earlier in the chapter?
\item
  What does the \texttt{se} argument to \texttt{geom\_smooth()} do?
\item
  Will these two graphs look different? Why/why not?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}

\KeywordTok{ggplot}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}
\end{Highlighting}
\end{Shaded}
\item
  Recreate the R code necessary to generate the following graphs.

  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-29-1}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-29-2}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-29-3}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-29-4}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-29-5}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-29-6}
\end{enumerate}

\section{Statistical transformations}\label{statistical-transformations}

Next, let's take a look at a bar chart. Bar charts seem simple, but they
are interesting because they reveal something subtle about plots.
Consider a basic bar chart, as drawn with \texttt{geom\_bar()}. The
following chart displays the total number of diamonds in the
\texttt{diamonds} dataset, grouped by \texttt{cut}. The
\texttt{diamonds} dataset comes in ggplot2 and contains information
about \textasciitilde{}54,000 diamonds, including the \texttt{price},
\texttt{carat}, \texttt{color}, \texttt{clarity}, and \texttt{cut} of
each diamond. The chart shows that more diamonds are available with high
quality cuts than with low quality cuts.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-30-1} \end{center}

On the x-axis, the chart displays \texttt{cut}, a variable from
\texttt{diamonds}. On the y-axis, it displays count, but count is not a
variable in \texttt{diamonds}! Where does count come from? Many graphs,
like scatterplots, plot the raw values of your dataset. Other graphs,
like bar charts, calculate new values to plot:

\begin{itemize}
\item
  bar charts, histograms, and frequency polygons bin your data and then
  plot bin counts, the number of points that fall in each bin.
\item
  smoothers fit a model to your data and then plot predictions from the
  model.
\item
  boxplots compute a robust summary of the distribution and then display
  a specially formatted box.
\end{itemize}

The algorithm used to calculate new values for a graph is called a
\textbf{stat}, short for statistical transformation. The figure below
describes how this process works with \texttt{geom\_bar()}.

\begin{center}\includegraphics[width=1\linewidth]{images/visualization-stat-bar} \end{center}

You can learn which stat a geom uses by inspecting the default value for
the \texttt{stat} argument. For example, \texttt{?geom\_bar} shows the
default value for \texttt{stat} is ``count'', which means that
\texttt{geom\_bar()} uses \texttt{stat\_count()}. \texttt{stat\_count()}
is documented on the same page as \texttt{geom\_bar()}, and if you
scroll down you can find a section called ``Computed variables''. That
tells that it computes two new variables: \texttt{count} and
\texttt{prop}.

You can generally use geoms and stats interchangeably. For example, you
can recreate the previous plot using \texttt{stat\_count()} instead of
\texttt{geom\_bar()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{stat_count}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-32-1} \end{center}

This works because every geom has a default stat; and every stat has a
default geom. This means that you can typically use geoms without
worrying about the underlying statistical transformation. There are
three reasons you might need to use a stat explicitly:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You might want to override the default stat. In the code below, I
  change the stat of \texttt{geom\_bar()} from count (the default) to
  identity. This lets me map the height of the bars to the raw values of
  a \(y\) variable. Unfortunately when people talk about bar charts
  casually, they might be referring to this type of bar chart, where the
  height of the bar is already present in the data, or the previous bar
  chart where the height of the bar is generated by counting rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demo <-}\StringTok{ }\NormalTok{tibble::}\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~a,      ~b,}
  \StringTok{"bar_1"}\NormalTok{, }\DecValTok{20}\NormalTok{,}
  \StringTok{"bar_2"}\NormalTok{, }\DecValTok{30}\NormalTok{,}
  \StringTok{"bar_3"}\NormalTok{, }\DecValTok{40}
\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{demo) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{a, }\DataTypeTok{y =} \NormalTok{b), }\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-33-1} \end{center}

  (Don't worry that you haven't seen \texttt{\textless{}-} or
  \texttt{tibble()} before. You might be able to guess at their meaning
  from the context, and you'll learn exactly what they do soon!)
\item
  You might want to override the default mapping from transformed
  variables to aesthetics. For example, you might want to display a bar
  chart of proportion, rather than count:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{y =} \NormalTok{..prop.., }\DataTypeTok{group =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-34-1} \end{center}

  To find the variables computed by the stat, look for the help section
  titled ``computed variables''.
\item
  You might want to draw greater attention to the statistical
  transformation in your code. For example, you might use
  \texttt{stat\_summary()}, which summarises the y values for each
  unique x value, to draw attention to the summary that you're
  computing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{stat_summary}\NormalTok{(}
    \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{y =} \NormalTok{depth),}
    \DataTypeTok{fun.ymin =} \NormalTok{min,}
    \DataTypeTok{fun.ymax =} \NormalTok{max,}
    \DataTypeTok{fun.y =} \NormalTok{median}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-35-1} \end{center}
\end{enumerate}

ggplot2 provides over 20 stats for you to use. Each stat is a function,
so you can get help in usual way, e.g. \texttt{?stat\_bin}. To see a
complete list of stats, try the ggplot2 cheatsheet.

\subsection{Exercises}\label{exercises-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the default geom associated with \texttt{stat\_summary()}? How
  could you rewrite the previous plot to use that geom function instead
  of the stat function?
\item
  What does \texttt{geom\_col()} do? How is it different to
  \texttt{geom\_bar()}?
\item
  Most geoms and stats come in pairs that are almost always used in
  concert. Read through the documentation and make a list of all the
  pairs. What do they have in common?
\item
  What variables does \texttt{stat\_smooth()} compute? What parameters
  control its behaviour?
\item
  In our proportion bar chart, we need to set \texttt{group\ =\ 1}. Why?
  In other words what is the problem with these two graphs?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{y =} \NormalTok{..prop..))}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{fill =} \NormalTok{color, }\DataTypeTok{y =} \NormalTok{..prop..))}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{Position adjustments}\label{position-adjustments}

There's one more piece of magic associated with bar charts. You can
colour a bar chart using either the \texttt{colour} aesthetic, or more
usefully, \texttt{fill}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{colour =} \NormalTok{cut))}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{fill =} \NormalTok{cut))}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-37-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-37-2}

Note what happens if you map the fill aesthetic to another variable,
like \texttt{clarity}: the bars are automatically stacked. Each colored
rectangle represents a combination of \texttt{cut} and \texttt{clarity}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{fill =} \NormalTok{clarity))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-38-1} \end{center}

The stacking is performed automatically by the \textbf{position
adjustment} specified by the \texttt{position} argument. If you don't
want a stacked bar chart, you can use one of three other options:
\texttt{"identity"}, \texttt{"dodge"} or \texttt{"fill"}.

\begin{itemize}
\item
  \texttt{position\ =\ "identity"} will place each object exactly where
  it falls in the context of the graph. This is not very useful for
  bars, because it overlaps them. To see that overlapping we either need
  to make the bars slightly transparent by setting \texttt{alpha} to a
  small value, or completely transparent by setting
  \texttt{fill\ =\ NA}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{fill =} \NormalTok{clarity)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{5}\NormalTok{, }\DataTypeTok{position =} \StringTok{"identity"}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{colour =} \NormalTok{clarity)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{fill =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{position =} \StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-39-1}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-39-2}

  The identity position adjustment is more useful for 2d geoms, like
  points, where it is the default.
\item
  \texttt{position\ =\ "fill"} works like stacking, but makes each set
  of stacked bars the same height. This makes it easier to compare
  proportions across groups.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{fill =} \NormalTok{clarity), }\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-40-1} \end{center}
\item
  \texttt{position\ =\ "dodge"} places overlapping objects directly
  \emph{beside} one another. This makes it easier to compare individual
  values.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{fill =} \NormalTok{clarity), }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-41-1} \end{center}
\end{itemize}

There's one other type of adjustment that's not useful for bar charts,
but it can be very useful for scatterplots. Recall our first
scatterplot. Did you notice that the plot displays only 126 points, even
though there are 234 observations in the dataset?

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-42-1} \end{center}

The values of \texttt{hwy} and \texttt{displ} are rounded so the points
appear on a grid and many points overlap each other. This problem is
known as \textbf{overplotting}. This arrangement makes it hard to see
where the mass of the data is. Are the data points spread equally
throughout the graph, or is there one special combination of
\texttt{hwy} and \texttt{displ} that contains 109 values?

You can avoid this gridding by setting the position adjustment to
``jitter''. \texttt{position\ =\ "jitter"} adds a small amount of random
noise to each point. This spreads the points out because no two points
are likely to receive the same amount of random noise.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy), }\DataTypeTok{position =} \StringTok{"jitter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-43-1} \end{center}

Adding randomness seems like a strange way to improve your plot, but
while it makes your graph less accurate at small scales, it makes your
graph \emph{more} revealing at large scales. Because this is such a
useful operation, ggplot2 comes with a shorthand for
\texttt{geom\_point(position\ =\ "jitter")}: \texttt{geom\_jitter()}.

To learn more about a position adjustment, look up the help page
associated with each adjustment: \texttt{?position\_dodge},
\texttt{?position\_fill}, \texttt{?position\_identity},
\texttt{?position\_jitter}, and \texttt{?position\_stack}.

\subsection{Exercises}\label{exercises-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the problem with this plot? How could you improve it?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cty, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-44-1} \end{center}
\item
  What parameters to \texttt{geom\_jitter()} control the amount of
  jittering?
\item
  Compare and contrast \texttt{geom\_jitter()} with
  \texttt{geom\_count()}.
\item
  What's the default position adjustment for \texttt{geom\_boxplot()}?
  Create a visualisation of the \texttt{mpg} dataset that demonstrates
  it.
\end{enumerate}

\section{Coordinate systems}\label{coordinate-systems}

Coordinate systems are probably the most complicated part of ggplot2.
The default coordinate system is the Cartesian coordinate system where
the x and y position act independently to find the location of each
point. There are a number of other coordinate systems that are
occasionally helpful.

\begin{itemize}
\item
  \texttt{coord\_flip()} switches the x and y axes. This is useful (for
  example), if you want horizontal boxplots. It's also useful for long
  labels: it's hard to get them to fit without overlapping on the
  x-axis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{class, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{class, }\DataTypeTok{y =} \NormalTok{hwy)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() +}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-45-1}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-45-2}
\item
  \texttt{coord\_quickmap()} sets the aspect ratio correctly for maps.
  This is very important if you're plotting spatial data with ggplot2
  (which unfortunately we don't have the space to cover in this book).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nz <-}\StringTok{ }\KeywordTok{map_data}\NormalTok{(}\StringTok{"nz"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(nz, }\KeywordTok{aes}\NormalTok{(long, lat, }\DataTypeTok{group =} \NormalTok{group)) +}
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"black"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(nz, }\KeywordTok{aes}\NormalTok{(long, lat, }\DataTypeTok{group =} \NormalTok{group)) +}
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"black"}\NormalTok{) +}
\StringTok{  }\KeywordTok{coord_quickmap}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-46-1}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-46-2}
\item
  \texttt{coord\_polar()} uses polar coordinates. Polar coordinates
  reveal an interesting connection between a bar chart and a Coxcomb
  chart.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}
    \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{fill =} \NormalTok{cut), }
    \DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{width =} \DecValTok{1}
  \NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{aspect.ratio =} \DecValTok{1}\NormalTok{) +}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \OtherTok{NULL}\NormalTok{)}

\NormalTok{bar +}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{()}
\NormalTok{bar +}\StringTok{ }\KeywordTok{coord_polar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-47-1}
  \includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-47-2}
\end{itemize}

\subsection{Exercises}\label{exercises-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Turn a stacked bar chart into a pie chart using
  \texttt{coord\_polar()}.
\item
  What does \texttt{labs()} do? Read the documentation.
\item
  What's the difference between \texttt{coord\_quickmap()} and
  \texttt{coord\_map()}?
\item
  What does the plot below tell you about the relationship between city
  and highway mpg? Why is \texttt{coord\_fixed()} important? What does
  \texttt{geom\_abline()} do?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cty, }\DataTypeTok{y =} \NormalTok{hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{() +}
\StringTok{  }\KeywordTok{coord_fixed}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.5\linewidth]{_bookdown_files/visualize_files/figure-latex/unnamed-chunk-48-1} \end{center}
\end{enumerate}

\section{The layered grammar of
graphics}\label{the-layered-grammar-of-graphics}

In the previous sections, you learned much more than how to make
scatterplots, bar charts, and boxplots. You learned a foundation that
you can use to make \emph{any} type of plot with ggplot2. To see this,
let's add position adjustments, stats, coordinate systems, and faceting
to our code template:

\begin{verbatim}
ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(
     mapping = aes(<MAPPINGS>),
     stat = <STAT>, 
     position = <POSITION>
  ) +
  <COORDINATE_FUNCTION> +
  <FACET_FUNCTION>
\end{verbatim}

Our new template takes seven parameters, the bracketed words that appear
in the template. In practice, you rarely need to supply all seven
parameters to make a graph because ggplot2 will provide useful defaults
for everything except the data, the mappings, and the geom function.

The seven parameters in the template compose the grammar of graphics, a
formal system for building plots. The grammar of graphics is based on
the insight that you can uniquely describe \emph{any} plot as a
combination of a dataset, a geom, a set of mappings, a stat, a position
adjustment, a coordinate system, and a faceting scheme.

To see how this works, consider how you could build a basic plot from
scratch: you could start with a dataset and then transform it into the
information that you want to display (with a stat).

\begin{center}\includegraphics[width=1\linewidth]{images/visualization-grammar-1} \end{center}

Next, you could choose a geometric object to represent each observation
in the transformed data. You could then use the aesthetic properties of
the geoms to represent variables in the data. You would map the values
of each variable to the levels of an aesthetic.

\begin{center}\includegraphics[width=1\linewidth]{images/visualization-grammar-2} \end{center}

You'd then select a coordinate system to place the geoms into. You'd use
the location of the objects (which is itself an aesthetic property) to
display the values of the x and y variables. At that point, you would
have a complete graph, but you could further adjust the positions of the
geoms within the coordinate system (a position adjustment) or split the
graph into subplots (faceting). You could also extend the plot by adding
one or more additional layers, where each additional layer uses a
dataset, a geom, a set of mappings, a stat, and a position adjustment.

\begin{center}\includegraphics[width=1\linewidth]{images/visualization-grammar-3} \end{center}

You could use this method to build \emph{any} plot that you imagine. In
other words, you can use the code template that you've learned in this
chapter to build hundreds of thousands of unique plots.

\hypertarget{workflow-basics}{\chapter{Workflow:
basics}\label{workflow-basics}}

You now have some experience running R code. I didn't give you many
details, but you've obviously figured out the basics, or you would've
thrown this book away in frustration! Frustration is natural when you
start programming in R, because it is such a stickler for punctuation,
and even one character out of place will cause it to complain. But while
you should expect to be a little frustrated, take comfort in that it's
both typical and temporary: it happens to everyone, and the only way to
get over it is to keep trying.

Before we go any further, let's make sure you've got a solid foundation
in running R code, and that you know about some of the most helpful
RStudio features.

\section{Coding basics}\label{coding-basics}

Let's review some basics we've so far omitted in the interests of
getting you plotting as quickly as possible. You can use R as a
calculator:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \NormalTok{/}\StringTok{ }\DecValTok{200} \NormalTok{*}\StringTok{ }\DecValTok{30}
\CommentTok{#> [1] 0.15}
\NormalTok{(}\DecValTok{59} \NormalTok{+}\StringTok{ }\DecValTok{73} \NormalTok{+}\StringTok{ }\DecValTok{2}\NormalTok{) /}\StringTok{ }\DecValTok{3}
\CommentTok{#> [1] 44.7}
\KeywordTok{sin}\NormalTok{(pi /}\StringTok{ }\DecValTok{2}\NormalTok{)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

You can create new objects with \texttt{\textless{}-}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{3} \NormalTok{*}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

All R statements where you create objects, \textbf{assignment}
statements, have the same form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{object_name <-}\StringTok{ }\NormalTok{value}
\end{Highlighting}
\end{Shaded}

When reading that code say ``object name gets value'' in your head.

You will make lots of assignments and \texttt{\textless{}-} is a pain to
type. Don't be lazy and use \texttt{=}: it will work, but it will cause
confusion later. Instead, use RStudio's keyboard shortcut: Alt + - (the
minus sign). Notice that RStudio automagically surrounds
\texttt{\textless{}-} with spaces, which is a good code formatting
practice. Code is miserable to read on a good day, so giveyoureyesabreak
and use spaces.

\section{What's in a name?}\label{whats-in-a-name}

Object names must start with a letter, and can only contain letters,
numbers, \texttt{\_} and \texttt{.}. You want your object names to be
descriptive, so you'll need a convention for multiple words. I recommend
\textbf{snake\_case} where you separate lowercase words with
\texttt{\_}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i_use_snake_case}
\NormalTok{otherPeopleUseCamelCase}
\NormalTok{some.people.use.periods}
\NormalTok{And_aFew.People_RENOUNCEconvention}
\end{Highlighting}
\end{Shaded}

We'll come back to code style later, in
\protect\hyperlink{functions}{functions}.

You can inspect an object by typing its name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}
\CommentTok{#> [1] 12}
\end{Highlighting}
\end{Shaded}

Make another assignment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{this_is_a_really_long_name <-}\StringTok{ }\FloatTok{2.5}
\end{Highlighting}
\end{Shaded}

To inspect this object, try out RStudio's completion facility: type
``this'', press TAB, add characters until you have a unique prefix, then
press return.

Ooops, you made a mistake! \texttt{this\_is\_a\_really\_long\_name}
should have value 3.5 not 2.5. Use another keyboard shortcut to help you
fix it. Type ``this'' then press Cmd/Ctrl + ↑. That will list all the
commands you've typed that start those letters. Use the arrow keys to
navigate, then press enter to retype the command. Change 2.5 to 3.5 and
rerun.

Make yet another assignment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r_rocks <-}\StringTok{ }\DecValTok{2} \NormalTok{^}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

Let's try to inspect it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r_rock}
\CommentTok{#> Error: object 'r_rock' not found}
\NormalTok{R_rocks}
\CommentTok{#> Error: object 'R_rocks' not found}
\end{Highlighting}
\end{Shaded}

There's an implied contract between you and R: it will do the tedious
computation for you, but in return, you must be completely precise in
your instructions. Typos matter. Case matters.

\section{Calling functions}\label{calling-functions}

R has a large collection of built-in functions that are called like
this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{function_name}\NormalTok{(}\DataTypeTok{arg1 =} \NormalTok{val1, }\DataTypeTok{arg2 =} \NormalTok{val2, ...)}
\end{Highlighting}
\end{Shaded}

Let's try using \texttt{seq()} which makes regular \textbf{seq}uences of
numbers and, while we're at it, learn more helpful features of RStudio.
Type \texttt{se} and hit TAB. A popup shows you possible completions.
Specify \texttt{seq()} by typing more (a ``q'') to disambiguate, or by
using ↑/↓ arrows to select. Notice the floating tooltip that pops up,
reminding you of the function's arguments and purpose. If you want more
help, press F1 to get all the details in help tab in the lower right
pane.

Press TAB once more when you've selected the function you want. RStudio
will add matching opening (\texttt{(}) and closing (\texttt{)})
parentheses for you. Type the arguments \texttt{1,\ 10} and hit return.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\CommentTok{#>  [1]  1  2  3  4  5  6  7  8  9 10}
\end{Highlighting}
\end{Shaded}

Type this code and notice similar assistance help with the paired
quotation marks:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "hello world"}
\end{Highlighting}
\end{Shaded}

Quotation marks and parentheses must always come in a pair. RStudio does
its best to help you, but it's still possible to mess up and end up with
a mismatch. If this happens, R will show you the continuation character
``+'':

\begin{verbatim}
> x <- "hello
+
\end{verbatim}

The \texttt{+} tells you that R is waiting for more input; it doesn't
think you're done yet. Usually that means you've forgotten either a
\texttt{"} or a \texttt{)}. Either add the missing pair, or press ESCAPE
to abort the expression and try again.

If you make an assignment, you don't get to see the value. You're then
tempted to immediately double-check the result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{5}\NormalTok{)}
\NormalTok{y}
\CommentTok{#> [1]  1.00  3.25  5.50  7.75 10.00}
\end{Highlighting}
\end{Shaded}

This common action can be shortened by surrounding the assignment with
parentheses, which causes assignment and ``print to screen'' to happen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(y <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{5}\NormalTok{))}
\CommentTok{#> [1]  1.00  3.25  5.50  7.75 10.00}
\end{Highlighting}
\end{Shaded}

Now look at your environment in the upper right pane:

\begin{center}\includegraphics{screenshots/rstudio-env} \end{center}

Here you can see all of the objects that you've created.

\section{Practice}\label{practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why does this code not work?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_variable <-}\StringTok{ }\DecValTok{10}
\NormalTok{my_varıable}
\CommentTok{#> Error in eval(expr, envir, enclos): object 'my_varıable' not found}
\end{Highlighting}
\end{Shaded}

  Look carefully! (This may seem like an exercise in pointlessness, but
  training your brain to notice even the tiniest difference will pay off
  when programming.)
\item
  Tweak each of the following R commands so that they run correctly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{dota =} \NormalTok{mpg) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{displ, }\DataTypeTok{y =} \NormalTok{hwy))}

\KeywordTok{fliter}\NormalTok{(mpg, }\DataTypeTok{cyl =} \DecValTok{8}\NormalTok{)}
\KeywordTok{filter}\NormalTok{(diamond, carat >}\StringTok{ }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  Press Alt + Shift + K. What happens? How can you get to the same place
  using the menus?
\end{enumerate}

\hypertarget{transform}{\chapter{Data transformation}\label{transform}}

\section{Introduction}\label{introduction-2}

Visualisation is an important tool for insight generation, but it is
rare that you get the data in exactly the right form you need. Often
you'll need to create some new variables or summaries, or maybe you just
want to rename the variables or reorder the observations in order to
make the data a little easier to work with. You'll learn how to do all
that (and more!) in this chapter, which will teach you how to transform
your data using the dplyr package and a new dataset on flights departing
New York City in 2013.

\subsection{Prerequisites}\label{prerequisites-2}

In this chapter we're going to focus on how to use the dplyr package,
another core member of the tidyverse. We'll illustrate the key ideas
using data from the nycflights13 package, and use ggplot2 to help us
understand the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nycflights13)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Take careful note of the conflicts message that's printed when you load
the tidyverse. It tells you that dplyr overwrites some functions in base
R. If you want to use the base version of these functions after loading
dplyr, you'll need to use their full names: \texttt{stats::filter()} and
\texttt{stats::lag()}.

\subsection{nycflights13}\label{nycflights13}

To explore the basic data manipulation verbs of dplyr, we'll use
\texttt{nycflights13::flights}. This data frame contains all 336,776
flights that departed from New York City in 2013. The data comes from
the US
\href{http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120\&Link=0}{Bureau
of Transportation Statistics}, and is documented in \texttt{?flights}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights}
\CommentTok{#> # A tibble: 336,776 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      517            515         2      830}
\CommentTok{#> 2  2013     1     1      533            529         4      850}
\CommentTok{#> 3  2013     1     1      542            540         2      923}
\CommentTok{#> 4  2013     1     1      544            545        -1     1004}
\CommentTok{#> 5  2013     1     1      554            600        -6      812}
\CommentTok{#> 6  2013     1     1      554            558        -4      740}
\CommentTok{#> # ... with 3.368e+05 more rows, and 12 more variables:}
\CommentTok{#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,}
\CommentTok{#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

You might notice that this data frame prints a little differently from
other data frames you might have used in the past: it only shows the
first few rows and all the columns that fit on one screen. (To see the
whole dataset, you can run \texttt{View(flights)} which will open the
dataset in the RStudio viewer). It prints differently because it's a
\textbf{tibble}. Tibbles are data frames, but slightly tweaked to work
better in the tidyverse. For now, you don't need to worry about the
differences; we'll come back to tibbles in more detail in
\protect\hyperlink{wrangle-intro}{wrangle}.

You might also have noticed the row of three (or four) letter
abbreviations under the column names. These describe the type of each
variable:

\begin{itemize}
\item
  \texttt{int} stands for integers.
\item
  \texttt{dbl} stands for doubles, or real numbers.
\item
  \texttt{chr} stands for character vectors, or strings.
\item
  \texttt{dttm} stands for date-times (a date + a time).
\end{itemize}

There are three other common types of variables that aren't used in this
dataset but you'll encounter later in the book:

\begin{itemize}
\item
  \texttt{lgl} stands for logical, vectors that contain only
  \texttt{TRUE} or \texttt{FALSE}.
\item
  \texttt{fctr} stands for factors, which R uses to represent
  categorical variables with fixed possible values.
\item
  \texttt{date} stands for dates.
\end{itemize}

\subsection{dplyr basics}\label{dplyr-basics}

In this chapter you are going to learn the five key dplyr functions that
allow you to solve the vast majority of your data manipulation
challenges:

\begin{itemize}
\tightlist
\item
  Pick observations by their values (\texttt{filter()}).
\item
  Reorder the rows (\texttt{arrange()}).
\item
  Pick variables by their names (\texttt{select()}).
\item
  Create new variables with functions of existing variables
  (\texttt{mutate()}).
\item
  Collapse many values down to a single summary (\texttt{summarise()}).
\end{itemize}

These can all be used in conjunction with \texttt{group\_by()} which
changes the scope of each function from operating on the entire dataset
to operating on it group-by-group. These six functions provide the verbs
for a language of data manipulation.

All verbs work similarly:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The first argument is a data frame.
\item
  The subsequent arguments describe what to do with the data frame,
  using the variable names (without quotes).
\item
  The result is a new data frame.
\end{enumerate}

Together these properties make it easy to chain together multiple simple
steps to achieve a complex result. Let's dive in and see how these verbs
work.

\section{\texorpdfstring{Filter rows with
\texttt{filter()}}{Filter rows with filter()}}\label{filter-rows-with-filter}

\texttt{filter()} allows you to subset observations based on their
values. The first argument is the name of the data frame. The second and
subsequent arguments are the expressions that filter the data frame. For
example, we can select all flights on January 1st with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(flights, month ==}\StringTok{ }\DecValTok{1}\NormalTok{, day ==}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> # A tibble: 842 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      517            515         2      830}
\CommentTok{#> 2  2013     1     1      533            529         4      850}
\CommentTok{#> 3  2013     1     1      542            540         2      923}
\CommentTok{#> 4  2013     1     1      544            545        -1     1004}
\CommentTok{#> 5  2013     1     1      554            600        -6      812}
\CommentTok{#> 6  2013     1     1      554            558        -4      740}
\CommentTok{#> # ... with 836 more rows, and 12 more variables: sched_arr_time <int>,}
\CommentTok{#> #   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,}
\CommentTok{#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,}
\CommentTok{#> #   minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

When you run that line of code, dplyr executes the filtering operation
and returns a new data frame. dplyr functions never modify their inputs,
so if you want to save the result, you'll need to use the assignment
operator, \texttt{\textless{}-}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jan1 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, month ==}\StringTok{ }\DecValTok{1}\NormalTok{, day ==}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

R either prints out the results, or saves them to a variable. If you
want to do both, you can wrap the assignment in parentheses:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(dec25 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, month ==}\StringTok{ }\DecValTok{12}\NormalTok{, day ==}\StringTok{ }\DecValTok{25}\NormalTok{))}
\CommentTok{#> # A tibble: 719 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013    12    25      456            500        -4      649}
\CommentTok{#> 2  2013    12    25      524            515         9      805}
\CommentTok{#> 3  2013    12    25      542            540         2      832}
\CommentTok{#> 4  2013    12    25      546            550        -4     1022}
\CommentTok{#> 5  2013    12    25      556            600        -4      730}
\CommentTok{#> 6  2013    12    25      557            600        -3      743}
\CommentTok{#> # ... with 713 more rows, and 12 more variables: sched_arr_time <int>,}
\CommentTok{#> #   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,}
\CommentTok{#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,}
\CommentTok{#> #   minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparisons}{\subsection{Comparisons}\label{comparisons}}

To use filtering effectively, you have to know how to select the
observations that you want using the comparison operators. R provides
the standard suite: \texttt{\textgreater{}}, \texttt{\textgreater{}=},
\texttt{\textless{}}, \texttt{\textless{}=}, \texttt{!=} (not equal),
and \texttt{==} (equal).

When you're starting out with R, the easiest mistake to make is to use
\texttt{=} instead of \texttt{==} when testing for equality. When this
happens you'll get an informative error:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(flights, }\DataTypeTok{month =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> Error: filter() takes unnamed arguments. Do you need `==`?}
\end{Highlighting}
\end{Shaded}

There's another common problem you might encounter when using
\texttt{==}: floating point numbers. These results might surprise you!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{) ^}\StringTok{ }\DecValTok{2} \NormalTok{==}\StringTok{ }\DecValTok{2}
\CommentTok{#> [1] FALSE}
\DecValTok{1}\NormalTok{/}\DecValTok{49} \NormalTok{*}\StringTok{ }\DecValTok{49} \NormalTok{==}\StringTok{ }\DecValTok{1}
\CommentTok{#> [1] FALSE}
\end{Highlighting}
\end{Shaded}

Computers use finite precision arithmetic (they obviously can't store an
infinite number of digits!) so remember that every number you see is an
approximation. Instead of relying on \texttt{==}, use \texttt{near()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{near}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{) ^}\StringTok{ }\DecValTok{2}\NormalTok{,  }\DecValTok{2}\NormalTok{)}
\CommentTok{#> [1] TRUE}
\KeywordTok{near}\NormalTok{(}\DecValTok{1} \NormalTok{/}\StringTok{ }\DecValTok{49} \NormalTok{*}\StringTok{ }\DecValTok{49}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}

\subsection{Logical operators}\label{logical-operators}

Multiple arguments to \texttt{filter()} are combined with ``and'': every
expression must be true in order for a row to be included in the output.
For other types of combinations, you'll need to use Boolean operators
yourself: \texttt{\&} is ``and'', \texttt{\textbar{}} is ``or'', and
\texttt{!} is ``not''. Figure \ref{fig:bool-ops} shows the complete set
of Boolean operations.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{diagrams/transform-logical} 

}

\caption{Complete set of boolean operations. `x` is the left-hand circle, `y` is the right-hand circle, and the shaded region show which parts each operator selects.}\label{fig:bool-ops}
\end{figure}

The following code finds all flights that departed in November or
December:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(flights, month ==}\StringTok{ }\DecValTok{11} \NormalTok{|}\StringTok{ }\NormalTok{month ==}\StringTok{ }\DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The order of operations doesn't work like English. You can't write
\texttt{filter(flights,\ month\ ==\ 11\ \textbar{}\ 12)}, which you
might literally translate into ``finds all flights that departed in
November or December''. Instead it finds all months that equal
\texttt{11\ \textbar{}\ 12}, an expression that evaluates to
\texttt{TRUE}. In a numeric context (like here), \texttt{TRUE} becomes
one, so this finds all flights in January, not November or December.
This is quite confusing!

A useful short-hand for this problem is \texttt{x\ \%in\%\ y}. This will
select every row where \texttt{x} is one of the values in \texttt{y}. We
could use it to rewrite the code above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nov_dec <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, month %in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Sometimes you can simplify complicated subsetting by remembering De
Morgan's law: \texttt{!(x\ \&\ y)} is the same as
\texttt{!x\ \textbar{}\ !y}, and \texttt{!(x\ \textbar{}\ y)} is the
same as \texttt{!x\ \&\ !y}. For example, if you wanted to find flights
that weren't delayed (on arrival or departure) by more than two hours,
you could use either of the following two filters:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(flights, !(arr_delay >}\StringTok{ }\DecValTok{120} \NormalTok{|}\StringTok{ }\NormalTok{dep_delay >}\StringTok{ }\DecValTok{120}\NormalTok{))}
\KeywordTok{filter}\NormalTok{(flights, arr_delay <=}\StringTok{ }\DecValTok{120}\NormalTok{, dep_delay <=}\StringTok{ }\DecValTok{120}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As well as \texttt{\&} and \texttt{\textbar{}}, R also has \texttt{\&\&}
and \texttt{\textbar{}\textbar{}}. Don't use them here! You'll learn
when you should use them in
\protect\hyperlink{conditional-execution}{conditional execution}.

Whenever you start using complicated, multipart expressions in
\texttt{filter()}, consider making them explicit variables instead. That
makes it much easier to check your work. You'll learn how to create new
variables shortly.

\subsection{Missing values}\label{missing-values}

One important feature of R that can make comparison tricky are missing
values, or \texttt{NA}s (``not availables''). \texttt{NA} represents an
unknown value so missing values are ``contagious'': almost any operation
involving an unknown value will also be unknown.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{NA} \NormalTok{>}\StringTok{ }\DecValTok{5}
\CommentTok{#> [1] NA}
\DecValTok{10} \NormalTok{==}\StringTok{ }\OtherTok{NA}
\CommentTok{#> [1] NA}
\OtherTok{NA} \NormalTok{+}\StringTok{ }\DecValTok{10}
\CommentTok{#> [1] NA}
\OtherTok{NA} \NormalTok{/}\StringTok{ }\DecValTok{2}
\CommentTok{#> [1] NA}
\end{Highlighting}
\end{Shaded}

The most confusing result is this one:

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{NA} \NormalTok{==}\StringTok{ }\OtherTok{NA}
\CommentTok{#> [1] NA}
\end{Highlighting}
\end{Shaded}

It's easiest to understand why this is true with a bit more context:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let x be Mary's age. We don't know how old she is.}
\NormalTok{x <-}\StringTok{ }\OtherTok{NA}

\CommentTok{# Let y be John's age. We don't know how old he is.}
\NormalTok{y <-}\StringTok{ }\OtherTok{NA}

\CommentTok{# Are John and Mary the same age?}
\NormalTok{x ==}\StringTok{ }\NormalTok{y}
\CommentTok{#> [1] NA}
\CommentTok{# We don't know!}
\end{Highlighting}
\end{Shaded}

If you want to determine if a value is missing, use \texttt{is.na()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.na}\NormalTok{(x)}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}

\texttt{filter()} only includes rows where the condition is
\texttt{TRUE}; it excludes both \texttt{FALSE} and \texttt{NA} values.
If you want to preserve missing values, ask for them explicitly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\KeywordTok{filter}\NormalTok{(df, x >}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> # A tibble: 1 × 1}
\CommentTok{#>       x}
\CommentTok{#>   <dbl>}
\CommentTok{#> 1     3}
\KeywordTok{filter}\NormalTok{(df, }\KeywordTok{is.na}\NormalTok{(x) |}\StringTok{ }\NormalTok{x >}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 1}
\CommentTok{#>       x}
\CommentTok{#>   <dbl>}
\CommentTok{#> 1    NA}
\CommentTok{#> 2     3}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find all flights that

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Had an arrival delay of two or more hours
  \item
    Flew to Houston (\texttt{IAH} or \texttt{HOU})
  \item
    Were operated by United, American, or Delta
  \item
    Departed in summer (July, August, and September)
  \item
    Arrived more than two hours late, but didn't leave late
  \item
    Were delayed by at least an hour, but made up over 30 minutes in
    flight
  \item
    Departed between midnight and 6am (inclusive)
  \end{enumerate}
\item
  Another useful dplyr filtering helper is \texttt{between()}. What does
  it do? Can you use it to simplify the code needed to answer the
  previous challenges?
\item
  How many flights have a missing \texttt{dep\_time}? What other
  variables are missing? What might these rows represent?
\item
  Why is \texttt{NA\ \^{}\ 0} not missing? Why is
  \texttt{NA\ \textbar{}\ TRUE} not missing? Why is
  \texttt{FALSE\ \&\ NA} not missing? Can you figure out the general
  rule? (\texttt{NA\ *\ 0} is a tricky counterexample!)
\end{enumerate}

\section{\texorpdfstring{Arrange rows with
\texttt{arrange()}}{Arrange rows with arrange()}}\label{arrange-rows-with-arrange}

\texttt{arrange()} works similarly to \texttt{filter()} except that
instead of selecting rows, it changes their order. It takes a data frame
and a set of column names (or more complicated expressions) to order by.
If you provide more than one column name, each additional column will be
used to break ties in the values of preceding columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(flights, year, month, day)}
\CommentTok{#> # A tibble: 336,776 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      517            515         2      830}
\CommentTok{#> 2  2013     1     1      533            529         4      850}
\CommentTok{#> 3  2013     1     1      542            540         2      923}
\CommentTok{#> 4  2013     1     1      544            545        -1     1004}
\CommentTok{#> 5  2013     1     1      554            600        -6      812}
\CommentTok{#> 6  2013     1     1      554            558        -4      740}
\CommentTok{#> # ... with 3.368e+05 more rows, and 12 more variables:}
\CommentTok{#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,}
\CommentTok{#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

Use \texttt{desc()} to re-order by a column in descending order:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(flights, }\KeywordTok{desc}\NormalTok{(arr_delay))}
\CommentTok{#> # A tibble: 336,776 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     9      641            900      1301     1242}
\CommentTok{#> 2  2013     6    15     1432           1935      1137     1607}
\CommentTok{#> 3  2013     1    10     1121           1635      1126     1239}
\CommentTok{#> 4  2013     9    20     1139           1845      1014     1457}
\CommentTok{#> 5  2013     7    22      845           1600      1005     1044}
\CommentTok{#> 6  2013     4    10     1100           1900       960     1342}
\CommentTok{#> # ... with 3.368e+05 more rows, and 12 more variables:}
\CommentTok{#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,}
\CommentTok{#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

Missing values are always sorted at the end:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{))}
\KeywordTok{arrange}\NormalTok{(df, x)}
\CommentTok{#> # A tibble: 3 × 1}
\CommentTok{#>       x}
\CommentTok{#>   <dbl>}
\CommentTok{#> 1     2}
\CommentTok{#> 2     5}
\CommentTok{#> 3    NA}
\KeywordTok{arrange}\NormalTok{(df, }\KeywordTok{desc}\NormalTok{(x))}
\CommentTok{#> # A tibble: 3 × 1}
\CommentTok{#>       x}
\CommentTok{#>   <dbl>}
\CommentTok{#> 1     5}
\CommentTok{#> 2     2}
\CommentTok{#> 3    NA}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How could you use \texttt{arrange()} to sort all missing values to the
  start? (Hint: use \texttt{is.na()}).
\item
  Sort \texttt{flights} to find the most delayed flights. Find the
  flights that left earliest.
\item
  Sort \texttt{flights} to find the fastest flights.
\item
  Which flights travelled the longest? Which travelled the shortest?
\end{enumerate}

\section{\texorpdfstring{Select columns with
\texttt{select()}}{Select columns with select()}}\label{select-columns-with-select}

It's not uncommon to get datasets with hundreds or even thousands of
variables. In this case, the first challenge is often narrowing in on
the variables you're actually interested in. \texttt{select()} allows
you to rapidly zoom in on a useful subset using operations based on the
names of the variables.

\texttt{select()} is not terribly useful with the flights data because
we only have 19 variables, but you can still get the general idea:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Select columns by name}
\KeywordTok{select}\NormalTok{(flights, year, month, day)}
\CommentTok{#> # A tibble: 336,776 × 3}
\CommentTok{#>    year month   day}
\CommentTok{#>   <int> <int> <int>}
\CommentTok{#> 1  2013     1     1}
\CommentTok{#> 2  2013     1     1}
\CommentTok{#> 3  2013     1     1}
\CommentTok{#> 4  2013     1     1}
\CommentTok{#> 5  2013     1     1}
\CommentTok{#> 6  2013     1     1}
\CommentTok{#> # ... with 3.368e+05 more rows}
\CommentTok{# Select all columns between year and day (inclusive)}
\KeywordTok{select}\NormalTok{(flights, year:day)}
\CommentTok{#> # A tibble: 336,776 × 3}
\CommentTok{#>    year month   day}
\CommentTok{#>   <int> <int> <int>}
\CommentTok{#> 1  2013     1     1}
\CommentTok{#> 2  2013     1     1}
\CommentTok{#> 3  2013     1     1}
\CommentTok{#> 4  2013     1     1}
\CommentTok{#> 5  2013     1     1}
\CommentTok{#> 6  2013     1     1}
\CommentTok{#> # ... with 3.368e+05 more rows}
\CommentTok{# Select all columns except those from year to day (inclusive)}
\KeywordTok{select}\NormalTok{(flights, -(year:day))}
\CommentTok{#> # A tibble: 336,776 × 16}
\CommentTok{#>   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay}
\CommentTok{#>      <int>          <int>     <dbl>    <int>          <int>     <dbl>}
\CommentTok{#> 1      517            515         2      830            819        11}
\CommentTok{#> 2      533            529         4      850            830        20}
\CommentTok{#> 3      542            540         2      923            850        33}
\CommentTok{#> 4      544            545        -1     1004           1022       -18}
\CommentTok{#> 5      554            600        -6      812            837       -25}
\CommentTok{#> 6      554            558        -4      740            728        12}
\CommentTok{#> # ... with 3.368e+05 more rows, and 10 more variables: carrier <chr>,}
\CommentTok{#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

There are a number of helper functions you can use within
\texttt{select()}:

\begin{itemize}
\item
  \texttt{starts\_with("abc")}: matches names that begin with ``abc''.
\item
  \texttt{ends\_with("xyz")}: matches names that end with ``xyz''.
\item
  \texttt{contains("ijk")}: matches names that contain ``ijk''.
\item
  \texttt{matches("(.)\textbackslash{}\textbackslash{}1")}: selects
  variables that match a regular expression. This one matches any
  variables that contain repeated characters. You'll learn more about
  regular expressions in \protect\hyperlink{strings}{strings}.
\item
  \texttt{num\_range("x",\ 1:3)} matches \texttt{x1}, \texttt{x2} and
  \texttt{x3}.
\end{itemize}

See \texttt{?select} for more details.

\texttt{select()} can be used to rename variables, but it's rarely
useful because it drops all of the variables not explicitly mentioned.
Instead, use \texttt{rename()}, which is a variant of \texttt{select()}
that keeps all the variables that aren't explicitly mentioned:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rename}\NormalTok{(flights, }\DataTypeTok{tail_num =} \NormalTok{tailnum)}
\CommentTok{#> # A tibble: 336,776 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      517            515         2      830}
\CommentTok{#> 2  2013     1     1      533            529         4      850}
\CommentTok{#> 3  2013     1     1      542            540         2      923}
\CommentTok{#> 4  2013     1     1      544            545        -1     1004}
\CommentTok{#> 5  2013     1     1      554            600        -6      812}
\CommentTok{#> 6  2013     1     1      554            558        -4      740}
\CommentTok{#> # ... with 3.368e+05 more rows, and 12 more variables:}
\CommentTok{#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,}
\CommentTok{#> #   tail_num <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

Another option is to use \texttt{select()} in conjunction with the
\texttt{everything()} helper. This is useful if you have a handful of
variables you'd like to move to the start of the data frame.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(flights, time_hour, air_time, }\KeywordTok{everything}\NormalTok{())}
\CommentTok{#> # A tibble: 336,776 × 19}
\CommentTok{#>             time_hour air_time  year month   day dep_time sched_dep_time}
\CommentTok{#>                <dttm>    <dbl> <int> <int> <int>    <int>          <int>}
\CommentTok{#> 1 2013-01-01 05:00:00      227  2013     1     1      517            515}
\CommentTok{#> 2 2013-01-01 05:00:00      227  2013     1     1      533            529}
\CommentTok{#> 3 2013-01-01 05:00:00      160  2013     1     1      542            540}
\CommentTok{#> 4 2013-01-01 05:00:00      183  2013     1     1      544            545}
\CommentTok{#> 5 2013-01-01 06:00:00      116  2013     1     1      554            600}
\CommentTok{#> 6 2013-01-01 05:00:00      150  2013     1     1      554            558}
\CommentTok{#> # ... with 3.368e+05 more rows, and 12 more variables: dep_delay <dbl>,}
\CommentTok{#> #   arr_time <int>, sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,}
\CommentTok{#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, distance <dbl>,}
\CommentTok{#> #   hour <dbl>, minute <dbl>}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Brainstorm as many ways as possible to select \texttt{dep\_time},
  \texttt{dep\_delay}, \texttt{arr\_time}, and \texttt{arr\_delay} from
  \texttt{flights}.
\item
  What happens if you include the name of a variable multiple times in a
  \texttt{select()} call?
\item
  What does the \texttt{one\_of()} function do? Why might it be helpful
  in conjunction with this vector?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"year"}\NormalTok{, }\StringTok{"month"}\NormalTok{, }\StringTok{"day"}\NormalTok{, }\StringTok{"dep_delay"}\NormalTok{, }\StringTok{"arr_delay"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  Does the result of running the following code surprise you? How do the
  select helpers deal with case by default? How can you change that
  default?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(flights, }\KeywordTok{contains}\NormalTok{(}\StringTok{"TIME"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{\texorpdfstring{Add new variables with
\texttt{mutate()}}{Add new variables with mutate()}}\label{add-new-variables-with-mutate}

Besides selecting sets of existing columns, it's often useful to add new
columns that are functions of existing columns. That's the job of
\texttt{mutate()}.

\texttt{mutate()} always adds new columns at the end of your dataset so
we'll start by creating a narrower dataset so we can see the new
variables. Remember that when you're in RStudio, the easiest way to see
all the columns is \texttt{View()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_sml <-}\StringTok{ }\KeywordTok{select}\NormalTok{(flights, }
  \NormalTok{year:day, }
  \KeywordTok{ends_with}\NormalTok{(}\StringTok{"delay"}\NormalTok{), }
  \NormalTok{distance, }
  \NormalTok{air_time}
\NormalTok{)}
\KeywordTok{mutate}\NormalTok{(flights_sml,}
  \DataTypeTok{gain =} \NormalTok{arr_delay -}\StringTok{ }\NormalTok{dep_delay,}
  \DataTypeTok{speed =} \NormalTok{distance /}\StringTok{ }\NormalTok{air_time *}\StringTok{ }\DecValTok{60}
\NormalTok{)}
\CommentTok{#> # A tibble: 336,776 × 9}
\CommentTok{#>    year month   day dep_delay arr_delay distance air_time  gain speed}
\CommentTok{#>   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl> <dbl> <dbl>}
\CommentTok{#> 1  2013     1     1         2        11     1400      227     9   370}
\CommentTok{#> 2  2013     1     1         4        20     1416      227    16   374}
\CommentTok{#> 3  2013     1     1         2        33     1089      160    31   408}
\CommentTok{#> 4  2013     1     1        -1       -18     1576      183   -17   517}
\CommentTok{#> 5  2013     1     1        -6       -25      762      116   -19   394}
\CommentTok{#> 6  2013     1     1        -4        12      719      150    16   288}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}

Note that you can refer to columns that you've just created:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mutate}\NormalTok{(flights_sml,}
  \DataTypeTok{gain =} \NormalTok{arr_delay -}\StringTok{ }\NormalTok{dep_delay,}
  \DataTypeTok{hours =} \NormalTok{air_time /}\StringTok{ }\DecValTok{60}\NormalTok{,}
  \DataTypeTok{gain_per_hour =} \NormalTok{gain /}\StringTok{ }\NormalTok{hours}
\NormalTok{)}
\CommentTok{#> # A tibble: 336,776 × 10}
\CommentTok{#>    year month   day dep_delay arr_delay distance air_time  gain hours}
\CommentTok{#>   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl> <dbl> <dbl>}
\CommentTok{#> 1  2013     1     1         2        11     1400      227     9  3.78}
\CommentTok{#> 2  2013     1     1         4        20     1416      227    16  3.78}
\CommentTok{#> 3  2013     1     1         2        33     1089      160    31  2.67}
\CommentTok{#> 4  2013     1     1        -1       -18     1576      183   -17  3.05}
\CommentTok{#> 5  2013     1     1        -6       -25      762      116   -19  1.93}
\CommentTok{#> 6  2013     1     1        -4        12      719      150    16  2.50}
\CommentTok{#> # ... with 3.368e+05 more rows, and 1 more variables: gain_per_hour <dbl>}
\end{Highlighting}
\end{Shaded}

If you only want to keep the new variables, use \texttt{transmute()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{transmute}\NormalTok{(flights,}
  \DataTypeTok{gain =} \NormalTok{arr_delay -}\StringTok{ }\NormalTok{dep_delay,}
  \DataTypeTok{hours =} \NormalTok{air_time /}\StringTok{ }\DecValTok{60}\NormalTok{,}
  \DataTypeTok{gain_per_hour =} \NormalTok{gain /}\StringTok{ }\NormalTok{hours}
\NormalTok{)}
\CommentTok{#> # A tibble: 336,776 × 3}
\CommentTok{#>    gain hours gain_per_hour}
\CommentTok{#>   <dbl> <dbl>         <dbl>}
\CommentTok{#> 1     9  3.78          2.38}
\CommentTok{#> 2    16  3.78          4.23}
\CommentTok{#> 3    31  2.67         11.62}
\CommentTok{#> 4   -17  3.05         -5.57}
\CommentTok{#> 5   -19  1.93         -9.83}
\CommentTok{#> 6    16  2.50          6.40}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{mutate-funs}{\subsection{Useful creation
functions}\label{mutate-funs}}

There are many functions for creating new variables that you can use
with \texttt{mutate()}. The key property is that the function must be
vectorised: it must take a vector of values as input, return a vector
with the same number of values as output. There's no way to list every
possible function that you might use, but here's a selection of
functions that are frequently useful:

\begin{itemize}
\item
  Arithmetic operators: \texttt{+}, \texttt{-}, \texttt{*}, \texttt{/},
  \texttt{\^{}}. These are all vectorised, using the so called
  ``recycling rules''. If one parameter is shorter than the other, it
  will be automatically extended to be the same length. This is most
  useful when one of the arguments is a single number:
  \texttt{air\_time\ /\ 60}, \texttt{hours\ *\ 60\ +\ minute}, etc.

  Arithmetic operators are also useful in conjunction with the aggregate
  functions you'll learn about later. For example, \texttt{x\ /\ sum(x)}
  calculates the proportion of a total, and \texttt{y\ -\ mean(y)}
  computes the difference from the mean.
\item
  Modular arithmetic: \texttt{\%/\%} (integer division) and
  \texttt{\%\%} (remainder), where
  \texttt{x\ ==\ y\ *\ (x\ \%/\%\ y)\ +\ (x\ \%\%\ y)}. Modular
  arithmetic is a handy tool because it allows you to break integers up
  into pieces. For example, in the flights dataset, you can compute
  \texttt{hour} and \texttt{minute} from \texttt{dep\_time} with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{transmute}\NormalTok{(flights,}
  \NormalTok{dep_time,}
  \DataTypeTok{hour =} \NormalTok{dep_time %/%}\StringTok{ }\DecValTok{100}\NormalTok{,}
  \DataTypeTok{minute =} \NormalTok{dep_time %%}\StringTok{ }\DecValTok{100}
\NormalTok{)}
\CommentTok{#> # A tibble: 336,776 × 3}
\CommentTok{#>   dep_time  hour minute}
\CommentTok{#>      <int> <dbl>  <dbl>}
\CommentTok{#> 1      517     5     17}
\CommentTok{#> 2      533     5     33}
\CommentTok{#> 3      542     5     42}
\CommentTok{#> 4      544     5     44}
\CommentTok{#> 5      554     5     54}
\CommentTok{#> 6      554     5     54}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}
\item
  Logs: \texttt{log()}, \texttt{log2()}, \texttt{log10()}. Logarithms
  are an incredibly useful transformation for dealing with data that
  ranges across multiple orders of magnitude. They also convert
  multiplicative relationships to additive, a feature we'll come back to
  in modelling.

  All else being equal, I recommend using \texttt{log2()} because it's
  easy to interpret: a difference of 1 on the log scale corresponds to
  doubling on the original scale and a difference of -1 corresponds to
  halving.
\item
  Offsets: \texttt{lead()} and \texttt{lag()} allow you to refer to
  leading or lagging values. This allows you to compute running
  differences (e.g. \texttt{x\ -\ lag(x)}) or find when values change
  (\texttt{x\ !=\ lag(x))}. They are most useful in conjunction with
  \texttt{group\_by()}, which you'll learn about shortly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x <-}\StringTok{ }\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{)}
\CommentTok{#>  [1]  1  2  3  4  5  6  7  8  9 10}
\KeywordTok{lag}\NormalTok{(x)}
\CommentTok{#>  [1] NA  1  2  3  4  5  6  7  8  9}
\KeywordTok{lead}\NormalTok{(x)}
\CommentTok{#>  [1]  2  3  4  5  6  7  8  9 10 NA}
\end{Highlighting}
\end{Shaded}
\item
  Cumulative and rolling aggregates: R provides functions for running
  sums, products, mins and maxes: \texttt{cumsum()}, \texttt{cumprod()},
  \texttt{cummin()}, \texttt{cummax()}; and dplyr provides
  \texttt{cummean()} for cumulative means. If you need rolling
  aggregates (i.e.~a sum computed over a rolling window), try the
  RcppRoll package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}
\CommentTok{#>  [1]  1  2  3  4  5  6  7  8  9 10}
\KeywordTok{cumsum}\NormalTok{(x)}
\CommentTok{#>  [1]  1  3  6 10 15 21 28 36 45 55}
\KeywordTok{cummean}\NormalTok{(x)}
\CommentTok{#>  [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5}
\end{Highlighting}
\end{Shaded}
\item
  Logical comparisons, \texttt{\textless{}}, \texttt{\textless{}=},
  \texttt{\textgreater{}}, \texttt{\textgreater{}=}, \texttt{!=}, which
  you learned about earlier. If you're doing a complex sequence of
  logical operations it's often a good idea to store the interim values
  in new variables so you can check that each step is working as
  expected.
\item
  Ranking: there are a number of ranking functions, but you should start
  with \texttt{min\_rank()}. It does the most usual type of ranking
  (e.g.~1st, 2nd, 2nd, 4th). The default gives smallest values the small
  ranks; use \texttt{desc(x)} to give the largest values the smallest
  ranks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\KeywordTok{min_rank}\NormalTok{(y)}
\CommentTok{#> [1]  1  2  2 NA  4  5}
\KeywordTok{min_rank}\NormalTok{(}\KeywordTok{desc}\NormalTok{(y))}
\CommentTok{#> [1]  5  3  3 NA  2  1}
\end{Highlighting}
\end{Shaded}

  If \texttt{min\_rank()} doesn't do what you need, look at the variants
  \texttt{row\_number()}, \texttt{dense\_rank()},
  \texttt{percent\_rank()}, \texttt{cume\_dist()}, \texttt{ntile()}. See
  their help pages for more details.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{row_number}\NormalTok{(y)}
\CommentTok{#> [1]  1  2  3 NA  4  5}
\KeywordTok{dense_rank}\NormalTok{(y)}
\CommentTok{#> [1]  1  2  2 NA  3  4}
\KeywordTok{percent_rank}\NormalTok{(y)}
\CommentTok{#> [1] 0.00 0.25 0.25   NA 0.75 1.00}
\KeywordTok{cume_dist}\NormalTok{(y)}
\CommentTok{#> [1] 0.2 0.6 0.6  NA 0.8 1.0}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsection{Exercises}\label{exercises-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Currently \texttt{dep\_time} and \texttt{sched\_dep\_time} are
  convenient to look at, but hard to compute with because they're not
  really continuous numbers. Convert them to a more convenient
  representation of number of minutes since midnight.
\item
  Compare \texttt{air\_time} with \texttt{arr\_time\ -\ dep\_time}. What
  do you expect to see? What do you see? What do you need to do to fix
  it?
\item
  Compare \texttt{dep\_time}, \texttt{sched\_dep\_time}, and
  \texttt{dep\_delay}. How would you expect those three numbers to be
  related?
\item
  Find the 10 most delayed flights using a ranking function. How do you
  want to handle ties? Carefully read the documentation for
  \texttt{min\_rank()}.
\item
  What does \texttt{1:3\ +\ 1:10} return? Why?
\item
  What trigonometric functions does R provide?
\end{enumerate}

\section{\texorpdfstring{Grouped summaries with
\texttt{summarise()}}{Grouped summaries with summarise()}}\label{grouped-summaries-with-summarise}

The last key verb is \texttt{summarise()}. It collapses a data frame to
a single row:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(flights, }\DataTypeTok{delay =} \KeywordTok{mean}\NormalTok{(dep_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> # A tibble: 1 × 1}
\CommentTok{#>   delay}
\CommentTok{#>   <dbl>}
\CommentTok{#> 1  12.6}
\end{Highlighting}
\end{Shaded}

(We'll come back to what that \texttt{na.rm\ =\ TRUE} means very
shortly.)

\texttt{summarise()} is not terribly useful unless we pair it with
\texttt{group\_by()}. This changes the unit of analysis from the
complete dataset to individual groups. Then, when you use the dplyr
verbs on a grouped data frame they'll be automatically applied ``by
group''. For example, if we applied exactly the same code to a data
frame grouped by date, we get the average delay per date:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_day <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(flights, year, month, day)}
\KeywordTok{summarise}\NormalTok{(by_day, }\DataTypeTok{delay =} \KeywordTok{mean}\NormalTok{(dep_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> Source: local data frame [365 x 4]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day delay}
\CommentTok{#>   <int> <int> <int> <dbl>}
\CommentTok{#> 1  2013     1     1 11.55}
\CommentTok{#> 2  2013     1     2 13.86}
\CommentTok{#> 3  2013     1     3 10.99}
\CommentTok{#> 4  2013     1     4  8.95}
\CommentTok{#> 5  2013     1     5  5.73}
\CommentTok{#> 6  2013     1     6  7.15}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}

Together \texttt{group\_by()} and \texttt{summarise()} provide one of
the tools that you'll use most commonly when working with dplyr: grouped
summaries. But before we go any further with this, we need to introduce
a powerful new idea: the pipe.

\subsection{Combining multiple operations with the
pipe}\label{combining-multiple-operations-with-the-pipe}

Imagine that we want to explore the relationship between the distance
and average delay for each location. Using what you know about dplyr,
you might write code like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_dest <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(flights, dest)}
\NormalTok{delay <-}\StringTok{ }\KeywordTok{summarise}\NormalTok{(by_dest,}
  \DataTypeTok{count =} \KeywordTok{n}\NormalTok{(),}
  \DataTypeTok{dist =} \KeywordTok{mean}\NormalTok{(distance, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
  \DataTypeTok{delay =} \KeywordTok{mean}\NormalTok{(arr_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{)}
\NormalTok{delay <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(delay, count >}\StringTok{ }\DecValTok{20}\NormalTok{, dest !=}\StringTok{ "HNL"}\NormalTok{)}

\CommentTok{# It looks like delays increase with distance up to ~750 miles }
\CommentTok{# and then decrease. Maybe as flights get longer there's more }
\CommentTok{# ability to make up delays in the air?}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{delay, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{dist, }\DataTypeTok{y =} \NormalTok{delay)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{size =} \NormalTok{count), }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{3}\NormalTok{) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/transform_files/figure-latex/unnamed-chunk-36-1} \end{center}

There are three steps to prepare this data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Group flights by destination.
\item
  Summarise to compute distance, average delay, and number of flights.
\item
  Filter to remove noisy points and Honolulu airport, which is almost
  twice as far away as the next closest airport.
\end{enumerate}

This code is a little frustrating to write because we have to give each
intermediate data frame a name, even though we don't care about it.
Naming things is hard, so this slows down our analysis.

There's another way to tackle the same problem with the pipe,
\texttt{\%\textgreater{}\%}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delays <-}\StringTok{ }\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(dest) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{count =} \KeywordTok{n}\NormalTok{(),}
    \DataTypeTok{dist =} \KeywordTok{mean}\NormalTok{(distance, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{delay =} \KeywordTok{mean}\NormalTok{(arr_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
  \NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(count >}\StringTok{ }\DecValTok{20}\NormalTok{, dest !=}\StringTok{ "HNL"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This focuses on the transformations, not what's being transformed, which
makes the code easier to read. You can read it as a series of imperative
statements: group, then summarise, then filter. As suggested by this
reading, a good way to pronounce \texttt{\%\textgreater{}\%} when
reading code is ``then''.

Behind the scenes, \texttt{x\ \%\textgreater{}\%\ f(y)} turns into
\texttt{f(x,\ y)}, and
\texttt{x\ \%\textgreater{}\%\ f(y)\ \%\textgreater{}\%\ g(z)} turns
into \texttt{g(f(x,\ y),\ z)} and so on. You can use the pipe to rewrite
multiple operations in a way that you can read left-to-right,
top-to-bottom. We'll use piping frequently from now on because it
considerably improves the readability of code, and we'll come back to it
in more detail in \protect\hyperlink{pipes}{pipes}.

Working with the pipe is one of the key criteria for belonging to the
tidyverse. The only exception is ggplot2: it was written before the pipe
was discovered. Unfortunately, the next iteration of ggplot2, ggvis,
which does use the pipe, isn't quite ready for prime time yet.

\subsection{Missing values}\label{missing-values-1}

You may have wondered about the \texttt{na.rm} argument we used above.
What happens if we don't set it?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dep_delay))}
\CommentTok{#> Source: local data frame [365 x 4]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day  mean}
\CommentTok{#>   <int> <int> <int> <dbl>}
\CommentTok{#> 1  2013     1     1    NA}
\CommentTok{#> 2  2013     1     2    NA}
\CommentTok{#> 3  2013     1     3    NA}
\CommentTok{#> 4  2013     1     4    NA}
\CommentTok{#> 5  2013     1     5    NA}
\CommentTok{#> 6  2013     1     6    NA}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}

We get a lot of missing values! That's because aggregation functions
obey the usual rule of missing values: if there's any missing value in
the input, the output will be a missing value. Fortunately, all
aggregation functions have an \texttt{na.rm} argument which removes the
missing values prior to computation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dep_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> Source: local data frame [365 x 4]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day  mean}
\CommentTok{#>   <int> <int> <int> <dbl>}
\CommentTok{#> 1  2013     1     1 11.55}
\CommentTok{#> 2  2013     1     2 13.86}
\CommentTok{#> 3  2013     1     3 10.99}
\CommentTok{#> 4  2013     1     4  8.95}
\CommentTok{#> 5  2013     1     5  5.73}
\CommentTok{#> 6  2013     1     6  7.15}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}

In this case, where missing values represent cancelled flights, we could
also tackle the problem by first removing the cancelled flights. We'll
save this dataset so we can reuse in the next few examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_cancelled <-}\StringTok{ }\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(!}\KeywordTok{is.na}\NormalTok{(dep_delay), !}\KeywordTok{is.na}\NormalTok{(arr_delay))}

\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dep_delay))}
\CommentTok{#> Source: local data frame [365 x 4]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day  mean}
\CommentTok{#>   <int> <int> <int> <dbl>}
\CommentTok{#> 1  2013     1     1 11.44}
\CommentTok{#> 2  2013     1     2 13.68}
\CommentTok{#> 3  2013     1     3 10.91}
\CommentTok{#> 4  2013     1     4  8.97}
\CommentTok{#> 5  2013     1     5  5.73}
\CommentTok{#> 6  2013     1     6  7.15}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}

\subsection{Counts}\label{counts}

Whenever you do any aggregation, it's always a good idea to include
either a count (\texttt{n()}), or a count of non-missing values
(\texttt{sum(!is.na(x))}). That way you can check that you're not
drawing conclusions based on very small amounts of data. For example,
let's look at the planes (identified by their tail number) that have the
highest average delays:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delays <-}\StringTok{ }\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(tailnum) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{delay =} \KeywordTok{mean}\NormalTok{(arr_delay)}
  \NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{delays, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{delay)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/transform_files/figure-latex/unnamed-chunk-41-1} \end{center}

Wow, there are some planes that have an \emph{average} delay of 5 hours
(300 minutes)!

The story is actually a little more nuanced. We can get more insight if
we draw a scatterplot of number of flights vs.~average delay:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delays <-}\StringTok{ }\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(tailnum) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{delay =} \KeywordTok{mean}\NormalTok{(arr_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{()}
  \NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{delays, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{n, }\DataTypeTok{y =} \NormalTok{delay)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/transform_files/figure-latex/unnamed-chunk-42-1} \end{center}

Not surprisingly, there is much greater variation in the average delay
when there are few flights. The shape of this plot is very
characteristic: whenever you plot a mean (or other summary) vs.~group
size, you'll see that the variation decreases as the sample size
increases.

When looking at this sort of plot, it's often useful to filter out the
groups with the smallest numbers of observations, so you can see more of
the pattern and less of the extreme variation in the smallest groups.
This is what the following code does, as well as showing you a handy
pattern for integrating ggplot2 into dplyr flows. It's a bit painful
that you have to switch from \texttt{\%\textgreater{}\%} to \texttt{+},
but once you get the hang of it, it's quite convenient.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delays %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n >}\StringTok{ }\DecValTok{25}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{n, }\DataTypeTok{y =} \NormalTok{delay)) +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/transform_files/figure-latex/unnamed-chunk-43-1} \end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

RStudio tip: a useful keyboard shortcut is Cmd/Ctrl + Shift + P. This
resends the previously sent chunk from the editor to the console. This
is very convenient when you're (e.g.) exploring the value of \texttt{n}
in the example above. You send the whole block once with Cmd/Ctrl +
Enter, then you modify the value of \texttt{n} and press Cmd/Ctrl +
Shift + P to resend the complete block.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

There's another common variation of this type of pattern. Let's look at
how the average performance of batters in baseball is related to the
number of times they're at bat. Here I use data from the \textbf{Lahman}
package to compute the batting average (number of hits / number of
attempts) of every major league baseball player.

When I plot the skill of the batter (measured by the batting average,
\texttt{ba}) against the number of opportunities to hit the ball
(measured by at bat, \texttt{ab}), you see two patterns:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  As above, the variation in our aggregate decreases as we get more data
  points.
\item
  There's a positive correlation between skill (\texttt{ba}) and
  opportunities to hit the ball (\texttt{ab}). This is because teams
  control who gets to play, and obviously they'll pick their best
  players.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Convert to a tibble so it prints nicely}
\NormalTok{batting <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(Lahman::Batting)}

\NormalTok{batters <-}\StringTok{ }\NormalTok{batting %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(playerID) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{ba =} \KeywordTok{sum}\NormalTok{(H, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) /}\StringTok{ }\KeywordTok{sum}\NormalTok{(AB, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{ab =} \KeywordTok{sum}\NormalTok{(AB, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
  \NormalTok{)}

\NormalTok{batters %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(ab >}\StringTok{ }\DecValTok{100}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{ab, }\DataTypeTok{y =} \NormalTok{ba)) +}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/transform_files/figure-latex/unnamed-chunk-44-1} \end{center}

This also has important implications for ranking. If you naively sort on
\texttt{desc(ba)}, the people with the best batting averages are clearly
lucky, not skilled:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{batters %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(ba))}
\CommentTok{#> # A tibble: 18,659 × 3}
\CommentTok{#>    playerID    ba    ab}
\CommentTok{#>       <chr> <dbl> <int>}
\CommentTok{#> 1 abramge01     1     1}
\CommentTok{#> 2 banisje01     1     1}
\CommentTok{#> 3 bartocl01     1     1}
\CommentTok{#> 4  bassdo01     1     1}
\CommentTok{#> 5 birasst01     1     2}
\CommentTok{#> 6 bruneju01     1     1}
\CommentTok{#> # ... with 1.865e+04 more rows}
\end{Highlighting}
\end{Shaded}

You can find a good explanation of this problem at
\url{http://varianceexplained.org/r/empirical_bayes_baseball/} and
\url{http://www.evanmiller.org/how-not-to-sort-by-average-rating.html}.

\subsection{Useful summary functions}\label{summarise-funs}

Just using means, counts, and sum can get you a long way, but R provides
many other useful summary functions:

\begin{itemize}
\item
  Measures of location: we've used \texttt{mean(x)}, but
  \texttt{median(x)} is also useful. The mean is the sum divided by the
  length; the median is a value where 50\% of \texttt{x} is above it,
  and 50\% is below it.

  It's sometimes useful to combine aggregation with logical subsetting.
  We haven't talked about this sort of subsetting yet, but you'll learn
  more about it in \protect\hyperlink{subsetting-1}{subsetting}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{avg_delay1 =} \KeywordTok{mean}\NormalTok{(arr_delay),}
    \DataTypeTok{avg_delay2 =} \KeywordTok{mean}\NormalTok{(arr_delay[arr_delay >}\StringTok{ }\DecValTok{0}\NormalTok{]) }\CommentTok{# the average positive delay}
  \NormalTok{)}
\CommentTok{#> Source: local data frame [365 x 5]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day avg_delay1 avg_delay2}
\CommentTok{#>   <int> <int> <int>      <dbl>      <dbl>}
\CommentTok{#> 1  2013     1     1      12.65       32.5}
\CommentTok{#> 2  2013     1     2      12.69       32.0}
\CommentTok{#> 3  2013     1     3       5.73       27.7}
\CommentTok{#> 4  2013     1     4      -1.93       28.3}
\CommentTok{#> 5  2013     1     5      -1.53       22.6}
\CommentTok{#> 6  2013     1     6       4.24       24.4}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}
\item
  Measures of spread: \texttt{sd(x)}, \texttt{IQR(x)}, \texttt{mad(x)}.
  The mean squared deviation, or standard deviation or sd for short, is
  the standard measure of spread. The interquartile range \texttt{IQR()}
  and median absolute deviation \texttt{mad(x)} are robust equivalents
  that may be more useful if you have outliers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Why is distance to some destinations more variable than to others?}
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(dest) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{distance_sd =} \KeywordTok{sd}\NormalTok{(distance)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(distance_sd))}
\CommentTok{#> # A tibble: 104 × 2}
\CommentTok{#>    dest distance_sd}
\CommentTok{#>   <chr>       <dbl>}
\CommentTok{#> 1   EGE       10.54}
\CommentTok{#> 2   SAN       10.35}
\CommentTok{#> 3   SFO       10.22}
\CommentTok{#> 4   HNL       10.00}
\CommentTok{#> 5   SEA        9.98}
\CommentTok{#> 6   LAS        9.91}
\CommentTok{#> # ... with 98 more rows}
\end{Highlighting}
\end{Shaded}
\item
  Measures of rank: \texttt{min(x)}, \texttt{quantile(x,\ 0.25)},
  \texttt{max(x)}. Quantiles are a generalisation of the median. For
  example, \texttt{quantile(x,\ 0.25)} will find a value of \texttt{x}
  that is greater than 25\% of the values, and less than the remaining
  75\%.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# When do the first and last flights leave each day?}
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{first =} \KeywordTok{min}\NormalTok{(dep_time),}
    \DataTypeTok{last =} \KeywordTok{max}\NormalTok{(dep_time)}
  \NormalTok{)}
\CommentTok{#> Source: local data frame [365 x 5]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day first  last}
\CommentTok{#>   <int> <int> <int> <int> <int>}
\CommentTok{#> 1  2013     1     1   517  2356}
\CommentTok{#> 2  2013     1     2    42  2354}
\CommentTok{#> 3  2013     1     3    32  2349}
\CommentTok{#> 4  2013     1     4    25  2358}
\CommentTok{#> 5  2013     1     5    14  2357}
\CommentTok{#> 6  2013     1     6    16  2355}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}
\item
  Measures of position: \texttt{first(x)}, \texttt{nth(x,\ 2)},
  \texttt{last(x)}. These work similarly to \texttt{x{[}1{]}},
  \texttt{x{[}2{]}}, and \texttt{x{[}length(x){]}} but let you set a
  default value if that position does not exist (i.e.~you're trying to
  get the 3rd element from a group that only has two elements). For
  example, we can find the first and last departure for each day:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{first_dep =} \KeywordTok{first}\NormalTok{(dep_time), }
    \DataTypeTok{last_dep =} \KeywordTok{last}\NormalTok{(dep_time)}
  \NormalTok{)}
\CommentTok{#> Source: local data frame [365 x 5]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day first_dep last_dep}
\CommentTok{#>   <int> <int> <int>     <int>    <int>}
\CommentTok{#> 1  2013     1     1       517     2356}
\CommentTok{#> 2  2013     1     2        42     2354}
\CommentTok{#> 3  2013     1     3        32     2349}
\CommentTok{#> 4  2013     1     4        25     2358}
\CommentTok{#> 5  2013     1     5        14     2357}
\CommentTok{#> 6  2013     1     6        16     2355}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}

  These functions are complementary to filtering on ranks. Filtering
  gives you all variables, with each observation in a separate row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{r =} \KeywordTok{min_rank}\NormalTok{(}\KeywordTok{desc}\NormalTok{(dep_time))) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(r %in%}\StringTok{ }\KeywordTok{range}\NormalTok{(r))}
\CommentTok{#> Source: local data frame [770 x 20]}
\CommentTok{#> Groups: year, month, day [365]}
\CommentTok{#> }
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      517            515         2      830}
\CommentTok{#> 2  2013     1     1     2356           2359        -3      425}
\CommentTok{#> 3  2013     1     2       42           2359        43      518}
\CommentTok{#> 4  2013     1     2     2354           2359        -5      413}
\CommentTok{#> 5  2013     1     3       32           2359        33      504}
\CommentTok{#> 6  2013     1     3     2349           2359       -10      434}
\CommentTok{#> # ... with 764 more rows, and 13 more variables: sched_arr_time <int>,}
\CommentTok{#> #   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,}
\CommentTok{#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,}
\CommentTok{#> #   minute <dbl>, time_hour <dttm>, r <int>}
\end{Highlighting}
\end{Shaded}
\item
  Counts: You've seen \texttt{n()}, which takes no arguments, and
  returns the size of the current group. To count the number of
  non-missing values, use \texttt{sum(!is.na(x))}. To count the number
  of distinct (unique) values, use \texttt{n\_distinct(x)}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Which destinations have the most carriers?}
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(dest) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{carriers =} \KeywordTok{n_distinct}\NormalTok{(carrier)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(carriers))}
\CommentTok{#> # A tibble: 104 × 2}
\CommentTok{#>    dest carriers}
\CommentTok{#>   <chr>    <int>}
\CommentTok{#> 1   ATL        7}
\CommentTok{#> 2   BOS        7}
\CommentTok{#> 3   CLT        7}
\CommentTok{#> 4   ORD        7}
\CommentTok{#> 5   TPA        7}
\CommentTok{#> 6   AUS        6}
\CommentTok{#> # ... with 98 more rows}
\end{Highlighting}
\end{Shaded}

  Counts are so useful that dplyr provides a simple helper if all you
  want is a count:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(dest)}
\CommentTok{#> # A tibble: 104 × 2}
\CommentTok{#>    dest     n}
\CommentTok{#>   <chr> <int>}
\CommentTok{#> 1   ABQ   254}
\CommentTok{#> 2   ACK   264}
\CommentTok{#> 3   ALB   418}
\CommentTok{#> 4   ANC     8}
\CommentTok{#> 5   ATL 16837}
\CommentTok{#> 6   AUS  2411}
\CommentTok{#> # ... with 98 more rows}
\end{Highlighting}
\end{Shaded}

  You can optionally provide a weight variable. For example, you could
  use this to ``count'' (sum) the total number of miles a plane flew:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(tailnum, }\DataTypeTok{wt =} \NormalTok{distance)}
\CommentTok{#> # A tibble: 4,037 × 2}
\CommentTok{#>   tailnum      n}
\CommentTok{#>     <chr>  <dbl>}
\CommentTok{#> 1  D942DN   3418}
\CommentTok{#> 2  N0EGMQ 239143}
\CommentTok{#> 3  N10156 109664}
\CommentTok{#> 4  N102UW  25722}
\CommentTok{#> 5  N103US  24619}
\CommentTok{#> 6  N104UW  24616}
\CommentTok{#> # ... with 4,031 more rows}
\end{Highlighting}
\end{Shaded}
\item
  Counts and proportions of logical values:
  \texttt{sum(x\ \textgreater{}\ 10)}, \texttt{mean(y\ ==\ 0)}. When
  used with numeric functions, \texttt{TRUE} is converted to 1 and
  \texttt{FALSE} to 0. This makes \texttt{sum()} and \texttt{mean()}
  very useful: \texttt{sum(x)} gives the number of \texttt{TRUE}s in
  \texttt{x}, and \texttt{mean(x)} gives the proportion.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# How many flights left before 5am? (these usually indicate delayed}
\CommentTok{# flights from the previous day)}
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n_early =} \KeywordTok{sum}\NormalTok{(dep_time <}\StringTok{ }\DecValTok{500}\NormalTok{))}
\CommentTok{#> Source: local data frame [365 x 4]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day n_early}
\CommentTok{#>   <int> <int> <int>   <int>}
\CommentTok{#> 1  2013     1     1       0}
\CommentTok{#> 2  2013     1     2       3}
\CommentTok{#> 3  2013     1     3       4}
\CommentTok{#> 4  2013     1     4       3}
\CommentTok{#> 5  2013     1     5       3}
\CommentTok{#> 6  2013     1     6       2}
\CommentTok{#> # ... with 359 more rows}

\CommentTok{# What proportion of flights are delayed by more than an hour?}
\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{hour_perc =} \KeywordTok{mean}\NormalTok{(arr_delay >}\StringTok{ }\DecValTok{60}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> Source: local data frame [365 x 4]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day hour_perc}
\CommentTok{#>   <int> <int> <int>     <dbl>}
\CommentTok{#> 1  2013     1     1    0.0722}
\CommentTok{#> 2  2013     1     2    0.0851}
\CommentTok{#> 3  2013     1     3    0.0567}
\CommentTok{#> 4  2013     1     4    0.0396}
\CommentTok{#> 5  2013     1     5    0.0349}
\CommentTok{#> 6  2013     1     6    0.0470}
\CommentTok{#> # ... with 359 more rows}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsection{Grouping by multiple
variables}\label{grouping-by-multiple-variables}

When you group by multiple variables, each summary peels off one level
of the grouping. That makes it easy to progressively roll up a dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(flights, year, month, day)}
\NormalTok{(per_day   <-}\StringTok{ }\KeywordTok{summarise}\NormalTok{(daily, }\DataTypeTok{flights =} \KeywordTok{n}\NormalTok{()))}
\CommentTok{#> Source: local data frame [365 x 4]}
\CommentTok{#> Groups: year, month [?]}
\CommentTok{#> }
\CommentTok{#>    year month   day flights}
\CommentTok{#>   <int> <int> <int>   <int>}
\CommentTok{#> 1  2013     1     1     842}
\CommentTok{#> 2  2013     1     2     943}
\CommentTok{#> 3  2013     1     3     914}
\CommentTok{#> 4  2013     1     4     915}
\CommentTok{#> 5  2013     1     5     720}
\CommentTok{#> 6  2013     1     6     832}
\CommentTok{#> # ... with 359 more rows}
\NormalTok{(per_month <-}\StringTok{ }\KeywordTok{summarise}\NormalTok{(per_day, }\DataTypeTok{flights =} \KeywordTok{sum}\NormalTok{(flights)))}
\CommentTok{#> Source: local data frame [12 x 3]}
\CommentTok{#> Groups: year [?]}
\CommentTok{#> }
\CommentTok{#>    year month flights}
\CommentTok{#>   <int> <int>   <int>}
\CommentTok{#> 1  2013     1   27004}
\CommentTok{#> 2  2013     2   24951}
\CommentTok{#> 3  2013     3   28834}
\CommentTok{#> 4  2013     4   28330}
\CommentTok{#> 5  2013     5   28796}
\CommentTok{#> 6  2013     6   28243}
\CommentTok{#> # ... with 6 more rows}
\NormalTok{(per_year  <-}\StringTok{ }\KeywordTok{summarise}\NormalTok{(per_month, }\DataTypeTok{flights =} \KeywordTok{sum}\NormalTok{(flights)))}
\CommentTok{#> # A tibble: 1 × 2}
\CommentTok{#>    year flights}
\CommentTok{#>   <int>   <int>}
\CommentTok{#> 1  2013  336776}
\end{Highlighting}
\end{Shaded}

Be careful when progressively rolling up summaries: it's OK for sums and
counts, but you need to think about weighting means and variances, and
it's not possible to do it exactly for rank-based statistics like the
median. In other words, the sum of groupwise sums is the overall sum,
but the median of groupwise medians is not the overall median.

\subsection{Ungrouping}\label{ungrouping}

If you need to remove grouping, and return to operations on ungrouped
data, use \texttt{ungroup()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() %>%}\StringTok{             }\CommentTok{# no longer grouped by date}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{flights =} \KeywordTok{n}\NormalTok{())  }\CommentTok{# all flights}
\CommentTok{#> # A tibble: 1 × 1}
\CommentTok{#>   flights}
\CommentTok{#>     <int>}
\CommentTok{#> 1  336776}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Brainstorm at least 5 different ways to assess the typical delay
  characteristics of a group of flights. Consider the following
  scenarios:

  \begin{itemize}
  \item
    A flight is 15 minutes early 50\% of the time, and 15 minutes late
    50\% of the time.
  \item
    A flight is always 10 minutes late.
  \item
    A flight is 30 minutes early 50\% of the time, and 30 minutes late
    50\% of the time.
  \item
    99\% of the time a flight is on time. 1\% of the time it's 2 hours
    late.
  \end{itemize}

  Which is more important: arrival delay or departure delay?
\item
  Come up with another approach that will give you the same output as
  \texttt{not\_cancelled\ \%\textgreater{}\%\ count(dest)} and
  \texttt{not\_cancelled\ \%\textgreater{}\%\ count(tailnum,\ wt\ =\ distance)}
  (without using \texttt{count()}).
\item
  Our definition of cancelled flights
  (\texttt{is.na(dep\_delay)\ \textbar{}\ is.na(arr\_delay)} ) is
  slightly suboptimal. Why? Which is the most important column?
\item
  Look at the number of cancelled flights per day. Is there a pattern?
  Is the proportion of cancelled flights related to the average delay?
\item
  Which carrier has the worst delays? Challenge: can you disentangle the
  effects of bad airports vs.~bad carriers? Why/why not? (Hint: think
  about
  \texttt{flights\ \%\textgreater{}\%\ group\_by(carrier,\ dest)\ \%\textgreater{}\%\ summarise(n())})
\item
  For each plane, count the number of flights before the first delay of
  greater than 1 hour.
\item
  What does the \texttt{sort} argument to \texttt{count()} do. When
  might you use it?
\end{enumerate}

\section{Grouped mutates (and
filters)}\label{grouped-mutates-and-filters}

Grouping is most useful in conjunction with \texttt{summarise()}, but
you can also do convenient operations with \texttt{mutate()} and
\texttt{filter()}:

\begin{itemize}
\item
  Find the worst members of each group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_sml %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{rank}\NormalTok{(}\KeywordTok{desc}\NormalTok{(arr_delay)) <}\StringTok{ }\DecValTok{10}\NormalTok{)}
\CommentTok{#> Source: local data frame [3,306 x 7]}
\CommentTok{#> Groups: year, month, day [365]}
\CommentTok{#> }
\CommentTok{#>    year month   day dep_delay arr_delay distance air_time}
\CommentTok{#>   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl>}
\CommentTok{#> 1  2013     1     1       853       851      184       41}
\CommentTok{#> 2  2013     1     1       290       338     1134      213}
\CommentTok{#> 3  2013     1     1       260       263      266       46}
\CommentTok{#> 4  2013     1     1       157       174      213       60}
\CommentTok{#> 5  2013     1     1       216       222      708      121}
\CommentTok{#> 6  2013     1     1       255       250      589      115}
\CommentTok{#> # ... with 3,300 more rows}
\end{Highlighting}
\end{Shaded}
\item
  Find all groups bigger than a threshold:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{popular_dests <-}\StringTok{ }\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(dest) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{n}\NormalTok{() >}\StringTok{ }\DecValTok{365}\NormalTok{)}
\NormalTok{popular_dests}
\CommentTok{#> Source: local data frame [332,577 x 19]}
\CommentTok{#> Groups: dest [77]}
\CommentTok{#> }
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      517            515         2      830}
\CommentTok{#> 2  2013     1     1      533            529         4      850}
\CommentTok{#> 3  2013     1     1      542            540         2      923}
\CommentTok{#> 4  2013     1     1      544            545        -1     1004}
\CommentTok{#> 5  2013     1     1      554            600        -6      812}
\CommentTok{#> 6  2013     1     1      554            558        -4      740}
\CommentTok{#> # ... with 3.326e+05 more rows, and 12 more variables:}
\CommentTok{#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,}
\CommentTok{#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}
\item
  Standardise to compute per group metrics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{popular_dests %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(arr_delay >}\StringTok{ }\DecValTok{0}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop_delay =} \NormalTok{arr_delay /}\StringTok{ }\KeywordTok{sum}\NormalTok{(arr_delay)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(year:day, dest, arr_delay, prop_delay)}
\CommentTok{#> Source: local data frame [131,106 x 6]}
\CommentTok{#> Groups: dest [77]}
\CommentTok{#> }
\CommentTok{#>    year month   day  dest arr_delay prop_delay}
\CommentTok{#>   <int> <int> <int> <chr>     <dbl>      <dbl>}
\CommentTok{#> 1  2013     1     1   IAH        11   1.11e-04}
\CommentTok{#> 2  2013     1     1   IAH        20   2.01e-04}
\CommentTok{#> 3  2013     1     1   MIA        33   2.35e-04}
\CommentTok{#> 4  2013     1     1   ORD        12   4.24e-05}
\CommentTok{#> 5  2013     1     1   FLL        19   9.38e-05}
\CommentTok{#> 6  2013     1     1   ORD         8   2.83e-05}
\CommentTok{#> # ... with 1.311e+05 more rows}
\end{Highlighting}
\end{Shaded}
\end{itemize}

A grouped filter is a grouped mutate followed by an ungrouped filter. I
generally avoid them except for quick and dirty manipulations: otherwise
it's hard to check that you've done the manipulation correctly.

Functions that work most naturally in grouped mutates and filters are
known as window functions (vs.~the summary functions used for
summaries). You can learn more about useful window functions in the
corresponding vignette: \texttt{vignette("window-functions")}.

\subsection{Exercises}\label{exercises-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Refer back to the table of useful mutate and filtering functions.
  Describe how each operation changes when you combine it with grouping.
\item
  Which plane (\texttt{tailnum}) has the worst on-time record?
\item
  What time of day should you fly if you want to avoid delays as much as
  possible?
\item
  For each destination, compute the total minutes of delay. For each,
  flight, compute the proportion of the total delay for its destination.
\item
  Delays are typically temporally correlated: even once the problem that
  caused the initial delay has been resolved, later flights are delayed
  to allow earlier flights to leave. Using \texttt{lag()} explore how
  the delay of a flight is related to the delay of the immediately
  preceding flight.
\item
  Look at each destination. Can you find flights that are suspiciously
  fast? (i.e.~flights that represent a potential data entry error).
  Compute the air time a flight relative to the shortest flight to that
  destination. Which flights were most delayed in the air?
\item
  Find all destinations that are flown by at least two carriers. Use
  that information to rank the carriers.
\end{enumerate}

\hypertarget{workflow-scripts}{\chapter{Workflow:
scripts}\label{workflow-scripts}}

So far you've been using the console to run code. That's a great place
to start, but you'll find it gets cramped pretty quickly as you create
more complex ggplot2 graphics and dplyr pipes. To give yourself more
room to work, it's a great idea to use the script editor. Open it up
either by clicking the File menu, and selecting New File, then R script,
or using the keyboard shortcut Cmd/Ctrl + Shift + N. Now you'll see four
panes:

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/rstudio-editor} \end{center}

The script editor is a great place to put code you care about. Keep
experimenting in the console, but once you have written code that works
and does what you want, put it in the script editor. RStudio will
automatically save the contents of the editor when you quit RStudio, and
will automatically load it when you re-open. Nevertheless, it's a good
idea to save your scripts regularly and to back them up.

\section{Running code}\label{running-code}

The script editor is also a great place to build up complex ggplot2
plots or long sequences of dplyr manipulations. The key to using the
script editor effectively is to memorise one of the most important
keyboard shortcuts: Cmd/Ctrl + Enter. This executes the current R
expression in the console. For example, take the code below. If your
cursor is at ?, pressing Cmd/Ctrl + Enter will run the complete command
that generates \texttt{not\_cancelled}. It will also move the cursor to
the next statement (beginning with
\texttt{not\_cancelled\ \%\textgreater{}\%}). That makes it easy to run
your complete script by repeatedly pressing Cmd/Ctrl + Enter.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(nycflights13)}

\NormalTok{not_cancelled <-}\StringTok{ }\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(!}\KeywordTok{is.na}\NormalTok{(dep_delay)?, !}\KeywordTok{is.na}\NormalTok{(arr_delay))}

\NormalTok{not_cancelled %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year, month, day) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dep_delay))}
\end{Highlighting}
\end{Shaded}

Instead of running expression-by-expression, you can also execute the
complete script in one step: Cmd/Ctrl + Shift + S. Doing this regularly
is a great way to check that you've captured all the important parts of
your code in the script.

I recommend that you always start your script with the packages that you
need. That way, if you share your code with others, they can easily see
what packages they need to install. Note, however, that you should never
include \texttt{install.packages()} or \texttt{setwd()} in a script that
you share. It's very antisocial to change settings on someone else's
computer!

When working through future chapters, I highly recommend starting in the
editor and practicing your keyboard shortcuts. Over time, sending code
to the console in this way will become so natural that you won't even
think about it.

\section{RStudio diagnostics}\label{rstudio-diagnostics}

The script editor will also highlight syntax errors with a red squiggly
line and a cross in the sidebar:

\begin{center}\includegraphics{screenshots/rstudio-diagnostic} \end{center}

Hover over the cross to see what the problem is:

\begin{center}\includegraphics{screenshots/rstudio-diagnostic-tip} \end{center}

RStudio will also let you know about potential problems:

\begin{center}\includegraphics{screenshots/rstudio-diagnostic-warn} \end{center}

\section{Practice}\label{practice-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Go to the RStudio Tips twitter account,
  \url{https://twitter.com/rstudiotips} and find one tip that looks
  interesting. Practice using it!
\item
  What other common mistakes will RStudio diagnostics report? Read
  \url{https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics}
  to find out.
\end{enumerate}

\hypertarget{exploratory-data-analysis}{\chapter{Exploratory Data
Analysis}\label{exploratory-data-analysis}}

\section{Introduction}\label{introduction-3}

This chapter will show you how to use visualisation and transformation
to explore your data in a systematic way, a task that statisticians call
exploratory data analysis, or EDA for short. EDA is an iterative cycle.
You:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate questions about your data.
\item
  Search for answers by visualising, transforming, and modelling your
  data.
\item
  Use what you learn to refine your questions and/or generate new
  questions.
\end{enumerate}

EDA is not a formal process with a strict set of rules. More than
anything, EDA is a state of mind. During the initial phases of EDA you
should feel free to investigate every idea that occurs to you. Some of
these ideas will pan out, and some will be dead ends. As your
exploration continues, you will hone in on a few particularly productive
areas that you'll eventually write up and communicate to others.

EDA is an important part of any data analysis, even if the questions are
handed to you on a platter, because you always need to investigate the
quality of your data. Data cleaning is just one application of EDA: you
ask questions about whether your data meets your expectations or not. To
do data cleaning, you'll need to deploy all the tools of EDA:
visualisation, transformation, and modelling.

\subsection{Prerequisites}\label{prerequisites-3}

In this chapter we'll combine what you've learned about dplyr and
ggplot2 to interactively ask questions, answer them with data, and then
ask new questions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{Questions}\label{questions}

\begin{quote}
``There are no routine statistical questions, only questionable
statistical routines.'' --- Sir David Cox
\end{quote}

\begin{quote}
``Far better an approximate answer to the right question, which is often
vague, than an exact answer to the wrong question, which can always be
made precise.'' --- John Tukey
\end{quote}

Your goal during EDA is to develop an understanding of your data. The
easiest way to do this is to use questions as tools to guide your
investigation. When you ask a question, the question focuses your
attention on a specific part of your dataset and helps you decide which
graphs, models, or transformations to make.

EDA is fundamentally a creative process. And like most creative
processes, the key to asking \emph{quality} questions is to generate a
large \emph{quantity} of questions. It is difficult to ask revealing
questions at the start of your analysis because you do not know what
insights are contained in your dataset. On the other hand, each new
question that you ask will expose you to a new aspect of your data and
increase your chance of making a discovery. You can quickly drill down
into the most interesting parts of your data---and develop a set of
thought-provoking questions---if you follow up each question with a new
question based on what you find.

There is no rule about which questions you should ask to guide your
research. However, two types of questions will always be useful for
making discoveries within your data. You can loosely word these
questions as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What type of variation occurs within my variables?
\item
  What type of covariation occurs between my variables?
\end{enumerate}

The rest of this chapter will look at these two questions. I'll explain
what variation and covariation are, and I'll show you several ways to
answer each question. To make the discussion easier, let's define some
terms:

\begin{itemize}
\item
  A \textbf{variable} is a quantity, quality, or property that you can
  measure.
\item
  A \textbf{value} is the state of a variable when you measure it. The
  value of a variable may change from measurement to measurement.
\item
  An \textbf{observation} is a set of measurements made under similar
  conditions (you usually make all of the measurements in an observation
  at the same time and on the same object). An observation will contain
  several values, each associated with a different variable. I'll
  sometimes refer to an observation as a data point.
\item
  \textbf{Tabular data} is a set of values, each associated with a
  variable and an observation. Tabular data is \emph{tidy} if each value
  is placed in its own ``cell'', each variable in its own column, and
  each observation in its own row.
\end{itemize}

So far, all of the data that you've seen has been tidy. In real-life,
most data isn't tidy, so we'll come back to these ideas again in
\protect\hyperlink{tidy-data-1}{tidy data}.

\section{Variation}\label{variation}

\textbf{Variation} is the tendency of the values of a variable to change
from measurement to measurement. You can see variation easily in real
life; if you measure any continuous variable twice, you will get two
different results. This is true even if you measure quantities that are
constant, like the speed of light. Each of your measurements will
include a small amount of error that varies from measurement to
measurement. Categorical variables can also vary if you measure across
different subjects (e.g.~the eye colors of different people), or
different times (e.g.~the energy levels of an electron at different
moments). Every variable has its own pattern of variation, which can
reveal interesting information. The best way to understand that pattern
is to visualise the distribution of the variable's values.

\subsection{Visualising distributions}\label{visualising-distributions}

How you visualise the distribution of a variable will depend on whether
the variable is categorical or continuous. A variable is
\textbf{categorical} if it can only take one of a small set of values.
In R, categorical variables are usually saved as factors or character
vectors. To examine the distribution of a categorical variable, use a
bar chart:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-2-1} \end{center}

The height of the bars displays how many observations occurred with each
x value. You can compute these values manually with
\texttt{dplyr::count()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(cut)}
\CommentTok{#> # A tibble: 5 × 2}
\CommentTok{#>         cut     n}
\CommentTok{#>       <ord> <int>}
\CommentTok{#> 1      Fair  1610}
\CommentTok{#> 2      Good  4906}
\CommentTok{#> 3 Very Good 12082}
\CommentTok{#> 4   Premium 13791}
\CommentTok{#> 5     Ideal 21551}
\end{Highlighting}
\end{Shaded}

A variable is \textbf{continuous} if it can take any of an infinite set
of ordered values. Numbers and date-times are two examples of continuous
variables. To examine the distribution of a continuous variable, use a
histogram:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat), }\DataTypeTok{binwidth =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-4-1} \end{center}

You can compute this by hand by combining \texttt{dplyr::count()} and
\texttt{ggplot2::cut\_width()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(}\KeywordTok{cut_width}\NormalTok{(carat, }\FloatTok{0.5}\NormalTok{))}
\CommentTok{#> # A tibble: 11 × 2}
\CommentTok{#>   `cut_width(carat, 0.5)`     n}
\CommentTok{#>                    <fctr> <int>}
\CommentTok{#> 1            [-0.25,0.25]   785}
\CommentTok{#> 2             (0.25,0.75] 29498}
\CommentTok{#> 3             (0.75,1.25] 15977}
\CommentTok{#> 4             (1.25,1.75]  5313}
\CommentTok{#> 5             (1.75,2.25]  2002}
\CommentTok{#> 6             (2.25,2.75]   322}
\CommentTok{#> # ... with 5 more rows}
\end{Highlighting}
\end{Shaded}

A histogram divides the x-axis into equally spaced bins and then uses
the height of a bar to display the number of observations that fall in
each bin. In the graph above, the tallest bar shows that almost 30,000
observations have a \texttt{carat} value between 0.25 and 0.75, which
are the left and right edges of the bar.

You can set the width of the intervals in a histogram with the
\texttt{binwidth} argument, which is measured in the units of the
\texttt{x} variable. You should always explore a variety of binwidths
when working with histograms, as different binwidths can reveal
different patterns. For example, here is how the graph above looks when
we zoom into just the diamonds with a size of less than three carats and
choose a smaller binwidth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smaller <-}\StringTok{ }\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(carat <}\StringTok{ }\DecValTok{3}\NormalTok{)}
  
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{smaller, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat)) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-6-1} \end{center}

If you wish to overlay multiple histograms in the same plot, I recommend
using \texttt{geom\_freqpoly()} instead of \texttt{geom\_histogram()}.
\texttt{geom\_freqpoly()} performs the same calculation as
\texttt{geom\_histogram()}, but instead of displaying the counts with
bars, uses lines instead. It's much easier to understand overlapping
lines than bars.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{smaller, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{colour =} \NormalTok{cut)) +}
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-7-1} \end{center}

There are a few challenges with this type of plot, which we will come
back to in \protect\hyperlink{cat-cont}{visualising a categorical and a
continuous variable}.

Now that you can visualise variation, what should you look for in your
plots? And what type of follow-up questions should you ask? I've put
together a list below of the most useful types of information that you
will find in your graphs, along with some follow-up questions for each
type of information. The key to asking good follow-up questions will be
to rely on your curiosity (What do you want to learn more about?) as
well as your skepticism (How could this be misleading?).

\subsection{Typical values}\label{typical-values}

In both bar charts and histograms, tall bars show the common values of a
variable, and shorter bars show less-common values. Places that do not
have bars reveal values that were not seen in your data. To turn this
information into useful questions, look for anything unexpected:

\begin{itemize}
\item
  Which values are the most common? Why?
\item
  Which values are rare? Why? Does that match your expectations?
\item
  Can you see any unusual patterns? What might explain them?
\end{itemize}

As an example, the histogram below suggests several interesting
questions:

\begin{itemize}
\item
  Why are there more diamonds at whole carats and common fractions of
  carats?
\item
  Why are there more diamonds slightly to the right of each peak than
  there are slightly to the left of each peak?
\item
  Why are there no diamonds bigger than 3 carats?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{smaller, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat)) +}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-8-1} \end{center}

Clusters of similar values suggest that subgroups exist in your data. To
understand the subgroups, ask:

\begin{itemize}
\item
  How are the observations within each cluster similar to each other?
\item
  How are the observations in separate clusters different from each
  other?
\item
  How can you explain or describe the clusters?
\item
  Why might the appearance of clusters be misleading?
\end{itemize}

The histogram below shows the length (in minutes) of 272 eruptions of
the Old Faithful Geyser in Yellowstone National Park. Eruption times
appear to be clustered into two groups: there are short eruptions (of
around 2 minutes) and long eruptions (4-5 minutes), but little in
between.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{faithful, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{eruptions)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-9-1} \end{center}

Many of the questions above will prompt you to explore a relationship
\emph{between} variables, for example, to see if the values of one
variable can explain the behavior of another variable. We'll get to that
shortly.

\subsection{Unusual values}\label{unusual-values}

Outliers are observations that are unusual; data points that don't seem
to fit the pattern. Sometimes outliers are data entry errors; other
times outliers suggest important new science. When you have a lot of
data, outliers are sometimes difficult to see in a histogram. For
example, take the distribution of the \texttt{y} variable from the
diamonds dataset. The only evidence of outliers is the unusually wide
limits on the y-axis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{y), }\DataTypeTok{binwidth =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-10-1} \end{center}

There are so many observations in the common bins that the rare bins are
so short that you can't see them (although maybe if you stare intently
at 0 you'll spot something). To make it easy to see the unusual values,
we need to zoom into to small values of the y-axis with
\texttt{coord\_cartesian()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{y), }\DataTypeTok{binwidth =} \FloatTok{0.5}\NormalTok{) +}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-11-1} \end{center}

(\texttt{coord\_cartesian()} also has an \texttt{xlim()} argument for
when you need to zoom into the x-axis. ggplot2 also has \texttt{xlim()}
and \texttt{ylim()} functions that work slightly differently: they throw
away the data outside the limits.)

This allows us to see that there are three unusual values: 0,
\textasciitilde{}30, and \textasciitilde{}60. We pluck them out with
dplyr:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unusual <-}\StringTok{ }\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(y <}\StringTok{ }\DecValTok{3} \NormalTok{|}\StringTok{ }\NormalTok{y >}\StringTok{ }\DecValTok{20}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(y)}
\NormalTok{unusual}
\CommentTok{#> # A tibble: 9 × 10}
\CommentTok{#>   carat       cut color clarity depth table price     x     y     z}
\CommentTok{#>   <dbl>     <ord> <ord>   <ord> <dbl> <dbl> <int> <dbl> <dbl> <dbl>}
\CommentTok{#> 1  1.00 Very Good     H     VS2  63.3    53  5139  0.00   0.0  0.00}
\CommentTok{#> 2  1.14      Fair     G     VS1  57.5    67  6381  0.00   0.0  0.00}
\CommentTok{#> 3  1.56     Ideal     G     VS2  62.2    54 12800  0.00   0.0  0.00}
\CommentTok{#> 4  1.20   Premium     D    VVS1  62.1    59 15686  0.00   0.0  0.00}
\CommentTok{#> 5  2.25   Premium     H     SI2  62.8    59 18034  0.00   0.0  0.00}
\CommentTok{#> 6  0.71      Good     F     SI2  64.1    60  2130  0.00   0.0  0.00}
\CommentTok{#> 7  0.71      Good     F     SI2  64.1    60  2130  0.00   0.0  0.00}
\CommentTok{#> 8  0.51     Ideal     E     VS1  61.8    55  2075  5.15  31.8  5.12}
\CommentTok{#> 9  2.00   Premium     H     SI2  58.9    57 12210  8.09  58.9  8.06}
\end{Highlighting}
\end{Shaded}

The \texttt{y} variable measures one of the three dimensions of these
diamonds, in mm. We know that diamonds can't have a width of 0mm, so
these values must be incorrect. We might also suspect that measurements
of 32mm and 59mm are implausible: those diamonds are over an inch long,
but don't cost hundreds of thousands of dollars!

It's good practice to repeat your analysis with and without the
outliers. If they have minimal effect on the results, and you can't
figure out why they're there, it's reasonable to replace them with
missing values, and move on. However, if they have a substantial effect
on your results, you shouldn't drop them without justification. You'll
need to figure out what caused them (e.g.~a data entry error) and
disclose that you removed them in your write-up.

\subsection{Exercises}\label{exercises-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explore the distribution of each of the \texttt{x}, \texttt{y}, and
  \texttt{z} variables in \texttt{diamonds}. What do you learn? Think
  about a diamond and how you might decide which dimension is the
  length, width, and depth.
\item
  Explore the distribution of \texttt{price}. Do you discover anything
  unusual or surprising? (Hint: Carefully think about the
  \texttt{binwidth} and make sure you try a wide range of values.)
\item
  How many diamonds are 0.99 carat? How many are 1 carat? What do you
  think is the cause of the difference?
\item
  Compare and contrast \texttt{coord\_cartesian()} vs \texttt{xlim()} or
  \texttt{ylim()} when zooming in on a histogram. What happens if you
  leave \texttt{binwidth} unset? What happens if you try and zoom so
  only half a bar shows?
\end{enumerate}

\section{Missing values}\label{missing-values-2}

If you've encountered unusual values in your dataset, and simply want to
move on to the rest of your analysis, you have two options.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Drop the entire row with the strange values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{between}\NormalTok{(y, }\DecValTok{3}\NormalTok{, }\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

  I don't recommend this option because just because one measurement is
  invalid, doesn't mean all the measurements are. Additionally, if you
  have low quality data, by time that you've applied this approach to
  every variable you might find that you don't have any data left!
\item
  Instead, I recommend replacing the unusual values with missing values.
  The easiest way to do this is to use \texttt{mutate()} to replace the
  variable with a modified copy. You can use the \texttt{ifelse()}
  function to replace unusual values with \texttt{NA}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{y =} \KeywordTok{ifelse}\NormalTok{(y <}\StringTok{ }\DecValTok{3} \NormalTok{|}\StringTok{ }\NormalTok{y >}\StringTok{ }\DecValTok{20}\NormalTok{, }\OtherTok{NA}\NormalTok{, y))}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\texttt{ifelse()} has three arguments. The first argument \texttt{test}
should be a logical vector. The result will contain the value of the
second argument, \texttt{yes}, when \texttt{test} is \texttt{TRUE}, and
the value of the third argument, \texttt{no}, when it is false.

Like R, ggplot2 subscribes to the philosophy that missing values should
never silently go missing. It's not obvious where you should plot
missing values, so ggplot2 doesn't include them in the plot, but it does
warn that they've been removed:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds2, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{x, }\DataTypeTok{y =} \NormalTok{y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\CommentTok{#> Warning: Removed 9 rows containing missing values (geom_point).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-17-1} \end{center}

To suppress that warning, set \texttt{na.rm\ =\ TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds2, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{x, }\DataTypeTok{y =} \NormalTok{y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Other times you want to understand what makes observations with missing
values different to observations with recorded values. For example, in
\texttt{nycflights13::flights}, missing values in the \texttt{dep\_time}
variable indicate that the flight was cancelled. So you might want to
compare the scheduled departure times for cancelled and non-cancelled
times. You can do by making a new variable with \texttt{is.na()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nycflights13::flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{cancelled =} \KeywordTok{is.na}\NormalTok{(dep_time),}
    \DataTypeTok{sched_hour =} \NormalTok{sched_dep_time %/%}\StringTok{ }\DecValTok{100}\NormalTok{,}
    \DataTypeTok{sched_min =} \NormalTok{sched_dep_time %%}\StringTok{ }\DecValTok{100}\NormalTok{,}
    \DataTypeTok{sched_dep_time =} \NormalTok{sched_hour +}\StringTok{ }\NormalTok{sched_min /}\StringTok{ }\DecValTok{60}
  \NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(sched_dep_time)) +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{cancelled), }\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{/}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-19-1} \end{center}

However this plot isn't great because there are many more non-cancelled
flights than cancelled flights. In the next section we'll explore some
techniques for improving this comparison.

\subsection{Exercises}\label{exercises-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What happens to missing values in a histogram? What happens to missing
  values in a bar chart? Why is there a difference?
\item
  What does \texttt{na.rm\ =\ TRUE} do in \texttt{mean()} and
  \texttt{sum()}?
\end{enumerate}

\section{Covariation}\label{covariation}

If variation describes the behavior \emph{within} a variable,
covariation describes the behavior \emph{between} variables.
\textbf{Covariation} is the tendency for the values of two or more
variables to vary together in a related way. The best way to spot
covariation is to visualise the relationship between two or more
variables. How you do that should again depend on the type of variables
involved.

\hypertarget{cat-cont}{\subsection{A categorical and continuous
variable}\label{cat-cont}}

It's common to want to explore the distribution of a continuous variable
broken down by a categorical variable, as in the previous frequency
polygon. The default appearance of \texttt{geom\_freqpoly()} is not that
useful for that sort of comparison because the height is given by the
count. That means if one of the groups is much smaller than the others,
it's hard to see the differences in shape. For example, let's explore
how the price of a diamond varies with its quality:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{price)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{cut), }\DataTypeTok{binwidth =} \DecValTok{500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-20-1} \end{center}

It's hard to see the difference in distribution because the overall
counts differ so much:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-21-1} \end{center}

To make the comparison easier we need to swap what is displayed on the
y-axis. Instead of displaying count, we'll display \textbf{density},
which is the count standardised so that the area under each frequency
polygon is one.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{price, }\DataTypeTok{y =} \NormalTok{..density..)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{cut), }\DataTypeTok{binwidth =} \DecValTok{500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-22-1} \end{center}

There's something rather surprising about this plot - it appears that
fair diamonds (the lowest quality) have the highest average price! But
maybe that's because frequency polygons are a little hard to interpret -
there's a lot going on in this plot.

Another alternative to display the distribution of a continuous variable
broken down by a categorical variable is the boxplot. A \textbf{boxplot}
is a type of visual shorthand for a distribution of values that is
popular among statisticians. Each boxplot consists of:

\begin{itemize}
\item
  A box that stretches from the 25th percentile of the distribution to
  the 75th percentile, a distance known as the interquartile range
  (IQR). In the middle of the box is a line that displays the median,
  i.e.~50th percentile, of the distribution. These three lines give you
  a sense of the spread of the distribution and whether or not the
  distribution is symmetric about the median or skewed to one side.
\item
  Visual points that display observations that fall more than 1.5 times
  the IQR from either edge of the box. These outlying points are unusual
  so are plotted individually.
\item
  A line (or whisker) that extends from each end of the box and goes to
  the\\
  farthest non-outlier point in the distribution.
\end{itemize}

\begin{center}\includegraphics[width=1\linewidth]{images/EDA-boxplot} \end{center}

Let's take a look at the distribution of price by cut using
\texttt{geom\_boxplot()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{y =} \NormalTok{price)) +}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-24-1} \end{center}

We see much less information about the distribution, but the boxplots
are much more compact so we can more easily compare them (and fit more
on one plot). It supports the counterintuitive finding that better
quality diamonds are cheaper on average! In the exercises, you'll be
challenged to figure out why.

\texttt{cut} is an ordered factor: fair is worse than good, which is
worse than very good and so on. Many categorical variables don't have
such an intrinsic order, so you might want to reorder them to make a
more informative display. One way to do that is with the
\texttt{reorder()} function.

For example, take the \texttt{class} variable in the \texttt{mpg}
dataset. You might be interested to know how highway mileage varies
across classes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{class, }\DataTypeTok{y =} \NormalTok{hwy)) +}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-25-1} \end{center}

To make the trend easier to see, we can reorder \texttt{class} based on
the median value of \texttt{hwy}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(class, hwy, }\DataTypeTok{FUN =} \NormalTok{median), }\DataTypeTok{y =} \NormalTok{hwy))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-26-1} \end{center}

If you have long variable names, \texttt{geom\_boxplot()} will work
better if you flip it 90°. You can do that with \texttt{coord\_flip()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{mpg) +}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(class, hwy, }\DataTypeTok{FUN =} \NormalTok{median), }\DataTypeTok{y =} \NormalTok{hwy)) +}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-27-1} \end{center}

\subsubsection{Exercises}\label{exercises-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use what you've learned to improve the visualisation of the departure
  times of cancelled vs.~non-cancelled flights.
\item
  What variable in the diamonds dataset is most important for predicting
  the price of a diamond? How is that variable correlated with cut? Why
  does the combination of those two relationships lead to lower quality
  diamonds being more expensive?
\item
  Install the ggstance package, and create a horizontal boxplot. How
  does this compare to using \texttt{coord\_flip()}?
\item
  One problem with boxplots is that they were developed in an era of
  much smaller datasets and tend to display a prohibitively large number
  of ``outlying values''. One approach to remedy this problem is the
  letter value plot. Install the lvplot package, and try using
  \texttt{geom\_lv()} to display the distribution of price vs cut. What
  do you learn? How do you interpret the plots?
\item
  Compare and contrast \texttt{geom\_violin()} with a facetted
  \texttt{geom\_histogram()}, or a coloured \texttt{geom\_freqpoly()}.
  What are the pros and cons of each method?
\item
  If you have a small dataset, it's sometimes useful to use
  \texttt{geom\_jitter()} to see the relationship between a continuous
  and categorical variable. The ggbeeswarm package provides a number of
  methods similar to \texttt{geom\_jitter()}. List them and briefly
  describe what each one does.
\end{enumerate}

\subsection{Two categorical variables}\label{two-categorical-variables}

To visualise the covariation between categorical variables, you'll need
to count the number of observations for each combination. One way to do
that is to rely on the built-in \texttt{geom\_count()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}
\StringTok{  }\KeywordTok{geom_count}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{y =} \NormalTok{color))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-28-1} \end{center}

The size of each circle in the plot displays how many observations
occurred at each combination of values. Covariation will appear as a
strong correlation between specific x values and specific y values.

Another approach is to compute the count with dplyr:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(color, cut)}
\CommentTok{#> Source: local data frame [35 x 3]}
\CommentTok{#> Groups: color [?]}
\CommentTok{#> }
\CommentTok{#>   color       cut     n}
\CommentTok{#>   <ord>     <ord> <int>}
\CommentTok{#> 1     D      Fair   163}
\CommentTok{#> 2     D      Good   662}
\CommentTok{#> 3     D Very Good  1513}
\CommentTok{#> 4     D   Premium  1603}
\CommentTok{#> 5     D     Ideal  2834}
\CommentTok{#> 6     E      Fair   224}
\CommentTok{#> # ... with 29 more rows}
\end{Highlighting}
\end{Shaded}

Then visualise with \texttt{geom\_tile()} and the fill aesthetic:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(color, cut) %>%}\StringTok{  }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{color, }\DataTypeTok{y =} \NormalTok{cut)) +}
\StringTok{    }\KeywordTok{geom_tile}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =} \NormalTok{n))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-30-1} \end{center}

If the categorical variables are unordered, you might want to use the
seriation package to simultaneously reorder the rows and columns in
order to more clearly reveal interesting patterns. For larger plots, you
might want to try the d3heatmap or heatmaply packages, which create
interactive plots.

\subsubsection{Exercises}\label{exercises-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How could you rescale the count dataset above to more clearly show the
  distribution of cut within colour, or colour within cut?
\item
  Use \texttt{geom\_tile()} together with dplyr to explore how average
  flight delays vary by destination and month of year. What makes the
  plot difficult to read? How could you improve it?
\item
  Why is it slightly better to use \texttt{aes(x\ =\ color,\ y\ =\ cut)}
  rather than \texttt{aes(x\ =\ cut,\ y\ =\ color)} in the example
  above?
\end{enumerate}

\subsection{Two continuous variables}\label{two-continuous-variables}

You've already seen one great way to visualise the covariation between
two continuous variables: draw a scatterplot with
\texttt{geom\_point()}. You can see covariation as a pattern in the
points. For example, you can see an exponential relationship between the
carat size and price of a diamond.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{y =} \NormalTok{price))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-31-1} \end{center}

Scatterplots become less useful as the size of your dataset grows,
because points begin to overplot, and pile up into areas of uniform
black (as above). You've already seen one way to fix the problem: using
the \texttt{alpha} aesthetic to add transparency.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{y =} \NormalTok{price), }\DataTypeTok{alpha =} \DecValTok{1} \NormalTok{/}\StringTok{ }\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-32-1} \end{center}

But using transparency can be challenging for very large datasets.
Another solution is to use bin. Previously you used
\texttt{geom\_histogram()} and \texttt{geom\_freqpoly()} to bin in one
dimension. Now you'll learn how to use \texttt{geom\_bin2d()} and
\texttt{geom\_hex()} to bin in two dimensions.

\texttt{geom\_bin2d()} and \texttt{geom\_hex()} divide the coordinate
plane into 2d bins and then use a fill color to display how many points
fall into each bin. \texttt{geom\_bin2d()} creates rectangular bins.
\texttt{geom\_hex()} creates hexagonal bins. You will need to install
the hexbin package to use \texttt{geom\_hex()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{smaller) +}
\StringTok{  }\KeywordTok{geom_bin2d}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{y =} \NormalTok{price))}

\CommentTok{# install.packages("hexbin")}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{smaller) +}
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{y =} \NormalTok{price))}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-33-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-33-2}

Another option is to bin one continuous variable so it acts like a
categorical variable. Then you can use one of the techniques for
visualising the combination of a categorical and a continuous variable
that you learned about. For example, you could bin \texttt{carat} and
then for each group, display a boxplot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{smaller, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{y =} \NormalTok{price)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{cut_width}\NormalTok{(carat, }\FloatTok{0.1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-34-1} \end{center}

\texttt{cut\_width(x,\ width)}, as used above, divides \texttt{x} into
bins of width \texttt{width}. By default, boxplots look roughly the same
(apart from number of outliers) regardless of how many observations
there are, so it's difficult to tell that each boxplot summarises a
different number of points. One way to show that is to make the width of
the boxplot proportional to the number of points with
\texttt{varwidth\ =\ TRUE}.

Another approach is to display approximately the same number of points
in each bin. That's the job of \texttt{cut\_number()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{smaller, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{y =} \NormalTok{price)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{cut_number}\NormalTok{(carat, }\DecValTok{20}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-35-1} \end{center}

\subsubsection{Exercises}\label{exercises-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Instead of summarising the conditional distribution with a boxplot,
  you could use a frequency polygon. What do you need to consider when
  using \texttt{cut\_width()} vs \texttt{cut\_number()}? How does that
  impact a visualisation of the 2d distribution of \texttt{carat} and
  \texttt{price}?
\item
  Visualise the distribution of carat, partitioned by price.
\item
  How does the price distribution of very large diamonds compare to
  small diamonds. Is it as you expect, or does it surprise you?
\item
  Combine two of the techniques you've learned to visualise the combined
  distribution of cut, carat, and price.
\item
  Two dimensional plots reveal outliers that are not visible in one
  dimensional plots. For example, some points in the plot below have an
  unusual combination of \texttt{x} and \texttt{y} values, which makes
  the points outliers even though their \texttt{x} and \texttt{y} values
  appear normal when examined separately.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{x, }\DataTypeTok{y =} \NormalTok{y)) +}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{11}\NormalTok{), }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{11}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-36-1} \end{center}

  Why is a scatterplot a better display than a binned plot for this
  case?
\end{enumerate}

\section{Patterns and models}\label{patterns-and-models}

Patterns in your data provide clues about relationships. If a systematic
relationship exists between two variables it will appear as a pattern in
the data. If you spot a pattern, ask yourself:

\begin{itemize}
\item
  Could this pattern be due to coincidence (i.e.~random chance)?
\item
  How can you describe the relationship implied by the pattern?
\item
  How strong is the relationship implied by the pattern?
\item
  What other variables might affect the relationship?
\item
  Does the relationship change if you look at individual subgroups of
  the data?
\end{itemize}

A scatterplot of Old Faithful eruption lengths versus the wait time
between eruptions shows a pattern: longer wait times are associated with
longer eruptions. The scatterplot also displays the two clusters that we
noticed above.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{faithful) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{eruptions, }\DataTypeTok{y =} \NormalTok{waiting))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-37-1} \end{center}

Patterns provide one of the most useful tools for data scientists
because they reveal covariation. If you think of variation as a
phenomenon that creates uncertainty, covariation is a phenomenon that
reduces it. If two variables covary, you can use the values of one
variable to make better predictions about the values of the second. If
the covariation is due to a causal relationship (a special case), then
you can use the value of one variable to control the value of the
second.

Models are a tool for extracting patterns out of data. For example,
consider the diamonds data. It's hard to understand the relationship
between cut and price, because cut and carat, and carat and price are
tightly related. It's possible to use a model to remove the very strong
relationship between price and carat so we can explore the subtleties
that remain. The following code fits a model that predicts
\texttt{price} from \texttt{carat} and then computes the residuals (the
difference between the predicted value and the actual value). The
residuals give us a view of the price of the diamond, once the effect of
carat has been removed.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(modelr)}

\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(price) ~}\StringTok{ }\KeywordTok{log}\NormalTok{(carat), }\DataTypeTok{data =} \NormalTok{diamonds)}

\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(mod) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{resid =} \KeywordTok{exp}\NormalTok{(resid))}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds2) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{carat, }\DataTypeTok{y =} \NormalTok{resid))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-38-1} \end{center}

Once you've removed the strong relationship between carat and price, you
can see what you expect in the relationship between cut and price:
relative to their size, better quality diamonds are more expensive.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{diamonds2) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{cut, }\DataTypeTok{y =} \NormalTok{resid))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/EDA_files/figure-latex/unnamed-chunk-39-1} \end{center}

You'll learn how models, and the modelr package, work in the final part
of the book, \protect\hyperlink{model-intro}{model}. We're saving
modelling for later because understanding what models are and how they
work is easiest once you have tools of data wrangling and programming in
hand.

\section{ggplot2 calls}\label{ggplot2-calls}

As we move on from these introductory chapters, we'll transition to a
more concise expression of ggplot2 code. So far we've been very
explicit, which is helpful when you are learning:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{faithful, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{eruptions)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Typically, the first one or two arguments to a function are so important
that you should know them by heart. The first two arguments to
\texttt{ggplot()} are \texttt{data} and \texttt{mapping}, and the first
two arguments to \texttt{aes()} are \texttt{x} and \texttt{y}. In the
remainder of the book, we won't supply those names. That saves typing,
and, by reducing the amount of boilerplate, makes it easier to see
what's different between plots. That's a really important programming
concern that we'll come back in
\protect\hyperlink{functions}{functions}.

Rewriting the previous plot more concisely yields:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(faithful, }\KeywordTok{aes}\NormalTok{(eruptions)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Sometimes we'll turn the end of a pipeline of data transformation into a
plot. Watch for the transition from \texttt{\%\textgreater{}\%} to
\texttt{+}. I wish this transition wasn't necessary but unfortunately
ggplot2 was created before the pipe was discovered.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(cut, clarity) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(clarity, cut, }\DataTypeTok{fill =} \NormalTok{n)) +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_tile}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\section{Learning more}\label{learning-more}

If you want learn more about the mechanics of ggplot2, I'd highly
recommend grabbing a copy of the ggplot2 book:
\url{https://amzn.com/331924275X}. It's been recently updated, so it
includes dplyr and tidyr code, and has much more space to explore all
the facets of visualisation. Unfortunately the book isn't generally
available for free, but if you have a connection to a university you can
probably get an electronic version for free through SpringerLink.

Another useful resource is the
\href{https://amzn.com/1449316956}{\emph{R Graphics Cookbook}} by
Winston Chang. Much of the contents are available online at
\url{http://www.cookbook-r.com/Graphs/}.

I also recommend \href{https://amzn.com/1498715230}{\emph{Graphical Data
Analysis with R}}, by Antony Unwin. This is a book-length treatment
similar to the material covered in this chapter, but has the space to go
into much greater depth.

\hypertarget{workflow-projects}{\chapter{Workflow:
projects}\label{workflow-projects}}

One day you will need to quit R, go do something else and return to your
analysis the next day. One day you will be working on multiple analyses
simultaneously that all use R and you want to keep them separate. One
day you will need to bring data from the outside world into R and send
numerical results and figures from R back out into the world. To handle
these real life situations, you need to make two decisions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What about your analysis is ``real'', i.e.~what will you save as your
  lasting record of what happened?
\item
  Where does your analysis ``live''?
\end{enumerate}

\section{What is real?}\label{what-is-real}

As a beginning R user, it's OK to consider your environment (i.e.~the
objects listed in the environment pane) ``real''. However, in the long
run, you'll be much better off if you consider your R scripts as
``real''.

With your R scripts (and your data files), you can recreate the
environment. It's much harder to recreate your R scripts from your
environment! You'll either have to retype a lot of code from memory
(making mistakes all the way) or you'll have to carefully mine your R
history.

To foster this behaviour, I highly recommend that you instruct RStudio
not to preserve your workspace between sessions:

\begin{center}\includegraphics[width=0.75\linewidth]{screenshots/rstudio-workspace} \end{center}

This will cause you some short-term pain, because now when you restart
RStudio it will not remember the results of the code that you ran last
time. But this short-term pain will save you long-term agony because it
forces you to capture all important interactions in your code. There's
nothing worse than discovering three months after the fact that you've
only stored the results of an important calculation in your workspace,
not the calculation itself in your code.

There is a great pair of keyboard shortcuts that will work together to
make sure you've captured the important parts of your code in the
editor:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Press Cmd/Ctrl + Shift + F10 to restart RStudio.
\item
  Press Cmd/Ctrl + Shift + S to rerun the current script.
\end{enumerate}

I use this pattern hundreds of times a week.

\section{Where does your analysis
live?}\label{where-does-your-analysis-live}

R has a powerful notion of the \textbf{working directory}. This is where
R looks for files that you ask it to load, and where it will put any
files that you ask it to save. RStudio shows your current working
directory at the top of the console:

\begin{center}\includegraphics[width=0.5\linewidth]{screenshots/rstudio-wd} \end{center}

And you can print this out in R code by running \texttt{getwd()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getwd}\NormalTok{()}
\CommentTok{#> [1] "/Users/hadley/Documents/r4ds/r4ds"}
\end{Highlighting}
\end{Shaded}

As a beginning R user, it's OK to let your home directory, documents
directory, or any other weird directory on your computer be R's working
directory. But you're six chapters into this book, and you're no longer
a rank beginner. Very soon now you should evolve to organising your
analytical projects into directories and, when working on a project,
setting R's working directory to the associated directory.

\textbf{I do not recommend it}, but you can also set the working
directory from within R:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setwd}\NormalTok{(}\StringTok{"/path/to/my/CoolProject"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

But you should never do this because there's a better way; a way that
also puts you on the path to managing your R work like an expert.

\section{Paths and directories}\label{paths-and-directories}

Paths and directories are a little complicated because there are two
basic styles of paths: Mac/Linux and Windows. There are three chief ways
in which they differ:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The most important difference is how you separate the components of
  the path. Mac and Linux uses slashes (e.g.
  \texttt{plots/diamonds.pdf}) and Windows uses backslashes (e.g.
  \texttt{plots\textbackslash{}diamonds.pdf}). R can work with either
  type (no matter what platform you're currently using), but
  unfortunately, backslashes mean something special to R, and to get a
  single backslash in the path, you need to type two backslashes! That
  makes life frustrating, so I recommend always using the Linux/Max
  style with forward slashes.
\item
  Absolute paths (i.e.~paths that point to the same place regardless of
  your working directory) look different. In Windows they start with a
  drive letter (e.g. \texttt{C:}) or two backslashes (e.g.
  \texttt{\textbackslash{}\textbackslash{}servername}) and in Mac/Linux
  they start with a slash ``/'' (e.g. \texttt{/users/hadley}). You
  should \textbf{never} use absolute paths in your scripts, because they
  hinder sharing: no one else will have exactly the same directory
  configuration as you.
\item
  The last minor difference is the place that \texttt{\textasciitilde{}}
  points to. \texttt{\textasciitilde{}} is a convenient shortcut to your
  home directory. Windows doesn't really have the notion of a home
  directory, so it instead points to your documents directory.
\end{enumerate}

\section{RStudio projects}\label{rstudio-projects}

R experts keep all the files associated with a project together ---
input data, R scripts, analytical results, figures. This is such a wise
and common practice that RStudio has built-in support for this via
\textbf{projects}.

Let's make a project for you to use while you're working through the
rest of this book. Click File \textgreater{} New Project, then:

\begin{center}\includegraphics[width=0.5\linewidth]{screenshots/rstudio-project-1} \end{center}

\begin{center}\includegraphics[width=0.5\linewidth]{screenshots/rstudio-project-2} \end{center}

\begin{center}\includegraphics[width=0.5\linewidth]{screenshots/rstudio-project-3} \end{center}

Call your project \texttt{r4ds} and think carefully about which
\emph{subdirectory} you put the project in. If you don't store it
somewhere sensible, it will be hard to find it in the future!

Once this process is complete, you'll get a new RStudio project just for
this book. Check that the ``home'' directory of your project is the
current working directory:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getwd}\NormalTok{()}
\CommentTok{#> [1] /Users/hadley/Documents/r4ds/r4ds}
\end{Highlighting}
\end{Shaded}

Whenever you refer to a file with a relative path it will look for it
here.

Now enter the following commands in the script editor, and save the
file, calling it ``diamonds.R''. Next, run the complete script which
will save a PDF and CSV file into your project directory. Don't worry
about the details, you'll learn them later in the book.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(carat, price)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{()}
\KeywordTok{ggsave}\NormalTok{(}\StringTok{"diamonds.pdf"}\NormalTok{)}

\KeywordTok{write_csv}\NormalTok{(diamonds, }\StringTok{"diamonds.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Quit RStudio. Inspect the folder associated with your project --- notice
the \texttt{.Rproj} file. Double-click that file to re-open the project.
Notice you get back to where you left off: it's the same working
directory and command history, and all the files you were working on are
still open. Because you followed my instructions above, you will,
however, have a completely fresh environment, guaranteeing that you're
starting with a clean slate.

In your favorite OS-specific way, search your computer for
\texttt{diamonds.pdf} and you will find the PDF (no surprise) but
\emph{also the script that created it} (\texttt{diamonds.r}). This is
huge win! One day you will want to remake a figure or just understand
where it came from. If you rigorously save figures to files \textbf{with
R code} and never with the mouse or the clipboard, you will be able to
reproduce old work with ease!

\section{Summary}\label{summary}

In summary, RStudio projects give you a solid workflow that will serve
you well in the future:

\begin{itemize}
\item
  Create an RStudio project for each data analyis project.
\item
  Keep data files there; we'll talk about loading them into R in
  \protect\hyperlink{data-import}{data import}.
\item
  Keep scripts there; edit them, run them in bits or as a whole.
\item
  Save your outputs (plots and cleaned data) there.
\item
  Only ever use relative paths, not absolute paths.
\end{itemize}

Everything you need is in one place, and cleanly separated from all the
other projects that you are working on.

\part{Wrangle}\label{part-wrangle}


\hypertarget{wrangle-intro}{\chapter{Introduction}\label{wrangle-intro}}

In this part of the book, you'll learn about data wrangling, the art of
getting your data into R in a useful form for visualisation and
modelling. Data wrangling is very important: without it you can't work
with your own data! There are three main parts to data wrangling:

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/data-science-wrangle} \end{center}

This part of the book proceeds as follows:

\begin{itemize}
\item
  In \protect\hyperlink{tibbles-1}{tibbles}, you'll learn about the
  variant of the data frame that we use in this book: the
  \textbf{tibble}. You'll learn what makes them different from regular
  data frames, and how you can construct them ``by hand''.
\item
  In \protect\hyperlink{data-import}{data import}, you'll learn how to
  get your data from disk and into R. We'll focus on plain-text
  rectangular formats, but will give you pointers to packages that help
  with other types of data.
\item
  In \protect\hyperlink{tidy-data-1}{tidy data}, you'll learn about tidy
  data, a consistent way of storing your data that makes transformation,
  visualisation, and modelling easier. You'll learn the underlying
  principles, and how to get your data into a tidy form.
\end{itemize}

Data wrangling also encompasses data transformation, which you've
already learned a little about. Now we'll focus on new skills for three
specific types of data you will frequently encounter in practice:

\begin{itemize}
\item
  \protect\hyperlink{relational-data}{Relational data} will give you
  tools for working with multiple interrelated datasets.
\item
  \protect\hyperlink{strings}{Strings} will introduce regular
  expressions, a powerful tool for manipulating strings.
\item
  \protect\hyperlink{factors-1}{Factors} are how R stores categorical
  data. They are used when a variable has a fixed set of possible
  values, or when you want to use a non-alphabetical ordering of a
  string.
\item
  \protect\hyperlink{dates-and-times}{Dates and times} will give you the
  key tools for working with dates and date-times.
\end{itemize}

\chapter{Tibbles}\label{tibbles}

\section{Introduction}\label{introduction-4}

Throughout this book we work with ``tibbles'' instead of R's traditional
\texttt{data.frame}. Tibbles \emph{are} data frames, but they tweak some
older behaviours to make life a little easier. R is an old language, and
some things that were useful 10 or 20 years ago now get in your way.
It's difficult to change base R without breaking existing code, so most
innovation occurs in packages. Here we will describe the \textbf{tibble}
package, which provides opinionated data frames that make working in the
tidyverse a little easier. In most places, I'll use the term tibble and
data frame interchangeably; when I want to draw particular attention to
R's built-in data frame, I'll call them \texttt{data.frame}s.

If this chapter leaves you wanting to learn more about tibbles, you
might enjoy \texttt{vignette("tibble")}.

\subsection{Prerequisites}\label{prerequisites-4}

In this chapter we'll explore the \textbf{tibble} package, part of the
core tidyverse.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{Creating tibbles}\label{tibbles}

Almost all of the functions that you'll use in this book produce
tibbles, as tibbles are one of the unifying features of the tidyverse.
Most other R packages use regular data frames, so you might want to
coerce a data frame to a tibble. You can do that with
\texttt{as\_tibble()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as_tibble}\NormalTok{(iris)}
\CommentTok{#> # A tibble: 150 × 5}
\CommentTok{#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{#>          <dbl>       <dbl>        <dbl>       <dbl>  <fctr>}
\CommentTok{#> 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{#> 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{#> 3          4.7         3.2          1.3         0.2  setosa}
\CommentTok{#> 4          4.6         3.1          1.5         0.2  setosa}
\CommentTok{#> 5          5.0         3.6          1.4         0.2  setosa}
\CommentTok{#> 6          5.4         3.9          1.7         0.4  setosa}
\CommentTok{#> # ... with 144 more rows}
\end{Highlighting}
\end{Shaded}

You can create a new tibble from individual vectors with
\texttt{tibble()}. \texttt{tibble()} will automatically recycle inputs
of length 1, and allows you to refer to variables that you just created,
as shown below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{, }
  \DataTypeTok{y =} \DecValTok{1}\NormalTok{, }
  \DataTypeTok{z =} \NormalTok{x ^}\StringTok{ }\DecValTok{2} \NormalTok{+}\StringTok{ }\NormalTok{y}
\NormalTok{)}
\CommentTok{#> # A tibble: 5 × 3}
\CommentTok{#>       x     y     z}
\CommentTok{#>   <int> <dbl> <dbl>}
\CommentTok{#> 1     1     1     2}
\CommentTok{#> 2     2     1     5}
\CommentTok{#> 3     3     1    10}
\CommentTok{#> 4     4     1    17}
\CommentTok{#> 5     5     1    26}
\end{Highlighting}
\end{Shaded}

If you're already familiar with \texttt{data.frame()}, note that
\texttt{tibble()} does much less: it never changes the type of the
inputs (e.g.~it never converts strings to factors!), it never changes
the names of variables, and it never creates row names.

It's possible for a tibble to have column names that are not valid R
variable names, aka \textbf{non-syntactic} names. For example, they
might not start with a letter, or they might contain unusual characters
like a space. To refer to these variables, you need to surround them
with backticks, \texttt{`}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \StringTok{`}\DataTypeTok{:)}\StringTok{`} \NormalTok{=}\StringTok{ "smile"}\NormalTok{, }
  \StringTok{`}\DataTypeTok{ }\StringTok{`} \NormalTok{=}\StringTok{ "space"}\NormalTok{,}
  \StringTok{`}\DataTypeTok{2000}\StringTok{`} \NormalTok{=}\StringTok{ "number"}
\NormalTok{)}
\NormalTok{tb}
\CommentTok{#> # A tibble: 1 × 3}
\CommentTok{#>    `:)`   ` ` `2000`}
\CommentTok{#>   <chr> <chr>  <chr>}
\CommentTok{#> 1 smile space number}
\end{Highlighting}
\end{Shaded}

You'll also need the backticks when working with these variables in
other packages, like ggplot2, dplyr, and tidyr.

Another way to create a tibble is with \texttt{tribble()}, short for
\textbf{tr}ansposed tibble. \texttt{tribble()} is customised for data
entry in code: column headings are defined by formulas (i.e.~they start
with \texttt{\textasciitilde{}}), and entries are separated by commas.
This makes it possible to lay out small amounts of data in easy to read
form.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x, ~y, ~z,}
  \CommentTok{#--|--|----}
  \StringTok{"a"}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{3.6}\NormalTok{,}
  \StringTok{"b"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{8.5}
\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>       x     y     z}
\CommentTok{#>   <chr> <dbl> <dbl>}
\CommentTok{#> 1     a     2   3.6}
\CommentTok{#> 2     b     1   8.5}
\end{Highlighting}
\end{Shaded}

I often add a comment (the line starting with \texttt{\#}), to make it
really clear where the header is.

\section{Tibbles vs.~data.frame}\label{tibbles-vs.data.frame}

There are two main differences in the usage of a tibble vs.~a classic
\texttt{data.frame}: printing and subsetting.

\subsection{Printing}\label{printing}

Tibbles have a refined print method that shows only the first 10 rows,
and all the columns that fit on screen. This makes it much easier to
work with large data. In addition to its name, each column reports its
type, a nice feature borrowed from \texttt{str()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{a =} \NormalTok{lubridate::}\KeywordTok{now}\NormalTok{() +}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\FloatTok{1e3}\NormalTok{) *}\StringTok{ }\DecValTok{86400}\NormalTok{,}
  \DataTypeTok{b =} \NormalTok{lubridate::}\KeywordTok{today}\NormalTok{() +}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\FloatTok{1e3}\NormalTok{) *}\StringTok{ }\DecValTok{30}\NormalTok{,}
  \DataTypeTok{c =} \DecValTok{1}\NormalTok{:}\FloatTok{1e3}\NormalTok{,}
  \DataTypeTok{d =} \KeywordTok{runif}\NormalTok{(}\FloatTok{1e3}\NormalTok{),}
  \DataTypeTok{e =} \KeywordTok{sample}\NormalTok{(letters, }\FloatTok{1e3}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{)}
\CommentTok{#> # A tibble: 1,000 × 5}
\CommentTok{#>                     a          b     c     d     e}
\CommentTok{#>                <dttm>     <date> <int> <dbl> <chr>}
\CommentTok{#> 1 2016-11-09 17:40:39 2016-11-16     1 0.368     h}
\CommentTok{#> 2 2016-11-10 11:45:48 2016-11-21     2 0.612     n}
\CommentTok{#> 3 2016-11-10 06:09:28 2016-12-01     3 0.415     l}
\CommentTok{#> 4 2016-11-09 19:30:45 2016-11-30     4 0.212     x}
\CommentTok{#> 5 2016-11-09 15:55:01 2016-11-27     5 0.733     a}
\CommentTok{#> 6 2016-11-10 02:55:58 2016-11-23     6 0.460     v}
\CommentTok{#> # ... with 994 more rows}
\end{Highlighting}
\end{Shaded}

Tibbles are designed so that you don't accidentally overwhelm your
console when you print large data frames. But sometimes you need more
output than the default display. There are a few options that can help.

First, you can explicitly \texttt{print()} the data frame and control
the number of rows (\texttt{n}) and the \texttt{width} of the display.
\texttt{width\ =\ Inf} will display all columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nycflights13::flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{width =} \OtherTok{Inf}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can also control the default print behaviour by setting options:

\begin{itemize}
\item
  \texttt{options(tibble.print\_max\ =\ n,\ tibble.print\_min\ =\ m)}:
  if more than \texttt{m} rows, print only \texttt{n} rows. Use
  \texttt{options(dplyr.print\_min\ =\ Inf)} to always show all rows.
\item
  Use \texttt{options(tibble.width\ =\ Inf)} to always print all
  columns, regardless of the width of the screen.
\end{itemize}

You can see a complete list of options by looking at the package help
with \texttt{package?tibble}.

A final option is to use RStudio's built-in data viewer to get a
scrollable view of the complete dataset. This is also often useful at
the end of a long chain of manipulations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nycflights13::flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{View}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsection{Subsetting}\label{subsetting}

So far all the tools you've learned have worked with complete data
frames. If you want to pull out a single variable, you need some new
tools, \texttt{\$} and \texttt{{[}{[}}. \texttt{{[}{[}} can extract by
name or position; \texttt{\$} only extracts by name but is a little less
typing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{),}
  \DataTypeTok{y =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{)}

\CommentTok{# Extract by name}
\NormalTok{df$x}
\CommentTok{#> [1] 0.434 0.395 0.548 0.762 0.254}
\NormalTok{df[[}\StringTok{"x"}\NormalTok{]]}
\CommentTok{#> [1] 0.434 0.395 0.548 0.762 0.254}

\CommentTok{# Extract by position}
\NormalTok{df[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{#> [1] 0.434 0.395 0.548 0.762 0.254}
\end{Highlighting}
\end{Shaded}

To use these in a pipe, you'll need to use the special placeholder
\texttt{.}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df %>%}\StringTok{ }\NormalTok{.$x}
\CommentTok{#> [1] 0.434 0.395 0.548 0.762 0.254}
\NormalTok{df %>%}\StringTok{ }\NormalTok{.[[}\StringTok{"x"}\NormalTok{]]}
\CommentTok{#> [1] 0.434 0.395 0.548 0.762 0.254}
\end{Highlighting}
\end{Shaded}

Compared to a \texttt{data.frame}, tibbles are more strict: they never
do partial matching, and they will generate a warning if the column you
are trying to access does not exist.

\section{Interacting with older code}\label{interacting-with-older-code}

Some older functions don't work with tibbles. If you encounter one of
these functions, use \texttt{as.data.frame()} to turn a tibble back to a
\texttt{data.frame}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(tb))}
\CommentTok{#> [1] "data.frame"}
\end{Highlighting}
\end{Shaded}

The main reason that some older functions don't work with tibble is the
\texttt{{[}} function. We don't use \texttt{{[}} much in this book
because \texttt{dplyr::filter()} and \texttt{dplyr::select()} allow you
to solve the same problems with clearer code (but you will learn a
little about it in \protect\hyperlink{vector-subsetting}{vector
subsetting}). With base R data frames, \texttt{{[}} sometimes returns a
data frame, and sometimes returns a vector. With tibbles, \texttt{{[}}
always returns another tibble.

\section{Exercises}\label{exercises-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How can you tell if an object is a tibble? (Hint: try printing
  \texttt{mtcars}, which is a regular data frame).
\item
  Compare and contrast the following operations on a \texttt{data.frame}
  and equivalent tibble. What is different? Why might the default data
  frame behaviours cause you frustration?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{abc =} \DecValTok{1}\NormalTok{, }\DataTypeTok{xyz =} \StringTok{"a"}\NormalTok{)}
\NormalTok{df$x}
\NormalTok{df[, }\StringTok{"xyz"}\NormalTok{]}
\NormalTok{df[, }\KeywordTok{c}\NormalTok{(}\StringTok{"abc"}\NormalTok{, }\StringTok{"xyz"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}
\item
  If you have the name of a variable stored in an object, e.g.
  \texttt{var\ \textless{}-\ "mpg"}, how can you extract the reference
  variable from a tibble?
\item
  Practice referring to non-syntactic names in the following data frame
  by:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Extracting the variable called \texttt{1}.
  \item
    Plotting a scatterplot of \texttt{1} vs \texttt{2}.
  \item
    Creating a new column called \texttt{3} which is \texttt{2} divided
    by \texttt{1}.
  \item
    Renaming the columns to \texttt{one}, \texttt{two} and
    \texttt{three}.
  \end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{annoying <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \StringTok{`}\DataTypeTok{1}\StringTok{`} \NormalTok{=}\StringTok{ }\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{,}
  \StringTok{`}\DataTypeTok{2}\StringTok{`} \NormalTok{=}\StringTok{ `}\DataTypeTok{1}\StringTok{`} \NormalTok{*}\StringTok{ }\DecValTok{2} \NormalTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(}\StringTok{`}\DataTypeTok{1}\StringTok{`}\NormalTok{))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  What does \texttt{tibble::enframe()} do? When might you use it?
\item
  What option controls how many additional column names are printed at
  the footer of a tibble?
\end{enumerate}

\hypertarget{data-import}{\chapter{Data import}\label{data-import}}

\section{Introduction}\label{introduction-5}

Working with data provided by R packages is a great way to learn the
tools of data science, but at some point you want to stop learning and
start working with your own data. In this chapter, you'll learn how to
read plain-text rectangular files into R. Here, we'll only scratch the
surface of data import, but many of the principles will translate to
other forms of data. We'll finish with a few pointers to packages that
are useful for other types of data.

\subsection{Prerequisites}\label{prerequisites-5}

In this chapter, you'll learn how to load flat files in R with the
\textbf{readr} package, which is part of the core tidyverse.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{Getting started}\label{getting-started}

Most of readr's functions are concerned with turning flat files into
data frames:

\begin{itemize}
\item
  \texttt{read\_csv()} reads comma delimited files,
  \texttt{read\_csv2()} reads semicolon separated files (common in
  countries where \texttt{,} is used as the decimal place),
  \texttt{read\_tsv()} reads tab delimited files, and
  \texttt{read\_delim()} reads in files with any delimiter.
\item
  \texttt{read\_fwf()} reads fixed width files. You can specify fields
  either by their widths with \texttt{fwf\_widths()} or their position
  with \texttt{fwf\_positions()}. \texttt{read\_table()} reads a common
  variation of fixed width files where columns are separated by white
  space.
\item
  \texttt{read\_log()} reads Apache style log files. (But also check out
  \href{https://github.com/Ironholds/webreadr}{webreadr} which is built
  on top of \texttt{read\_log()} and provides many more helpful tools.)
\end{itemize}

These functions all have similar syntax: once you've mastered one, you
can use the others with ease. For the rest of this chapter we'll focus
on \texttt{read\_csv()}. Not only are csv files one of the most common
forms of data storage, but once you understand \texttt{read\_csv()}, you
can easily apply your knowledge to all the other functions in readr.

The first argument to \texttt{read\_csv()} is the most important: it's
the path to the file to read.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{heights <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/heights.csv"}\NormalTok{)}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   earn = col_double(),}
\CommentTok{#>   height = col_double(),}
\CommentTok{#>   sex = col_character(),}
\CommentTok{#>   ed = col_integer(),}
\CommentTok{#>   age = col_integer(),}
\CommentTok{#>   race = col_character()}
\CommentTok{#> )}
\end{Highlighting}
\end{Shaded}

When you run \texttt{read\_csv()} it prints out a column specification
that gives the name and type of each column. That's an important part of
readr, which we'll come back to in
\protect\hyperlink{parsing-a-file}{parsing a file}.

You can also supply an inline csv file. This is useful for experimenting
with readr and for creating reproducible examples to share with others:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"a,b,c}
\StringTok{1,2,3}
\StringTok{4,5,6"}\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>       a     b     c}
\CommentTok{#>   <int> <int> <int>}
\CommentTok{#> 1     1     2     3}
\CommentTok{#> 2     4     5     6}
\end{Highlighting}
\end{Shaded}

In both cases \texttt{read\_csv()} uses the first line of the data for
the column names, which is a very common convention. There are two cases
where you might want to tweak this behaviour:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sometimes there are a few lines of metadata at the top of the file.
  You can use \texttt{skip\ =\ n} to skip the first \texttt{n} lines; or
  use \texttt{comment\ =\ "\#"} to drop all lines that start with (e.g.)
  \texttt{\#}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"The first line of metadata}
\StringTok{  The second line of metadata}
\StringTok{  x,y,z}
\StringTok{  1,2,3"}\NormalTok{, }\DataTypeTok{skip =} \DecValTok{2}\NormalTok{)}
\CommentTok{#> # A tibble: 1 × 3}
\CommentTok{#>       x     y     z}
\CommentTok{#>   <int> <int> <int>}
\CommentTok{#> 1     1     2     3}

\KeywordTok{read_csv}\NormalTok{(}\StringTok{"# A comment I want to skip}
\StringTok{  x,y,z}
\StringTok{  1,2,3"}\NormalTok{, }\DataTypeTok{comment =} \StringTok{"#"}\NormalTok{)}
\CommentTok{#> # A tibble: 1 × 3}
\CommentTok{#>       x     y     z}
\CommentTok{#>   <int> <int> <int>}
\CommentTok{#> 1     1     2     3}
\end{Highlighting}
\end{Shaded}
\item
  The data might not have column names. You can use
  \texttt{col\_names\ =\ FALSE} to tell \texttt{read\_csv()} not to
  treat the first row as headings, and instead label them sequentially
  from \texttt{X1} to \texttt{Xn}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"1,2,3}\CharTok{\textbackslash{}n}\StringTok{4,5,6"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>      X1    X2    X3}
\CommentTok{#>   <int> <int> <int>}
\CommentTok{#> 1     1     2     3}
\CommentTok{#> 2     4     5     6}
\end{Highlighting}
\end{Shaded}

  (\texttt{"\textbackslash{}n"} is a convenient shortcut for adding a
  new line. You'll learn more about it and other types of string escape
  in \protect\hyperlink{string-basics}{string basics}.)

  Alternatively you can pass \texttt{col\_names} a character vector
  which will be used as the column names:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"1,2,3}\CharTok{\textbackslash{}n}\StringTok{4,5,6"}\NormalTok{, }\DataTypeTok{col_names =} \KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, }\StringTok{"z"}\NormalTok{))}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>       x     y     z}
\CommentTok{#>   <int> <int> <int>}
\CommentTok{#> 1     1     2     3}
\CommentTok{#> 2     4     5     6}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

Another option that commonly needs tweaking is \texttt{na}: this
specifies the value (or values) that are used to represent missing
values in your file:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"a,b,c}\CharTok{\textbackslash{}n}\StringTok{1,2,."}\NormalTok{, }\DataTypeTok{na =} \StringTok{"."}\NormalTok{)}
\CommentTok{#> # A tibble: 1 × 3}
\CommentTok{#>       a     b     c}
\CommentTok{#>   <int> <int> <chr>}
\CommentTok{#> 1     1     2  <NA>}
\end{Highlighting}
\end{Shaded}

This is all you need to know to read \textasciitilde{}75\% of CSV files
that you'll encounter in practice. You can also easily adapt what you've
learned to read tab separated files with \texttt{read\_tsv()} and fixed
width files with \texttt{read\_fwf()}. To read in more challenging
files, you'll need to learn more about how readr parses each column,
turning them into R vectors.

\subsection{Compared to base R}\label{compared-to-base-r}

If you've used R before, you might wonder why we're not using
\texttt{read.csv()}. There are a few good reasons to favour readr
functions over the base equivalents:

\begin{itemize}
\item
  They are typically much faster (\textasciitilde{}10x) than their base
  equivalents. Long running jobs have a progress bar, so you can see
  what's happening. If you're looking for raw speed, try
  \texttt{data.table::fread()}. It doesn't fit quite so well into the
  tidyverse, but it can be quite a bit faster.
\item
  They produce tibbles, they don't convert character vectors to factors,
  use row names, or munge the column names. These are common sources of
  frustration with the base R functions.
\item
  They are more reproducible. Base R functions inherit some behaviour
  from your operating system and environment variables, so import code
  that works on your computer might not work on someone else's.
\end{itemize}

\subsection{Exercises}\label{exercises-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What function would you use to read a file where fields were separated
  with\\
  ``\textbar{}''?
\item
  Apart from \texttt{file}, \texttt{skip}, and \texttt{comment}, what
  other arguments do \texttt{read\_csv()} and \texttt{read\_tsv()} have
  in common?
\item
  What are the most important arguments to \texttt{read\_fwf()}?
\item
  Sometimes strings in a CSV file contain commas. To prevent them from
  causing problems they need to be surrounded by a quoting character,
  like \texttt{"} or \texttt{\textquotesingle{}}. By convention,
  \texttt{read\_csv()} assumes that the quoting character will be
  \texttt{"}, and if you want to change it you'll need to use
  \texttt{read\_delim()} instead. What arguments do you need to specify
  to read the following text into a data frame?

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"x,y}\CharTok{\textbackslash{}n}\StringTok{1,'a,b'"}
\end{Highlighting}
\end{Shaded}
\item
  Identify what is wrong with each of the following inline CSV files.
  What happens when you run the code?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"a,b}\CharTok{\textbackslash{}n}\StringTok{1,2,3}\CharTok{\textbackslash{}n}\StringTok{4,5,6"}\NormalTok{)}
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"a,b,c}\CharTok{\textbackslash{}n}\StringTok{1,2}\CharTok{\textbackslash{}n}\StringTok{1,2,3,4"}\NormalTok{)}
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"a,b}\CharTok{\textbackslash{}n\textbackslash{}"}\StringTok{1"}\NormalTok{)}
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"a,b}\CharTok{\textbackslash{}n}\StringTok{1,2}\CharTok{\textbackslash{}n}\StringTok{a,b"}\NormalTok{)}
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"a;b}\CharTok{\textbackslash{}n}\StringTok{1;3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{Parsing a vector}\label{parsing-a-vector}

Before we get into the details of how readr reads files from disk, we
need to take a little detour to talk about the \texttt{parse\_*()}
functions. These functions take a character vector and return a more
specialised vector like a logical, integer, or date:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(}\KeywordTok{parse_logical}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{, }\StringTok{"NA"}\NormalTok{)))}
\CommentTok{#>  logi [1:3] TRUE FALSE NA}
\KeywordTok{str}\NormalTok{(}\KeywordTok{parse_integer}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{)))}
\CommentTok{#>  int [1:3] 1 2 3}
\KeywordTok{str}\NormalTok{(}\KeywordTok{parse_date}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"2010-01-01"}\NormalTok{, }\StringTok{"1979-10-14"}\NormalTok{)))}
\CommentTok{#>  Date[1:2], format: "2010-01-01" "1979-10-14"}
\end{Highlighting}
\end{Shaded}

These functions are useful in their own right, but are also an important
building block for readr. Once you've learned how the individual parsers
work in this section, we'll circle back and see how they fit together to
parse a complete file in the next section.

Like all functions in the tidyverse, the \texttt{parse\_*()} functions
are uniform: the first argument is a character vector to parse, and the
\texttt{na} argument specifies which strings should be treated as
missing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{parse_integer}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"231"}\NormalTok{, }\StringTok{"."}\NormalTok{, }\StringTok{"456"}\NormalTok{), }\DataTypeTok{na =} \StringTok{"."}\NormalTok{)}
\CommentTok{#> [1]   1 231  NA 456}
\end{Highlighting}
\end{Shaded}

If parsing fails, you'll get a warning:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{parse_integer}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"123"}\NormalTok{, }\StringTok{"345"}\NormalTok{, }\StringTok{"abc"}\NormalTok{, }\StringTok{"123.45"}\NormalTok{))}
\CommentTok{#> Warning: 2 parsing failures.}
\CommentTok{#> row col               expected actual}
\CommentTok{#>   3  -- an integer                abc}
\CommentTok{#>   4  -- no trailing characters    .45}
\end{Highlighting}
\end{Shaded}

And the failures will be missing in the output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}
\CommentTok{#> [1] 123 345  NA  NA}
\CommentTok{#> attr(,"problems")}
\CommentTok{#> # A tibble: 2 × 4}
\CommentTok{#>     row   col               expected actual}
\CommentTok{#>   <int> <int>                  <chr>  <chr>}
\CommentTok{#> 1     3    NA             an integer    abc}
\CommentTok{#> 2     4    NA no trailing characters    .45}
\end{Highlighting}
\end{Shaded}

If there are many parsing failures, you'll need to use
\texttt{problems()} to get the complete set. This returns a tibble,
which you can then manipulate with dplyr.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{problems}\NormalTok{(x)}
\CommentTok{#> # A tibble: 2 × 4}
\CommentTok{#>     row   col               expected actual}
\CommentTok{#>   <int> <int>                  <chr>  <chr>}
\CommentTok{#> 1     3    NA             an integer    abc}
\CommentTok{#> 2     4    NA no trailing characters    .45}
\end{Highlighting}
\end{Shaded}

Using parsers is mostly a matter of understanding what's available and
how they deal with different types of input. There are eight
particularly important parsers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{parse\_logical()} and \texttt{parse\_integer()} parse logicals
  and integers respectively. There's basically nothing that can go wrong
  with these parsers so I won't describe them here further.
\item
  \texttt{parse\_double()} is a strict numeric parser, and
  \texttt{parse\_number()} is a flexible numeric parser. These are more
  complicated than you might expect because different parts of the world
  write numbers in different ways.
\item
  \texttt{parse\_character()} seems so simple that it shouldn't be
  necessary. But one complication makes it quite important: character
  encodings.
\item
  \texttt{parse\_factor()} create factors, the data structure that R
  uses to represent categorical variables with fixed and known values.
\item
  \texttt{parse\_datetime()}, \texttt{parse\_date()}, and
  \texttt{parse\_time()} allow you to parse various date \& time
  specifications. These are the most complicated because there are so
  many different ways of writing dates.
\end{enumerate}

The following sections describe these parsers in more detail.

\subsection{Numbers}\label{numbers}

It seems like it should be straightforward to parse a number, but three
problems make it tricky:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  People write numbers differently in different parts of the world. For
  example, some countries use \texttt{.} in between the integer and
  fractional parts of a real number, while others use \texttt{,}.
\item
  Numbers are often surrounded by other characters that provide some
  context, like ``\$1000'' or ``10\%''.
\item
  Numbers often contain ``grouping'' characters to make them easier to
  read, like ``1,000,000'', and these grouping characters vary around
  the world.
\end{enumerate}

To address the first problem, readr has the notion of a ``locale'', an
object that specifies parsing options that differ from place to place.
When parsing numbers, the most important option is the character you use
for the decimal mark. You can override the default value of \texttt{.}
by creating a new locale and setting the \texttt{decimal\_mark}
argument:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{parse_double}\NormalTok{(}\StringTok{"1.23"}\NormalTok{)}
\CommentTok{#> [1] 1.23}
\KeywordTok{parse_double}\NormalTok{(}\StringTok{"1,23"}\NormalTok{, }\DataTypeTok{locale =} \KeywordTok{locale}\NormalTok{(}\DataTypeTok{decimal_mark =} \StringTok{","}\NormalTok{))}
\CommentTok{#> [1] 1.23}
\end{Highlighting}
\end{Shaded}

readr's default locale is US-centric, because generally R is US-centric
(i.e.~the documentation of base R is written in American English). An
alternative approach would be to try and guess the defaults from your
operating system. This is hard to do well, and, more importantly, makes
your code fragile: even if it works on your computer, it might fail when
you email it to a colleague in another country.

\texttt{parse\_number()} addresses the second problem: it ignores
non-numeric characters before and after the number. This is particularly
useful for currencies and percentages, but also works to extract numbers
embedded in text.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{parse_number}\NormalTok{(}\StringTok{"$100"}\NormalTok{)}
\CommentTok{#> [1] 100}
\KeywordTok{parse_number}\NormalTok{(}\StringTok{"20%"}\NormalTok{)}
\CommentTok{#> [1] 20}
\KeywordTok{parse_number}\NormalTok{(}\StringTok{"It cost $123.45"}\NormalTok{)}
\CommentTok{#> [1] 123}
\end{Highlighting}
\end{Shaded}

The final problem is addressed by the combination of
\texttt{parse\_number()} and the locale as \texttt{parse\_number()} will
ignore the ``grouping mark'':

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Used in America}
\KeywordTok{parse_number}\NormalTok{(}\StringTok{"$123,456,789"}\NormalTok{)}
\CommentTok{#> [1] 1.23e+08}

\CommentTok{# Used in many parts of Europe}
\KeywordTok{parse_number}\NormalTok{(}\StringTok{"123.456.789"}\NormalTok{, }\DataTypeTok{locale =} \KeywordTok{locale}\NormalTok{(}\DataTypeTok{grouping_mark =} \StringTok{"."}\NormalTok{))}
\CommentTok{#> [1] 1.23e+08}

\CommentTok{# Used in Switzerland}
\KeywordTok{parse_number}\NormalTok{(}\StringTok{"123'456'789"}\NormalTok{, }\DataTypeTok{locale =} \KeywordTok{locale}\NormalTok{(}\DataTypeTok{grouping_mark =} \StringTok{"'"}\NormalTok{))}
\CommentTok{#> [1] 1.23e+08}
\end{Highlighting}
\end{Shaded}

\subsection{Strings}\label{readr-strings}

It seems like \texttt{parse\_character()} should be really simple --- it
could just return its input. Unfortunately life isn't so simple, as
there are multiple ways to represent the same string. To understand
what's going on, we need to dive into the details of how computers
represent strings. In R, we can get at the underlying representation of
a string using \texttt{charToRaw()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{charToRaw}\NormalTok{(}\StringTok{"Hadley"}\NormalTok{)}
\CommentTok{#> [1] 48 61 64 6c 65 79}
\end{Highlighting}
\end{Shaded}

Each hexadecimal number represents a byte of information: \texttt{48} is
H, \texttt{61} is a, and so on. The mapping from hexadecimal number to
character is called the encoding, and in this case the encoding is
called ASCII. ASCII does a great job of representing English characters,
because it's the \textbf{American} Standard Code for Information
Interchange.

Things get more complicated for languages other than English. In the
early days of computing there were many competing standards for encoding
non-English characters, and to correctly interpret a string you needed
to know both the values and the encoding. For example, two common
encodings are Latin1 (aka ISO-8859-1, used for Western European
languages) and Latin2 (aka ISO-8859-2, used for Eastern European
languages). In Latin1, the byte \texttt{b1} is ``±'', but in Latin2,
it's ``ą''! Fortunately, today there is one standard that is supported
almost everywhere: UTF-8. UTF-8 can encode just about every character
used by humans today, as well as many extra symbols (like emoji!).

readr uses UTF-8 everywhere: it assumes your data is UTF-8 encoded when
you read it, and always uses it when writing. This is a good default,
but will fail for data produced by older systems that don't understand
UTF-8. If this happens to you, your strings will look weird when you
print them. Sometimes just one or two characters might be messed up;
other times you'll get complete gibberish. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ "El Ni}\CharTok{\textbackslash{}xf1}\StringTok{o was particularly bad this year"}
\NormalTok{x2 <-}\StringTok{ "}\CharTok{\textbackslash{}x82\textbackslash{}xb1\textbackslash{}x82\textbackslash{}xf1\textbackslash{}x82\textbackslash{}xc9\textbackslash{}x82\textbackslash{}xbf\textbackslash{}x82\textbackslash{}xcd}\StringTok{"}

\NormalTok{x1}
\CommentTok{#> [1] "El Ni\textbackslash{}xf1o was particularly bad this year"}
\NormalTok{x2}
\CommentTok{#> [1] "\textbackslash{}x82\textbackslash{}xb1\textbackslash{}x82\textbackslash{}xf1\textbackslash{}x82\textbackslash{}u0242\textbackslash{}xbf\textbackslash{}x82\textbackslash{}xcd"}
\end{Highlighting}
\end{Shaded}

To fix the problem you need to specify the encoding in
\texttt{parse\_character()}:

How do you find the correct encoding? If you're lucky, it'll be included
somewhere in the data documentation. Unfortunately, that's rarely the
case, so readr provides \texttt{guess\_encoding()} to help you figure it
out. It's not foolproof, and it works better when you have lots of text
(unlike here), but it's a reasonable place to start. Expect to try a few
different encodings before you find the right one.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{guess_encoding}\NormalTok{(}\KeywordTok{charToRaw}\NormalTok{(x1))}
\CommentTok{#>     encoding confidence}
\CommentTok{#> 1 ISO-8859-1       0.46}
\CommentTok{#> 2 ISO-8859-9       0.23}
\KeywordTok{guess_encoding}\NormalTok{(}\KeywordTok{charToRaw}\NormalTok{(x2))}
\CommentTok{#>   encoding confidence}
\CommentTok{#> 1   KOI8-R       0.42}
\end{Highlighting}
\end{Shaded}

The first argument to \texttt{guess\_encoding()} can either be a path to
a file, or, as in this case, a raw vector (useful if the strings are
already in R).

Encodings are a rich and complex topic, and I've only scratched the
surface here. If you'd like to learn more I'd recommend reading the
detailed explanation at \url{http://kunststube.net/encoding/}.

\subsection{Factors}\label{readr-factors}

R uses factors to represent categorical variables that have a known set
of possible values. Give \texttt{parse\_factor()} a vector of known
\texttt{levels} to generate a warning whenever an unexpected value is
present:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fruit <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{)}
\KeywordTok{parse_factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{, }\StringTok{"bananana"}\NormalTok{), }\DataTypeTok{levels =} \NormalTok{fruit)}
\CommentTok{#> Warning: 1 parsing failure.}
\CommentTok{#> row col           expected   actual}
\CommentTok{#>   3  -- value in level set bananana}
\CommentTok{#> [1] apple  banana <NA>  }
\CommentTok{#> attr(,"problems")}
\CommentTok{#> # A tibble: 1 × 4}
\CommentTok{#>     row   col           expected   actual}
\CommentTok{#>   <int> <int>              <chr>    <chr>}
\CommentTok{#> 1     3    NA value in level set bananana}
\CommentTok{#> Levels: apple banana}
\end{Highlighting}
\end{Shaded}

But if you have many problematic entries, it's often easier to leave as
character vectors and then use the tools you'll learn about in
\protect\hyperlink{strings}{strings} and
\protect\hyperlink{factors-1}{factors} to clean them up.

\hypertarget{readr-datetimes}{\subsection{Dates, date-times, and
times}\label{readr-datetimes}}

You pick between three parsers depending on whether you want a date (the
number of days since 1970-01-01), a date-time (the number of seconds
since midnight 1970-01-01), or a time (the number of seconds since
midnight). When called without any additional arguments:

\begin{itemize}
\item
  \texttt{parse\_datetime()} expects an ISO8601 date-time. ISO8601 is an
  international standard in which the components of a date are organised
  from biggest to smallest: year, month, day, hour, minute, second.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{parse_datetime}\NormalTok{(}\StringTok{"2010-10-01T2010"}\NormalTok{)}
\CommentTok{#> [1] "2010-10-01 20:10:00 UTC"}
\CommentTok{# If time is omitted, it will be set to midnight}
\KeywordTok{parse_datetime}\NormalTok{(}\StringTok{"20101010"}\NormalTok{)}
\CommentTok{#> [1] "2010-10-10 UTC"}
\end{Highlighting}
\end{Shaded}

  This is the most important date/time standard, and if you work with
  dates and times frequently, I recommend reading
  \url{https://en.wikipedia.org/wiki/ISO_8601}
\item
  \texttt{parse\_date()} expects a four digit year, a \texttt{-} or
  \texttt{/}, the month, a \texttt{-} or \texttt{/}, then the day:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{parse_date}\NormalTok{(}\StringTok{"2010-10-01"}\NormalTok{)}
\CommentTok{#> [1] "2010-10-01"}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{parse\_time()} expects the hour, \texttt{:}, minutes,
  optionally \texttt{:} and seconds, and an optional am/pm specifier:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(hms)}
\KeywordTok{parse_time}\NormalTok{(}\StringTok{"01:10 am"}\NormalTok{)}
\CommentTok{#> 01:10:00}
\KeywordTok{parse_time}\NormalTok{(}\StringTok{"20:10:01"}\NormalTok{)}
\CommentTok{#> 20:10:01}
\end{Highlighting}
\end{Shaded}

  Base R doesn't have a great built in class for time data, so we use
  the one provided in the hms package.
\end{itemize}

If these defaults don't work for your data you can supply your own
date-time \texttt{format}, built up of the following pieces:

\begin{description}
\tightlist
\item[Year]
\texttt{\%Y} (4 digits).

\texttt{\%y} (2 digits); 00-69 -\textgreater{} 2000-2069, 70-99
-\textgreater{} 1970-1999.
\item[Month]
\texttt{\%m} (2 digits).

\texttt{\%b} (abbreviated name, like ``Jan'').

\texttt{\%B} (full name, ``January'').
\item[Day]
\texttt{\%d} (2 digits).

\texttt{\%e} (optional leading space).
\item[Time]
\texttt{\%H} 0-23 hour.

\texttt{\%I} 0-12, must be used with \texttt{\%p}.

\texttt{\%p} AM/PM indicator.

\texttt{\%M} minutes.

\texttt{\%S} integer seconds.

\texttt{\%OS} real seconds.

\texttt{\%Z} Time zone (as name, e.g. \texttt{America/Chicago}). Beware
of abbreviations: if you're American, note that ``EST'' is a Canadian
time zone that does not have daylight savings time. It is \emph{not}
Eastern Standard Time! We'll come back to this
\protect\hyperlink{time-zones}{time zones}.

\texttt{\%z} (as offset from UTC, e.g. \texttt{+0800}).
\item[Non-digits]
\texttt{\%.} skips one non-digit character.

\texttt{\%*} skips any number of non-digits.
\end{description}

The best way to figure out the correct format is to create a few
examples in a character vector, and test with one of the parsing
functions. For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{parse_date}\NormalTok{(}\StringTok{"01/02/15"}\NormalTok{, }\StringTok{"%m/%d/%y"}\NormalTok{)}
\CommentTok{#> [1] "2015-01-02"}
\KeywordTok{parse_date}\NormalTok{(}\StringTok{"01/02/15"}\NormalTok{, }\StringTok{"%d/%m/%y"}\NormalTok{)}
\CommentTok{#> [1] "2015-02-01"}
\KeywordTok{parse_date}\NormalTok{(}\StringTok{"01/02/15"}\NormalTok{, }\StringTok{"%y/%m/%d"}\NormalTok{)}
\CommentTok{#> [1] "2001-02-15"}
\end{Highlighting}
\end{Shaded}

If you're using \texttt{\%b} or \texttt{\%B} with non-English month
names, you'll need to set the \texttt{lang} argument to
\texttt{locale()}. See the list of built-in languages in
\texttt{date\_names\_langs()}, or if your language is not already
included, create your own with \texttt{date\_names()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{parse_date}\NormalTok{(}\StringTok{"1 janvier 2015"}\NormalTok{, }\StringTok{"%d %B %Y"}\NormalTok{, }\DataTypeTok{locale =} \KeywordTok{locale}\NormalTok{(}\StringTok{"fr"}\NormalTok{))}
\CommentTok{#> [1] "2015-01-01"}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What are the most important arguments to \texttt{locale()}?
\item
  What happens if you try and set \texttt{decimal\_mark} and
  \texttt{grouping\_mark} to the same character? What happens to the
  default value of \texttt{grouping\_mark} when you set
  \texttt{decimal\_mark} to ``,''? What happens to the default value of
  \texttt{decimal\_mark} when you set the \texttt{grouping\_mark} to
  ``.''?
\item
  I didn't discuss the \texttt{date\_format} and \texttt{time\_format}
  options to \texttt{locale()}. What do they do? Construct an example
  that shows when they might be useful.
\item
  If you live outside the US, create a new locale object that
  encapsulates the settings for the types of file you read most
  commonly.
\item
  What's the difference between \texttt{read\_csv()} and
  \texttt{read\_csv2()}?
\item
  What are the most common encodings used in Europe? What are the most
  common encodings used in Asia? Do some googling to find out.
\item
  Generate the correct format string to parse each of the following
  dates and times:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d1 <-}\StringTok{ "January 1, 2010"}
\NormalTok{d2 <-}\StringTok{ "2015-Mar-07"}
\NormalTok{d3 <-}\StringTok{ "06-Jun-2017"}
\NormalTok{d4 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"August 19 (2015)"}\NormalTok{, }\StringTok{"July 1 (2015)"}\NormalTok{)}
\NormalTok{d5 <-}\StringTok{ "12/30/14"} \CommentTok{# Dec 30, 2014}
\NormalTok{t1 <-}\StringTok{ "1705"}
\NormalTok{t2 <-}\StringTok{ "11:15:10.12 PM"}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\hypertarget{parsing-a-file}{\section{Parsing a
file}\label{parsing-a-file}}

Now that you've learned how to parse an individual vector, it's time to
return to the beginning and explore how readr parses a file. There are
two new things that you'll learn about in this section:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How readr automatically guesses the type of each column.
\item
  How to override the default specification.
\end{enumerate}

\subsection{Strategy}\label{strategy}

readr uses a heuristic to figure out the type of each column: it reads
the first 1000 rows and uses some (moderately conservative) heuristics
to figure out the type of each column. You can emulate this process with
a character vector using \texttt{guess\_parser()}, which returns readr's
best guess, and \texttt{parse\_guess()} which uses that guess to parse
the column:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{guess_parser}\NormalTok{(}\StringTok{"2010-10-01"}\NormalTok{)}
\CommentTok{#> [1] "date"}
\KeywordTok{guess_parser}\NormalTok{(}\StringTok{"15:01"}\NormalTok{)}
\CommentTok{#> [1] "time"}
\KeywordTok{guess_parser}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{))}
\CommentTok{#> [1] "logical"}
\KeywordTok{guess_parser}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"5"}\NormalTok{, }\StringTok{"9"}\NormalTok{))}
\CommentTok{#> [1] "integer"}
\KeywordTok{guess_parser}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"12,352,561"}\NormalTok{))}
\CommentTok{#> [1] "number"}

\KeywordTok{str}\NormalTok{(}\KeywordTok{parse_guess}\NormalTok{(}\StringTok{"2010-10-10"}\NormalTok{))}
\CommentTok{#>  Date[1:1], format: "2010-10-10"}
\end{Highlighting}
\end{Shaded}

The heuristic tries each of the following types, stopping when it finds
a match:

\begin{itemize}
\tightlist
\item
  logical: contains only ``F'', ``T'', ``FALSE'', or ``TRUE''.
\item
  integer: contains only numeric characters (and \texttt{-}).
\item
  double: contains only valid doubles (including numbers like
  \texttt{4.5e-5}).
\item
  number: contains valid doubles with the grouping mark inside.
\item
  time: matches the default \texttt{time\_format}.
\item
  date: matches the default \texttt{date\_format}.
\item
  date-time: any ISO8601 date.
\end{itemize}

If none of these rules apply, then the column will stay as a vector of
strings.

\subsection{Problems}\label{problems}

These defaults don't always work for larger files. There are two basic
problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The first thousand rows might be a special case, and readr guesses a
  type that is not sufficiently general. For example, you might have a
  column of doubles that only contains integers in the first 1000 rows.
\item
  The column might contain a lot of missing values. If the first 1000
  rows contain only \texttt{NA}s, readr will guess that it's a character
  vector, whereas you probably want to parse it as something more
  specific.
\end{enumerate}

readr contains a challenging CSV that illustrates both of these
problems:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{challenge <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{readr_example}\NormalTok{(}\StringTok{"challenge.csv"}\NormalTok{))}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   x = col_integer(),}
\CommentTok{#>   y = col_character()}
\CommentTok{#> )}
\CommentTok{#> Warning: 1000 parsing failures.}
\CommentTok{#>  row col               expected             actual}
\CommentTok{#> 1001   x no trailing characters .23837975086644292}
\CommentTok{#> 1002   x no trailing characters .41167997173033655}
\CommentTok{#> 1003   x no trailing characters .7460716762579978 }
\CommentTok{#> 1004   x no trailing characters .723450553836301  }
\CommentTok{#> 1005   x no trailing characters .614524137461558  }
\CommentTok{#> .... ... ...................... ..................}
\CommentTok{#> See problems(...) for more details.}
\end{Highlighting}
\end{Shaded}

(Note the use of \texttt{readr\_example()} which finds the path to one
of the files included with the package)

There are two printed outputs: the column specification generated by
looking at the first 1000 rows, and the first five parsing failures.
It's always a good idea to explicitly pull out the \texttt{problems()},
so you can explore them in more depth:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{problems}\NormalTok{(challenge)}
\CommentTok{#> # A tibble: 1,000 × 4}
\CommentTok{#>     row   col               expected             actual}
\CommentTok{#>   <int> <chr>                  <chr>              <chr>}
\CommentTok{#> 1  1001     x no trailing characters .23837975086644292}
\CommentTok{#> 2  1002     x no trailing characters .41167997173033655}
\CommentTok{#> 3  1003     x no trailing characters  .7460716762579978}
\CommentTok{#> 4  1004     x no trailing characters   .723450553836301}
\CommentTok{#> 5  1005     x no trailing characters   .614524137461558}
\CommentTok{#> 6  1006     x no trailing characters   .473980569280684}
\CommentTok{#> # ... with 994 more rows}
\end{Highlighting}
\end{Shaded}

A good strategy is to work column by column until there are no problems
remaining. Here we can see that there are a lot of parsing problems with
the \texttt{x} column - there are trailing characters after the integer
value. That suggests we need to use a double parser instead.

To fix the call, start by copying and pasting the column specification
into your original call:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{challenge <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}
  \KeywordTok{readr_example}\NormalTok{(}\StringTok{"challenge.csv"}\NormalTok{), }
  \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}
    \DataTypeTok{x =} \KeywordTok{col_integer}\NormalTok{(),}
    \DataTypeTok{y =} \KeywordTok{col_character}\NormalTok{()}
  \NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then you can tweak the type of the \texttt{x} column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{challenge <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}
  \KeywordTok{readr_example}\NormalTok{(}\StringTok{"challenge.csv"}\NormalTok{), }
  \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}
    \DataTypeTok{x =} \KeywordTok{col_double}\NormalTok{(),}
    \DataTypeTok{y =} \KeywordTok{col_character}\NormalTok{()}
  \NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That fixes the first problem, but if we look at the last few rows,
you'll see that they're dates stored in a character vector:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tail}\NormalTok{(challenge)}
\CommentTok{#> # A tibble: 6 × 2}
\CommentTok{#>       x          y}
\CommentTok{#>   <dbl>      <chr>}
\CommentTok{#> 1 0.805 2019-11-21}
\CommentTok{#> 2 0.164 2018-03-29}
\CommentTok{#> 3 0.472 2014-08-04}
\CommentTok{#> 4 0.718 2015-08-16}
\CommentTok{#> 5 0.270 2020-02-04}
\CommentTok{#> 6 0.608 2019-01-06}
\end{Highlighting}
\end{Shaded}

You can fix that by specifying that \texttt{y} is a date column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{challenge <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}
  \KeywordTok{readr_example}\NormalTok{(}\StringTok{"challenge.csv"}\NormalTok{), }
  \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}
    \DataTypeTok{x =} \KeywordTok{col_double}\NormalTok{(),}
    \DataTypeTok{y =} \KeywordTok{col_date}\NormalTok{()}
  \NormalTok{)}
\NormalTok{)}
\KeywordTok{tail}\NormalTok{(challenge)}
\CommentTok{#> # A tibble: 6 × 2}
\CommentTok{#>       x          y}
\CommentTok{#>   <dbl>     <date>}
\CommentTok{#> 1 0.805 2019-11-21}
\CommentTok{#> 2 0.164 2018-03-29}
\CommentTok{#> 3 0.472 2014-08-04}
\CommentTok{#> 4 0.718 2015-08-16}
\CommentTok{#> 5 0.270 2020-02-04}
\CommentTok{#> 6 0.608 2019-01-06}
\end{Highlighting}
\end{Shaded}

Every \texttt{parse\_xyz()} function has a corresponding
\texttt{col\_xyz()} function. You use \texttt{parse\_xyz()} when the
data is in a character vector in R already; you use \texttt{col\_xyz()}
when you want to tell readr how to load the data.

I highly recommend always supplying \texttt{col\_types}, building up
from the print-out provided by readr. This ensures that you have a
consistent and reproducible data import script. If you rely on the
default guesses and your data changes, readr will continue to read it
in. If you want to be really strict, use \texttt{stop\_for\_problems()}:
that will throw an error and stop your script if there are any parsing
problems.

\subsection{Other strategies}\label{other-strategies}

There are a few other general strategies to help you parse files:

\begin{itemize}
\item
  In the previous example, we just got unlucky: if we look at just one
  more row than the default, we can correctly parse in one shot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{challenge2 <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{readr_example}\NormalTok{(}\StringTok{"challenge.csv"}\NormalTok{), }\DataTypeTok{guess_max =} \DecValTok{1001}\NormalTok{)}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   x = col_double(),}
\CommentTok{#>   y = col_date(format = "")}
\CommentTok{#> )}
\NormalTok{challenge2}
\CommentTok{#> # A tibble: 2,000 × 2}
\CommentTok{#>       x      y}
\CommentTok{#>   <dbl> <date>}
\CommentTok{#> 1   404   <NA>}
\CommentTok{#> 2  4172   <NA>}
\CommentTok{#> 3  3004   <NA>}
\CommentTok{#> 4   787   <NA>}
\CommentTok{#> 5    37   <NA>}
\CommentTok{#> 6  2332   <NA>}
\CommentTok{#> # ... with 1,994 more rows}
\end{Highlighting}
\end{Shaded}
\item
  Sometimes it's easier to diagnose problems if you just read in all the
  columns as character vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{challenge2 <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{readr_example}\NormalTok{(}\StringTok{"challenge.csv"}\NormalTok{), }
  \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}\DataTypeTok{.default =} \KeywordTok{col_character}\NormalTok{())}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  This is particularly useful in conjunction with
  \texttt{type\_convert()}, which applies the parsing heuristics to the
  character columns in a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x,  ~y,}
  \StringTok{"1"}\NormalTok{, }\StringTok{"1.21"}\NormalTok{,}
  \StringTok{"2"}\NormalTok{, }\StringTok{"2.32"}\NormalTok{,}
  \StringTok{"3"}\NormalTok{, }\StringTok{"4.56"}
\NormalTok{)}
\NormalTok{df}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <chr> <chr>}
\CommentTok{#> 1     1  1.21}
\CommentTok{#> 2     2  2.32}
\CommentTok{#> 3     3  4.56}

\CommentTok{# Note the column types}
\KeywordTok{type_convert}\NormalTok{(df)}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   x = col_integer(),}
\CommentTok{#>   y = col_double()}
\CommentTok{#> )}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <int> <dbl>}
\CommentTok{#> 1     1  1.21}
\CommentTok{#> 2     2  2.32}
\CommentTok{#> 3     3  4.56}
\end{Highlighting}
\end{Shaded}
\item
  If you're reading a very large file, you might want to set
  \texttt{n\_max} to a smallish number like 10,000 or 100,000. That will
  accelerate your iterations while you eliminate common problems.
\item
  If you're having major parsing problems, sometimes it's easier to just
  read into a character vector of lines with \texttt{read\_lines()}, or
  even a character vector of length 1 with \texttt{read\_file()}. Then
  you can use the string parsing skills you'll learn later to parse more
  exotic formats.
\end{itemize}

\section{Writing to a file}\label{writing-to-a-file}

readr also comes with two useful functions for writing data back to
disk: \texttt{write\_csv()} and \texttt{write\_tsv()}. Both functions
increase the chances of the output file being read back in correctly by:

\begin{itemize}
\item
  Always encoding strings in UTF-8.
\item
  Saving dates and date-times in ISO8601 format so they are easily
  parsed elsewhere.
\end{itemize}

If you want to export a csv file to Excel, use
\texttt{write\_excel\_csv()} --- this writes a special character (a
``byte order mark'') at the start of the file which tells Excel that
you're using the UTF-8 encoding.

The most important arguments are \texttt{x} (the data frame to save),
and \texttt{path} (the location to save it). You can also specify how
missing values are written with \texttt{na}, and if you want to
\texttt{append} to an existing file.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write_csv}\NormalTok{(challenge, }\StringTok{"challenge.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that the type information is lost when you save to csv:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{challenge}
\CommentTok{#> # A tibble: 2,000 × 2}
\CommentTok{#>       x      y}
\CommentTok{#>   <dbl> <date>}
\CommentTok{#> 1   404   <NA>}
\CommentTok{#> 2  4172   <NA>}
\CommentTok{#> 3  3004   <NA>}
\CommentTok{#> 4   787   <NA>}
\CommentTok{#> 5    37   <NA>}
\CommentTok{#> 6  2332   <NA>}
\CommentTok{#> # ... with 1,994 more rows}
\KeywordTok{write_csv}\NormalTok{(challenge, }\StringTok{"challenge-2.csv"}\NormalTok{)}
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"challenge-2.csv"}\NormalTok{)}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   x = col_double(),}
\CommentTok{#>   y = col_character()}
\CommentTok{#> )}
\CommentTok{#> # A tibble: 2,000 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <dbl> <chr>}
\CommentTok{#> 1   404  <NA>}
\CommentTok{#> 2  4172  <NA>}
\CommentTok{#> 3  3004  <NA>}
\CommentTok{#> 4   787  <NA>}
\CommentTok{#> 5    37  <NA>}
\CommentTok{#> 6  2332  <NA>}
\CommentTok{#> # ... with 1,994 more rows}
\end{Highlighting}
\end{Shaded}

This makes CSVs a little unreliable for caching interim results---you
need to recreate the column specification every time you load in. There
are two alternatives:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{write\_rds()} and \texttt{read\_rds()} are uniform wrappers
  around the base functions \texttt{readRDS()} and \texttt{saveRDS()}.
  These store data in R's custom binary format called RDS:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write_rds}\NormalTok{(challenge, }\StringTok{"challenge.rds"}\NormalTok{)}
\KeywordTok{read_rds}\NormalTok{(}\StringTok{"challenge.rds"}\NormalTok{)}
\CommentTok{#> # A tibble: 2,000 × 2}
\CommentTok{#>       x      y}
\CommentTok{#>   <dbl> <date>}
\CommentTok{#> 1   404   <NA>}
\CommentTok{#> 2  4172   <NA>}
\CommentTok{#> 3  3004   <NA>}
\CommentTok{#> 4   787   <NA>}
\CommentTok{#> 5    37   <NA>}
\CommentTok{#> 6  2332   <NA>}
\CommentTok{#> # ... with 1,994 more rows}
\end{Highlighting}
\end{Shaded}
\item
  The feather package implements a fast binary file format that can be
  shared across programming languages:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(feather)}
\KeywordTok{write_feather}\NormalTok{(challenge, }\StringTok{"challenge.feather"}\NormalTok{)}
\KeywordTok{read_feather}\NormalTok{(}\StringTok{"challenge.feather"}\NormalTok{)}
\CommentTok{#> # A tibble: 2,000 x 2}
\CommentTok{#>       x      y}
\CommentTok{#>   <dbl> <date>}
\CommentTok{#> 1   404   <NA>}
\CommentTok{#> 2  4172   <NA>}
\CommentTok{#> 3  3004   <NA>}
\CommentTok{#> 4   787   <NA>}
\CommentTok{#> 5    37   <NA>}
\CommentTok{#> 6  2332   <NA>}
\CommentTok{#> # ... with 1,994 more rows}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

Feather tends to be faster than RDS and is usable outside of R. RDS
supports list-columns (which you'll learn about in
\protect\hyperlink{many-models}{many models}); feather currently does
not.

\section{Other types of data}\label{other-types-of-data}

To get other types of data into R, we recommend starting with the
tidyverse packages listed below. They're certainly not perfect, but they
are a good place to start. For rectangular data:

\begin{itemize}
\item
  \textbf{haven} reads SPSS, Stata, and SAS files.
\item
  \textbf{readxl} reads excel files (both \texttt{.xls} and
  \texttt{.xlsx}).
\item
  \textbf{DBI}, along with a database specific backend (e.g.
  \textbf{RMySQL}, \textbf{RSQLite}, \textbf{RPostgreSQL} etc) allows
  you to run SQL queries against a database and return a data frame.
\end{itemize}

For hierarchical data: use \textbf{jsonlite} (by Jeroen Ooms) for json,
and \textbf{xml2} for XML. Jenny Bryan has some excellent worked
examples at
\url{https://jennybc.github.io/purrr-tutorial/examples.html}.

For other file types, try the
\href{https://cran.r-project.org/doc/manuals/r-release/R-data.html}{R
data import/export manual} and the
\href{https://github.com/leeper/rio}{\textbf{rio}} package.

\chapter{Tidy data}\label{tidy-data}

\section{Introduction}\label{introduction-6}

\begin{quote}
``Happy families are all alike; every unhappy family is unhappy in its
own way.'' ---- Leo Tolstoy
\end{quote}

\begin{quote}
``Tidy datasets are all alike, but every messy dataset is messy in its
own way.'' ---- Hadley Wickham
\end{quote}

In this chapter, you will learn a consistent way to organise your data
in R, an organisation called \textbf{tidy data}. Getting your data into
this format requires some upfront work, but that work pays off in the
long term. Once you have tidy data and the tidy tools provided by
packages in the tidyverse, you will spend much less time munging data
from one representation to another, allowing you to spend more time on
the analytic questions at hand.

This chapter will give you a practical introduction to tidy data and the
accompanying tools in the \textbf{tidyr} package. If you'd like to learn
more about the underlying theory, you might enjoy the \emph{Tidy Data}
paper published in the Journal of Statistical Software,
\url{http://www.jstatsoft.org/v59/i10/paper}.

\subsection{Prerequisites}\label{prerequisites-6}

In this chapter we'll focus on tidyr, a package that provides a bunch of
tools to help tidy up your messy datasets. tidyr is a member of the core
tidyverse.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tidy-data-1}{\section{Tidy data}\label{tidy-data-1}}

You can represent the same underlying data in multiple ways. The example
below shows the same data organised in four different ways. Each dataset
shows the same values of four variables \emph{country}, \emph{year},
\emph{population}, and \emph{cases}, but each dataset organises the
values in a different way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table1}
\CommentTok{#> # A tibble: 6 × 4}
\CommentTok{#>       country  year  cases population}
\CommentTok{#>         <chr> <int>  <int>      <int>}
\CommentTok{#> 1 Afghanistan  1999    745   19987071}
\CommentTok{#> 2 Afghanistan  2000   2666   20595360}
\CommentTok{#> 3      Brazil  1999  37737  172006362}
\CommentTok{#> 4      Brazil  2000  80488  174504898}
\CommentTok{#> 5       China  1999 212258 1272915272}
\CommentTok{#> 6       China  2000 213766 1280428583}
\NormalTok{table2}
\CommentTok{#> # A tibble: 12 × 4}
\CommentTok{#>       country  year       type     count}
\CommentTok{#>         <chr> <int>      <chr>     <int>}
\CommentTok{#> 1 Afghanistan  1999      cases       745}
\CommentTok{#> 2 Afghanistan  1999 population  19987071}
\CommentTok{#> 3 Afghanistan  2000      cases      2666}
\CommentTok{#> 4 Afghanistan  2000 population  20595360}
\CommentTok{#> 5      Brazil  1999      cases     37737}
\CommentTok{#> 6      Brazil  1999 population 172006362}
\CommentTok{#> # ... with 6 more rows}
\NormalTok{table3}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>       country  year              rate}
\CommentTok{#> *       <chr> <int>             <chr>}
\CommentTok{#> 1 Afghanistan  1999      745/19987071}
\CommentTok{#> 2 Afghanistan  2000     2666/20595360}
\CommentTok{#> 3      Brazil  1999   37737/172006362}
\CommentTok{#> 4      Brazil  2000   80488/174504898}
\CommentTok{#> 5       China  1999 212258/1272915272}
\CommentTok{#> 6       China  2000 213766/1280428583}

\CommentTok{# Spread across two tibbles}
\NormalTok{table4a  }\CommentTok{# cases}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>       country `1999` `2000`}
\CommentTok{#> *       <chr>  <int>  <int>}
\CommentTok{#> 1 Afghanistan    745   2666}
\CommentTok{#> 2      Brazil  37737  80488}
\CommentTok{#> 3       China 212258 213766}
\NormalTok{table4b  }\CommentTok{# population}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>       country     `1999`     `2000`}
\CommentTok{#> *       <chr>      <int>      <int>}
\CommentTok{#> 1 Afghanistan   19987071   20595360}
\CommentTok{#> 2      Brazil  172006362  174504898}
\CommentTok{#> 3       China 1272915272 1280428583}
\end{Highlighting}
\end{Shaded}

These are all representations of the same underlying data, but they are
not equally easy to use. One dataset, the tidy dataset, will be much
easier to work with inside the tidyverse.

There are three interrelated rules which make a dataset tidy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable must have its own column.
\item
  Each observation must have its own row.
\item
  Each value must have its own cell.
\end{enumerate}

Figure \ref{fig:tidy-structure} shows the rules visually.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/tidy-1} 

}

\caption{Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells.}\label{fig:tidy-structure}
\end{figure}

These three rules are interrelated because it's impossible to only
satisfy two of the three. That interrelationship leads to an even
simpler set of practical instructions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Put each dataset in a tibble.
\item
  Put each variable in a column.
\end{enumerate}

In this example, only \texttt{table1} is tidy. It's the only
representation where each column is a variable.

Why ensure that your data is tidy? There are two main advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  There's a general advantage to picking one consistent way of storing
  data. If you have a consistent data structure, it's easier to learn
  the tools that work with it because they have an underlying
  uniformity.
\item
  There's a specific advantage to placing variables in columns because
  it allows R's vectorised nature to shine. As you learned in
  \protect\hyperlink{mutate-funs}{mutate} and
  \protect\hyperlink{summary-funs}{summary functions}, most built-in R
  functions work with vectors of values. That makes transforming tidy
  data feel particularly natural.
\end{enumerate}

dplyr, ggplot2, and all the other packages in the tidyverse are designed
to work with tidy data. Here are a couple of small examples showing how
you might work with \texttt{table1}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute rate per 10,000}
\NormalTok{table1 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{rate =} \NormalTok{cases /}\StringTok{ }\NormalTok{population *}\StringTok{ }\DecValTok{10000}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 5}
\CommentTok{#>       country  year  cases population  rate}
\CommentTok{#>         <chr> <int>  <int>      <int> <dbl>}
\CommentTok{#> 1 Afghanistan  1999    745   19987071 0.373}
\CommentTok{#> 2 Afghanistan  2000   2666   20595360 1.294}
\CommentTok{#> 3      Brazil  1999  37737  172006362 2.194}
\CommentTok{#> 4      Brazil  2000  80488  174504898 4.612}
\CommentTok{#> 5       China  1999 212258 1272915272 1.667}
\CommentTok{#> 6       China  2000 213766 1280428583 1.669}

\CommentTok{# Compute cases per year}
\NormalTok{table1 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(year, }\DataTypeTok{wt =} \NormalTok{cases)}
\CommentTok{#> # A tibble: 2 × 2}
\CommentTok{#>    year      n}
\CommentTok{#>   <int>  <int>}
\CommentTok{#> 1  1999 250740}
\CommentTok{#> 2  2000 296920}

\CommentTok{# Visualise changes over time}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(table1, }\KeywordTok{aes}\NormalTok{(year, cases)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =} \NormalTok{country), }\DataTypeTok{colour =} \StringTok{"grey50"}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{country))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{_bookdown_files/tidy_files/figure-latex/unnamed-chunk-3-1} \end{center}

\subsection{Exercises}\label{exercises-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Using prose, describe how the variables and observations are organised
  in each of the sample tables.
\item
  Compute the \texttt{rate} for \texttt{table2}, and \texttt{table4a} +
  \texttt{table4b}. You will need to perform four operations:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Extract the number of TB cases per country per year.
  \item
    Extract the matching population per country per year.
  \item
    Divide cases by population, and multiply by 10000.
  \item
    Store back in the appropriate place.
  \end{enumerate}

  Which representation is easiest to work with? Which is hardest? Why?
\item
  Recreate the plot showing change in cases over time using
  \texttt{table2} instead of \texttt{table1}. What do you need to do
  first?
\end{enumerate}

\section{Spreading and gathering}\label{spreading-and-gathering}

The principles of tidy data seem so obvious that you might wonder if
you'll ever encounter a dataset that isn't tidy. Unfortunately, however,
most data that you will encounter will be untidy. There are two main
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Most people aren't familiar with the principles of tidy data, and it's
  hard to derive them yourself unless you spend a \emph{lot} of time
  working with data.
\item
  Data is often organised to facilitate some use other than analysis.
  For example, data is often organised to make entry as easy as
  possible.
\end{enumerate}

This means for most real analyses, you'll need to do some tidying. The
first step is always to figure out what the variables and observations
are. Sometimes this is easy; other times you'll need to consult with the
people who originally generated the data. The second step is to resolve
one of two common problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  One variable might be spread across multiple columns.
\item
  One observation might be scattered across multiple rows.
\end{enumerate}

Typically a dataset will only suffer from one of these problems; it'll
only suffer from both if you're really unlucky! To fix these problems,
you'll need the two most important functions in tidyr: \texttt{gather()}
and \texttt{spread()}.

\subsection{Gathering}\label{gathering}

A common problem is a dataset where some of the column names are not
names of variables, but \emph{values} of a variable. Take
\texttt{table4a}: the column names \texttt{1999} and \texttt{2000}
represent values of the \texttt{year} variable, and each row represents
two observations, not one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table4a}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>       country `1999` `2000`}
\CommentTok{#> *       <chr>  <int>  <int>}
\CommentTok{#> 1 Afghanistan    745   2666}
\CommentTok{#> 2      Brazil  37737  80488}
\CommentTok{#> 3       China 212258 213766}
\end{Highlighting}
\end{Shaded}

To tidy a dataset like this, we need to \textbf{gather} those columns
into a new pair of variables. To describe that operation we need three
parameters:

\begin{itemize}
\item
  The set of columns that represent values, not variables. In this
  example, those are the columns \texttt{1999} and \texttt{2000}.
\item
  The name of the variable whose values form the column names. I call
  that the \texttt{key}, and here it is \texttt{year}.
\item
  The name of the variable whose values are spread over the cells. I
  call that \texttt{value}, and here it's the number of \texttt{cases}.
\end{itemize}

Together those parameters generate the call to \texttt{gather()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table4a %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{`}\DataTypeTok{1999}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{2000}\StringTok{`}\NormalTok{, }\DataTypeTok{key =} \StringTok{"year"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"cases"}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>       country  year  cases}
\CommentTok{#>         <chr> <chr>  <int>}
\CommentTok{#> 1 Afghanistan  1999    745}
\CommentTok{#> 2      Brazil  1999  37737}
\CommentTok{#> 3       China  1999 212258}
\CommentTok{#> 4 Afghanistan  2000   2666}
\CommentTok{#> 5      Brazil  2000  80488}
\CommentTok{#> 6       China  2000 213766}
\end{Highlighting}
\end{Shaded}

The columns to gather are specified with \texttt{dplyr::select()} style
notation. Here there are only two columns, so we list them individually.
Note that ``1999'' and ``2000'' are non-syntactic names so we have to
surround them in backticks. To refresh your memory of the other ways to
select columns, see \protect\hyperlink{select}{select}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/tidy-9} 

}

\caption{Gathering `table4` into a tidy form.}\label{fig:tidy-gather}
\end{figure}

In the final result, the gathered columns are dropped, and we get new
\texttt{key} and \texttt{value} columns. Otherwise, the relationships
between the original variables are preserved. Visually, this is shown in
Figure \ref{fig:tidy-gather}. We can use \texttt{gather()} to tidy
\texttt{table4b} in a similar fashion. The only difference is the
variable stored in the cell values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table4b %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{`}\DataTypeTok{1999}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{2000}\StringTok{`}\NormalTok{, }\DataTypeTok{key =} \StringTok{"year"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"population"}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>       country  year population}
\CommentTok{#>         <chr> <chr>      <int>}
\CommentTok{#> 1 Afghanistan  1999   19987071}
\CommentTok{#> 2      Brazil  1999  172006362}
\CommentTok{#> 3       China  1999 1272915272}
\CommentTok{#> 4 Afghanistan  2000   20595360}
\CommentTok{#> 5      Brazil  2000  174504898}
\CommentTok{#> 6       China  2000 1280428583}
\end{Highlighting}
\end{Shaded}

To combine the tidied versions of \texttt{table4a} and \texttt{table4b}
into a single tibble, we need to use \texttt{dplyr::left\_join()}, which
you'll learn about in \protect\hyperlink{relational-data}{relational
data}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy4a <-}\StringTok{ }\NormalTok{table4a %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{`}\DataTypeTok{1999}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{2000}\StringTok{`}\NormalTok{, }\DataTypeTok{key =} \StringTok{"year"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"cases"}\NormalTok{)}
\NormalTok{tidy4b <-}\StringTok{ }\NormalTok{table4b %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{`}\DataTypeTok{1999}\StringTok{`}\NormalTok{, }\StringTok{`}\DataTypeTok{2000}\StringTok{`}\NormalTok{, }\DataTypeTok{key =} \StringTok{"year"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"population"}\NormalTok{)}
\KeywordTok{left_join}\NormalTok{(tidy4a, tidy4b)}
\CommentTok{#> Joining, by = c("country", "year")}
\CommentTok{#> # A tibble: 6 × 4}
\CommentTok{#>       country  year  cases population}
\CommentTok{#>         <chr> <chr>  <int>      <int>}
\CommentTok{#> 1 Afghanistan  1999    745   19987071}
\CommentTok{#> 2      Brazil  1999  37737  172006362}
\CommentTok{#> 3       China  1999 212258 1272915272}
\CommentTok{#> 4 Afghanistan  2000   2666   20595360}
\CommentTok{#> 5      Brazil  2000  80488  174504898}
\CommentTok{#> 6       China  2000 213766 1280428583}
\end{Highlighting}
\end{Shaded}

\subsection{Spreading}\label{spreading}

Spreading is the opposite of gathering. You use it when an observation
is scattered across multiple rows. For example, take \texttt{table2}: an
observation is a country in a year, but each observation is spread
across two rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table2}
\CommentTok{#> # A tibble: 12 × 4}
\CommentTok{#>       country  year       type     count}
\CommentTok{#>         <chr> <int>      <chr>     <int>}
\CommentTok{#> 1 Afghanistan  1999      cases       745}
\CommentTok{#> 2 Afghanistan  1999 population  19987071}
\CommentTok{#> 3 Afghanistan  2000      cases      2666}
\CommentTok{#> 4 Afghanistan  2000 population  20595360}
\CommentTok{#> 5      Brazil  1999      cases     37737}
\CommentTok{#> 6      Brazil  1999 population 172006362}
\CommentTok{#> # ... with 6 more rows}
\end{Highlighting}
\end{Shaded}

To tidy this up, we first analyse the representation in similar way to
\texttt{gather()}. This time, however, we only need two parameters:

\begin{itemize}
\item
  The column that contains variable names, the \texttt{key} column.
  Here, it's \texttt{type}.
\item
  The column that contains values forms multiple variables, the
  \texttt{value} column. Here it's \texttt{count}.
\end{itemize}

Once we've figured that out, we can use \texttt{spread()}, as shown
programmatically below, and visually in Figure \ref{fig:tidy-spread}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{spread}\NormalTok{(table2, }\DataTypeTok{key =} \NormalTok{type, }\DataTypeTok{value =} \NormalTok{count)}
\CommentTok{#> # A tibble: 6 × 4}
\CommentTok{#>       country  year  cases population}
\CommentTok{#> *       <chr> <int>  <int>      <int>}
\CommentTok{#> 1 Afghanistan  1999    745   19987071}
\CommentTok{#> 2 Afghanistan  2000   2666   20595360}
\CommentTok{#> 3      Brazil  1999  37737  172006362}
\CommentTok{#> 4      Brazil  2000  80488  174504898}
\CommentTok{#> 5       China  1999 212258 1272915272}
\CommentTok{#> 6       China  2000 213766 1280428583}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/tidy-8} 

}

\caption{Spreading `table2` makes it tidy}\label{fig:tidy-spread}
\end{figure}

As you might have guessed from the common \texttt{key} and
\texttt{value} arguments, \texttt{spread()} and \texttt{gather()} are
complements. \texttt{gather()} makes wide tables narrower and longer;
\texttt{spread()} makes long tables shorter and wider.

\subsection{Exercises}\label{exercises-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why are \texttt{gather()} and \texttt{spread()} not perfectly
  symmetrical?\\
  Carefully consider the following example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stocks <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{year   =} \KeywordTok{c}\NormalTok{(}\DecValTok{2015}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DecValTok{2016}\NormalTok{),}
  \DataTypeTok{half  =} \KeywordTok{c}\NormalTok{(   }\DecValTok{1}\NormalTok{,    }\DecValTok{2}\NormalTok{,     }\DecValTok{1}\NormalTok{,    }\DecValTok{2}\NormalTok{),}
  \DataTypeTok{return =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.88}\NormalTok{, }\FloatTok{0.59}\NormalTok{, }\FloatTok{0.92}\NormalTok{, }\FloatTok{0.17}\NormalTok{)}
\NormalTok{)}
\NormalTok{stocks %>%}\StringTok{ }
\StringTok{  }\KeywordTok{spread}\NormalTok{(year, return) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{"year"}\NormalTok{, }\StringTok{"return"}\NormalTok{, }\StringTok{`}\DataTypeTok{2015}\StringTok{`}\NormalTok{:}\StringTok{`}\DataTypeTok{2016}\StringTok{`}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  (Hint: look at the variable types and think about column
  \emph{names}.)

  Both \texttt{spread()} and \texttt{gather()} have a \texttt{convert}
  argument. What does it do?
\item
  Why does this code fail?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table4a %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\DecValTok{1999}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DataTypeTok{key =} \StringTok{"year"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"cases"}\NormalTok{)}
\CommentTok{#> Error in eval(expr, envir, enclos): Position must be between 0 and n}
\end{Highlighting}
\end{Shaded}
\item
  Why does spreading this tibble fail? How could you add a new column to
  fix the problem?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~name,             ~key,    ~value,}
  \CommentTok{#-----------------|--------|------}
  \StringTok{"Phillip Woods"}\NormalTok{,   }\StringTok{"age"}\NormalTok{,       }\DecValTok{45}\NormalTok{,}
  \StringTok{"Phillip Woods"}\NormalTok{,   }\StringTok{"height"}\NormalTok{,   }\DecValTok{186}\NormalTok{,}
  \StringTok{"Phillip Woods"}\NormalTok{,   }\StringTok{"age"}\NormalTok{,       }\DecValTok{50}\NormalTok{,}
  \StringTok{"Jessica Cordero"}\NormalTok{, }\StringTok{"age"}\NormalTok{,       }\DecValTok{37}\NormalTok{,}
  \StringTok{"Jessica Cordero"}\NormalTok{, }\StringTok{"height"}\NormalTok{,   }\DecValTok{156}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  Tidy the simple tibble below. Do you need to spread or gather it? What
  are the variables?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preg <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~pregnant, ~male, ~female,}
  \StringTok{"yes"}\NormalTok{,     }\OtherTok{NA}\NormalTok{,    }\DecValTok{10}\NormalTok{,}
  \StringTok{"no"}\NormalTok{,      }\DecValTok{20}\NormalTok{,    }\DecValTok{12}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{Separating and uniting}\label{separating-and-uniting}

So far you've learned how to tidy \texttt{table2} and \texttt{table4},
but not \texttt{table3}. \texttt{table3} has a different problem: we
have one column (\texttt{rate}) that contains two variables
(\texttt{cases} and \texttt{population}). To fix this problem, we'll
need the \texttt{separate()} function. You'll also learn about the
complement of \texttt{separate()}: \texttt{unite()}, which you use if a
single variable is spread across multiple columns.

\subsection{Separate}\label{separate}

\texttt{separate()} pulls apart one column into multiple columns, by
splitting wherever a separator character appears. Take \texttt{table3}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table3}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>       country  year              rate}
\CommentTok{#> *       <chr> <int>             <chr>}
\CommentTok{#> 1 Afghanistan  1999      745/19987071}
\CommentTok{#> 2 Afghanistan  2000     2666/20595360}
\CommentTok{#> 3      Brazil  1999   37737/172006362}
\CommentTok{#> 4      Brazil  2000   80488/174504898}
\CommentTok{#> 5       China  1999 212258/1272915272}
\CommentTok{#> 6       China  2000 213766/1280428583}
\end{Highlighting}
\end{Shaded}

The \texttt{rate} column contains both \texttt{cases} and
\texttt{population} variables, and we need to split it into two
variables. \texttt{separate()} takes the name of the column to separate,
and the names of the columns to separate into, as shown in Figure
\ref{fig:tidy-separate} and the code below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(rate, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"cases"}\NormalTok{, }\StringTok{"population"}\NormalTok{))}
\CommentTok{#> # A tibble: 6 × 4}
\CommentTok{#>       country  year  cases population}
\CommentTok{#> *       <chr> <int>  <chr>      <chr>}
\CommentTok{#> 1 Afghanistan  1999    745   19987071}
\CommentTok{#> 2 Afghanistan  2000   2666   20595360}
\CommentTok{#> 3      Brazil  1999  37737  172006362}
\CommentTok{#> 4      Brazil  2000  80488  174504898}
\CommentTok{#> 5       China  1999 212258 1272915272}
\CommentTok{#> 6       China  2000 213766 1280428583}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/tidy-17} 

}

\caption{Separating `table3` makes it tidy}\label{fig:tidy-separate}
\end{figure}

By default, \texttt{separate()} will split values wherever it sees a
non-alphanumeric character (i.e.~a character that isn't a number or
letter). For example, in the code above, \texttt{separate()} split the
values of \texttt{rate} at the forward slash characters. If you wish to
use a specific character to separate a column, you can pass the
character to the \texttt{sep} argument of \texttt{separate()}. For
example, we could rewrite the code above as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(rate, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"cases"}\NormalTok{, }\StringTok{"population"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

(Formally, \texttt{sep} is a regular expression, which you'll learn more
about in \protect\hyperlink{strings}{strings}.)

Look carefully at the column types: you'll notice that \texttt{case} and
\texttt{population} are character columns. This is the default behaviour
in \texttt{separate()}: it leaves the type of the column as is. Here,
however, it's not very useful as those really are numbers. We can ask
\texttt{separate()} to try and convert to better types using
\texttt{convert\ =\ TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(rate, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"cases"}\NormalTok{, }\StringTok{"population"}\NormalTok{), }\DataTypeTok{convert =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 4}
\CommentTok{#>       country  year  cases population}
\CommentTok{#> *       <chr> <int>  <int>      <int>}
\CommentTok{#> 1 Afghanistan  1999    745   19987071}
\CommentTok{#> 2 Afghanistan  2000   2666   20595360}
\CommentTok{#> 3      Brazil  1999  37737  172006362}
\CommentTok{#> 4      Brazil  2000  80488  174504898}
\CommentTok{#> 5       China  1999 212258 1272915272}
\CommentTok{#> 6       China  2000 213766 1280428583}
\end{Highlighting}
\end{Shaded}

You can also pass a vector of integers to \texttt{sep}.
\texttt{separate()} will interpret the integers as positions to split
at. Positive values start at 1 on the far-left of the strings; negative
value start at -1 on the far-right of the strings. When using integers
to separate strings, the length of \texttt{sep} should be one less than
the number of names in \texttt{into}.

You can use this arrangement to separate the last two digits of each
year. This make this data less tidy, but is useful in other cases, as
you'll see in a little bit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(year, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"century"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }\DataTypeTok{sep =} \DecValTok{2}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 4}
\CommentTok{#>       country century  year              rate}
\CommentTok{#> *       <chr>   <chr> <chr>             <chr>}
\CommentTok{#> 1 Afghanistan      19    99      745/19987071}
\CommentTok{#> 2 Afghanistan      20    00     2666/20595360}
\CommentTok{#> 3      Brazil      19    99   37737/172006362}
\CommentTok{#> 4      Brazil      20    00   80488/174504898}
\CommentTok{#> 5       China      19    99 212258/1272915272}
\CommentTok{#> 6       China      20    00 213766/1280428583}
\end{Highlighting}
\end{Shaded}

\subsection{Unite}\label{unite}

\texttt{unite()} is the inverse of \texttt{separate()}: it combines
multiple columns into a single column. You'll need it much less
frequently than \texttt{separate()}, but it's still a useful tool to
have in your back pocket.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/tidy-18} 

}

\caption{Uniting `table5` makes it tidy}\label{fig:tidy-unite}
\end{figure}

We can use \texttt{unite()} to rejoin the \emph{century} and \emph{year}
columns that we created in the last example. That data is saved as
\texttt{tidyr::table5}. \texttt{unite()} takes a data frame, the name of
the new variable to create, and a set of columns to combine, again
specified in \texttt{dplyr::select()} style:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table5 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unite}\NormalTok{(new, century, year)}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>       country   new              rate}
\CommentTok{#> *       <chr> <chr>             <chr>}
\CommentTok{#> 1 Afghanistan 19_99      745/19987071}
\CommentTok{#> 2 Afghanistan 20_00     2666/20595360}
\CommentTok{#> 3      Brazil 19_99   37737/172006362}
\CommentTok{#> 4      Brazil 20_00   80488/174504898}
\CommentTok{#> 5       China 19_99 212258/1272915272}
\CommentTok{#> 6       China 20_00 213766/1280428583}
\end{Highlighting}
\end{Shaded}

In this case we also need to use the \texttt{sep} argument. The default
will place an underscore (\texttt{\_}) between the values from different
columns. Here we don't want any separator so we use \texttt{""}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table5 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unite}\NormalTok{(new, century, year, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>       country   new              rate}
\CommentTok{#> *       <chr> <chr>             <chr>}
\CommentTok{#> 1 Afghanistan  1999      745/19987071}
\CommentTok{#> 2 Afghanistan  2000     2666/20595360}
\CommentTok{#> 3      Brazil  1999   37737/172006362}
\CommentTok{#> 4      Brazil  2000   80488/174504898}
\CommentTok{#> 5       China  1999 212258/1272915272}
\CommentTok{#> 6       China  2000 213766/1280428583}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What do the \texttt{extra} and \texttt{fill} arguments do in
  \texttt{separate()}? Experiment with the various options for the
  following two toy datasets.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"a,b,c"}\NormalTok{, }\StringTok{"d,e,f,g"}\NormalTok{, }\StringTok{"h,i,j"}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(x, }\KeywordTok{c}\NormalTok{(}\StringTok{"one"}\NormalTok{, }\StringTok{"two"}\NormalTok{, }\StringTok{"three"}\NormalTok{))}

\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"a,b,c"}\NormalTok{, }\StringTok{"d,e"}\NormalTok{, }\StringTok{"f,g,i"}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(x, }\KeywordTok{c}\NormalTok{(}\StringTok{"one"}\NormalTok{, }\StringTok{"two"}\NormalTok{, }\StringTok{"three"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\item
  Both \texttt{unite()} and \texttt{separate()} have a \texttt{remove}
  argument. What does it do? Why would you set it to \texttt{FALSE}?
\item
  Compare and contrast \texttt{separate()} and \texttt{extract()}. Why
  are there three variations of separation (by position, by separator,
  and with groups), but only one unite?
\end{enumerate}

\section{Missing values}\label{missing-values-3}

Changing the representation of a dataset brings up an important subtlety
of missing values. Surprisingly, a value can be missing in one of two
possible ways:

\begin{itemize}
\tightlist
\item
  \textbf{Explicitly}, i.e.~flagged with \texttt{NA}.
\item
  \textbf{Implicitly}, i.e.~simply not present in the data.
\end{itemize}

Let's illustrate this idea with a very simple data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stocks <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{year   =} \KeywordTok{c}\NormalTok{(}\DecValTok{2015}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DecValTok{2016}\NormalTok{),}
  \DataTypeTok{qtr    =} \KeywordTok{c}\NormalTok{(   }\DecValTok{1}\NormalTok{,    }\DecValTok{2}\NormalTok{,    }\DecValTok{3}\NormalTok{,    }\DecValTok{4}\NormalTok{,    }\DecValTok{2}\NormalTok{,    }\DecValTok{3}\NormalTok{,    }\DecValTok{4}\NormalTok{),}
  \DataTypeTok{return =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.88}\NormalTok{, }\FloatTok{0.59}\NormalTok{, }\FloatTok{0.35}\NormalTok{,   }\OtherTok{NA}\NormalTok{, }\FloatTok{0.92}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\FloatTok{2.66}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are two missing values in this dataset:

\begin{itemize}
\item
  The return for the fourth quarter of 2015 is explicitly missing,
  because the cell where its value should be instead contains
  \texttt{NA}.
\item
  The return for the first quarter of 2016 is implicitly missing,
  because it simply does not appear in the dataset.
\end{itemize}

One way to think about the difference is with this Zen-like koan: An
explicit missing value is the presence of an absence; an implicit
missing value is the absence of a presence.

The way that a dataset is represented can make implicit values explicit.
For example, we can make the implicit missing value explicit by putting
years in the columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stocks %>%}\StringTok{ }
\StringTok{  }\KeywordTok{spread}\NormalTok{(year, return)}
\CommentTok{#> # A tibble: 4 × 3}
\CommentTok{#>     qtr `2015` `2016`}
\CommentTok{#> * <dbl>  <dbl>  <dbl>}
\CommentTok{#> 1     1   1.88     NA}
\CommentTok{#> 2     2   0.59   0.92}
\CommentTok{#> 3     3   0.35   0.17}
\CommentTok{#> 4     4     NA   2.66}
\end{Highlighting}
\end{Shaded}

Because these explicit missing values may not be important in other
representations of the data, you can set \texttt{na.rm\ =\ TRUE} in
\texttt{gather()} to turn explicit missing values implicit:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stocks %>%}\StringTok{ }
\StringTok{  }\KeywordTok{spread}\NormalTok{(year, return) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(year, return, }\StringTok{`}\DataTypeTok{2015}\StringTok{`}\NormalTok{:}\StringTok{`}\DataTypeTok{2016}\StringTok{`}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>     qtr  year return}
\CommentTok{#> * <dbl> <chr>  <dbl>}
\CommentTok{#> 1     1  2015   1.88}
\CommentTok{#> 2     2  2015   0.59}
\CommentTok{#> 3     3  2015   0.35}
\CommentTok{#> 4     2  2016   0.92}
\CommentTok{#> 5     3  2016   0.17}
\CommentTok{#> 6     4  2016   2.66}
\end{Highlighting}
\end{Shaded}

Another important tool for making missing values explicit in tidy data
is \texttt{complete()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stocks %>%}\StringTok{ }
\StringTok{  }\KeywordTok{complete}\NormalTok{(year, qtr)}
\CommentTok{#> # A tibble: 8 × 3}
\CommentTok{#>    year   qtr return}
\CommentTok{#>   <dbl> <dbl>  <dbl>}
\CommentTok{#> 1  2015     1   1.88}
\CommentTok{#> 2  2015     2   0.59}
\CommentTok{#> 3  2015     3   0.35}
\CommentTok{#> 4  2015     4     NA}
\CommentTok{#> 5  2016     1     NA}
\CommentTok{#> 6  2016     2   0.92}
\CommentTok{#> # ... with 2 more rows}
\end{Highlighting}
\end{Shaded}

\texttt{complete()} takes a set of columns, and finds all unique
combinations. It then ensures the original dataset contains all those
values, filling in explicit \texttt{NA}s where necessary.

There's one other important tool that you should know for working with
missing values. Sometimes when a data source has primarily been used for
data entry, missing values indicate that the previous value should be
carried forward:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{treatment <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~}\StringTok{ }\NormalTok{person,           ~}\StringTok{ }\NormalTok{treatment, ~response,}
  \StringTok{"Derrick Whitmore"}\NormalTok{, }\DecValTok{1}\NormalTok{,           }\DecValTok{7}\NormalTok{,}
  \OtherTok{NA}\NormalTok{,                 }\DecValTok{2}\NormalTok{,           }\DecValTok{10}\NormalTok{,}
  \OtherTok{NA}\NormalTok{,                 }\DecValTok{3}\NormalTok{,           }\DecValTok{9}\NormalTok{,}
  \StringTok{"Katherine Burke"}\NormalTok{,  }\DecValTok{1}\NormalTok{,           }\DecValTok{4}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can fill in these missing values with \texttt{fill()}. It takes a
set of columns where you want missing values to be replaced by the most
recent non-missing value (sometimes called last observation carried
forward).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{treatment %>%}\StringTok{ }
\StringTok{  }\KeywordTok{fill}\NormalTok{(person)}
\CommentTok{#> # A tibble: 4 × 3}
\CommentTok{#>             person treatment response}
\CommentTok{#>              <chr>     <dbl>    <dbl>}
\CommentTok{#> 1 Derrick Whitmore         1        7}
\CommentTok{#> 2 Derrick Whitmore         2       10}
\CommentTok{#> 3 Derrick Whitmore         3        9}
\CommentTok{#> 4  Katherine Burke         1        4}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compare and contrast the \texttt{fill} arguments to \texttt{spread()}
  and \texttt{complete()}.
\item
  What does the direction argument to \texttt{fill()} do?
\end{enumerate}

\section{Case Study}\label{case-study}

To finish off the chapter, let's pull together everything you've learned
to tackle a realistic data tidying problem. The \texttt{tidyr::who}
dataset contains tuberculosis (TB) cases broken down by year, country,
age, gender, and diagnosis method. The data comes from the \emph{2014
World Health Organization Global Tuberculosis Report}, available at
\url{http://www.who.int/tb/country/data/download/en/}.

There's a wealth of epidemiological information in this dataset, but
it's challenging to work with the data in the form that it's provided:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who}
\CommentTok{#> # A tibble: 7,240 × 60}
\CommentTok{#>       country  iso2  iso3  year new_sp_m014 new_sp_m1524 new_sp_m2534}
\CommentTok{#>         <chr> <chr> <chr> <int>       <int>        <int>        <int>}
\CommentTok{#> 1 Afghanistan    AF   AFG  1980          NA           NA           NA}
\CommentTok{#> 2 Afghanistan    AF   AFG  1981          NA           NA           NA}
\CommentTok{#> 3 Afghanistan    AF   AFG  1982          NA           NA           NA}
\CommentTok{#> 4 Afghanistan    AF   AFG  1983          NA           NA           NA}
\CommentTok{#> 5 Afghanistan    AF   AFG  1984          NA           NA           NA}
\CommentTok{#> 6 Afghanistan    AF   AFG  1985          NA           NA           NA}
\CommentTok{#> # ... with 7,234 more rows, and 53 more variables: new_sp_m3544 <int>,}
\CommentTok{#> #   new_sp_m4554 <int>, new_sp_m5564 <int>, new_sp_m65 <int>,}
\CommentTok{#> #   new_sp_f014 <int>, new_sp_f1524 <int>, new_sp_f2534 <int>,}
\CommentTok{#> #   new_sp_f3544 <int>, new_sp_f4554 <int>, new_sp_f5564 <int>,}
\CommentTok{#> #   new_sp_f65 <int>, new_sn_m014 <int>, new_sn_m1524 <int>,}
\CommentTok{#> #   new_sn_m2534 <int>, new_sn_m3544 <int>, new_sn_m4554 <int>,}
\CommentTok{#> #   new_sn_m5564 <int>, new_sn_m65 <int>, new_sn_f014 <int>,}
\CommentTok{#> #   new_sn_f1524 <int>, new_sn_f2534 <int>, new_sn_f3544 <int>,}
\CommentTok{#> #   new_sn_f4554 <int>, new_sn_f5564 <int>, new_sn_f65 <int>,}
\CommentTok{#> #   new_ep_m014 <int>, new_ep_m1524 <int>, new_ep_m2534 <int>,}
\CommentTok{#> #   new_ep_m3544 <int>, new_ep_m4554 <int>, new_ep_m5564 <int>,}
\CommentTok{#> #   new_ep_m65 <int>, new_ep_f014 <int>, new_ep_f1524 <int>,}
\CommentTok{#> #   new_ep_f2534 <int>, new_ep_f3544 <int>, new_ep_f4554 <int>,}
\CommentTok{#> #   new_ep_f5564 <int>, new_ep_f65 <int>, newrel_m014 <int>,}
\CommentTok{#> #   newrel_m1524 <int>, newrel_m2534 <int>, newrel_m3544 <int>,}
\CommentTok{#> #   newrel_m4554 <int>, newrel_m5564 <int>, newrel_m65 <int>,}
\CommentTok{#> #   newrel_f014 <int>, newrel_f1524 <int>, newrel_f2534 <int>,}
\CommentTok{#> #   newrel_f3544 <int>, newrel_f4554 <int>, newrel_f5564 <int>,}
\CommentTok{#> #   newrel_f65 <int>}
\end{Highlighting}
\end{Shaded}

This is a very typical real-life example dataset. It contains redundant
columns, odd variable codes, and many missing values. In short,
\texttt{who} is messy, and we'll need multiple steps to tidy it. Like
dplyr, tidyr is designed so that each function does one thing well. That
means in real-life situations you'll usually need to string together
multiple verbs into a pipeline.

The best place to start is almost always to gather together the columns
that are not variables. Let's have a look at what we've got:

\begin{itemize}
\item
  It looks like \texttt{country}, \texttt{iso2}, and \texttt{iso3} are
  three variables that redundantly specify the country.
\item
  \texttt{year} is clearly also a variable.
\item
  We don't know what all the other columns are yet, but given the
  structure in the variable names (e.g. \texttt{new\_sp\_m014},
  \texttt{new\_ep\_m014}, \texttt{new\_ep\_f014}) these are likely to be
  values, not variables.
\end{itemize}

So we need to gather together all the columns from
\texttt{new\_sp\_m014} to \texttt{newrel\_f65}. We don't know what those
values represent yet, so we'll give them the generic name
\texttt{"key"}. We know the cells represent the count of cases, so we'll
use the variable \texttt{cases}. There are a lot of missing values in
the current representation, so for now we'll use \texttt{na.rm} just so
we can focus on the values that are present.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who1 <-}\StringTok{ }\NormalTok{who %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(new_sp_m014:newrel_f65, }\DataTypeTok{key =} \StringTok{"key"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"cases"}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{who1}
\CommentTok{#> # A tibble: 76,046 × 6}
\CommentTok{#>       country  iso2  iso3  year         key cases}
\CommentTok{#> *       <chr> <chr> <chr> <int>       <chr> <int>}
\CommentTok{#> 1 Afghanistan    AF   AFG  1997 new_sp_m014     0}
\CommentTok{#> 2 Afghanistan    AF   AFG  1998 new_sp_m014    30}
\CommentTok{#> 3 Afghanistan    AF   AFG  1999 new_sp_m014     8}
\CommentTok{#> 4 Afghanistan    AF   AFG  2000 new_sp_m014    52}
\CommentTok{#> 5 Afghanistan    AF   AFG  2001 new_sp_m014   129}
\CommentTok{#> 6 Afghanistan    AF   AFG  2002 new_sp_m014    90}
\CommentTok{#> # ... with 7.604e+04 more rows}
\end{Highlighting}
\end{Shaded}

We can get some hint of the structure of the values in the new
\texttt{key} column by counting them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who1 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(key)}
\CommentTok{#> # A tibble: 56 × 2}
\CommentTok{#>            key     n}
\CommentTok{#>          <chr> <int>}
\CommentTok{#> 1  new_ep_f014  1032}
\CommentTok{#> 2 new_ep_f1524  1021}
\CommentTok{#> 3 new_ep_f2534  1021}
\CommentTok{#> 4 new_ep_f3544  1021}
\CommentTok{#> 5 new_ep_f4554  1017}
\CommentTok{#> 6 new_ep_f5564  1017}
\CommentTok{#> # ... with 50 more rows}
\end{Highlighting}
\end{Shaded}

You might be able to parse this out by yourself with a little thought
and some experimentation, but luckily we have the data dictionary handy.
It tells us:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The first three letters of each column denote whether the column
  contains new or old cases of TB. In this dataset, each column contains
  new cases.
\item
  The next two letters describe the type of TB:

  \begin{itemize}
  \tightlist
  \item
    \texttt{rel} stands for cases of relapse
  \item
    \texttt{ep} stands for cases of extrapulmonary TB
  \item
    \texttt{sn} stands for cases of pulmonary TB that could not be
    diagnosed by a pulmonary smear (smear negative)
  \item
    \texttt{sp} stands for cases of pulmonary TB that could be diagnosed
    be a pulmonary smear (smear positive)
  \end{itemize}
\item
  The sixth letter gives the sex of TB patients. The dataset groups
  cases by males (\texttt{m}) and females (\texttt{f}).
\item
  The remaining numbers gives the age group. The dataset groups cases
  into seven age groups:

  \begin{itemize}
  \tightlist
  \item
    \texttt{014} = 0 -- 14 years old
  \item
    \texttt{1524} = 15 -- 24 years old
  \item
    \texttt{2534} = 25 -- 34 years old
  \item
    \texttt{3544} = 35 -- 44 years old
  \item
    \texttt{4554} = 45 -- 54 years old
  \item
    \texttt{5564} = 55 -- 64 years old
  \item
    \texttt{65} = 65 or older
  \end{itemize}
\end{enumerate}

We need to make a minor fix to the format of the column names:
unfortunately the names are slightly inconsistent because instead of
\texttt{new\_rel} we have \texttt{newrel} (it's hard to spot this here
but if you don't fix it we'll get errors in subsequent steps). You'll
learn about \texttt{str\_replace()} in
\protect\hyperlink{strings}{strings}, but the basic idea is pretty
simple: replace the characters ``newrel'' with ``new\_rel''. This makes
all variable names consistent.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who2 <-}\StringTok{ }\NormalTok{who1 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{key =} \NormalTok{stringr::}\KeywordTok{str_replace}\NormalTok{(key, }\StringTok{"newrel"}\NormalTok{, }\StringTok{"new_rel"}\NormalTok{))}
\NormalTok{who2}
\CommentTok{#> # A tibble: 76,046 × 6}
\CommentTok{#>       country  iso2  iso3  year         key cases}
\CommentTok{#>         <chr> <chr> <chr> <int>       <chr> <int>}
\CommentTok{#> 1 Afghanistan    AF   AFG  1997 new_sp_m014     0}
\CommentTok{#> 2 Afghanistan    AF   AFG  1998 new_sp_m014    30}
\CommentTok{#> 3 Afghanistan    AF   AFG  1999 new_sp_m014     8}
\CommentTok{#> 4 Afghanistan    AF   AFG  2000 new_sp_m014    52}
\CommentTok{#> 5 Afghanistan    AF   AFG  2001 new_sp_m014   129}
\CommentTok{#> 6 Afghanistan    AF   AFG  2002 new_sp_m014    90}
\CommentTok{#> # ... with 7.604e+04 more rows}
\end{Highlighting}
\end{Shaded}

We can separate the values in each code with two passes of
\texttt{separate()}. The first pass will split the codes at each
underscore.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who3 <-}\StringTok{ }\NormalTok{who2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(key, }\KeywordTok{c}\NormalTok{(}\StringTok{"new"}\NormalTok{, }\StringTok{"type"}\NormalTok{, }\StringTok{"sexage"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"_"}\NormalTok{)}
\NormalTok{who3}
\CommentTok{#> # A tibble: 76,046 × 8}
\CommentTok{#>       country  iso2  iso3  year   new  type sexage cases}
\CommentTok{#> *       <chr> <chr> <chr> <int> <chr> <chr>  <chr> <int>}
\CommentTok{#> 1 Afghanistan    AF   AFG  1997   new    sp   m014     0}
\CommentTok{#> 2 Afghanistan    AF   AFG  1998   new    sp   m014    30}
\CommentTok{#> 3 Afghanistan    AF   AFG  1999   new    sp   m014     8}
\CommentTok{#> 4 Afghanistan    AF   AFG  2000   new    sp   m014    52}
\CommentTok{#> 5 Afghanistan    AF   AFG  2001   new    sp   m014   129}
\CommentTok{#> 6 Afghanistan    AF   AFG  2002   new    sp   m014    90}
\CommentTok{#> # ... with 7.604e+04 more rows}
\end{Highlighting}
\end{Shaded}

Then we might as well drop the \texttt{new} column because it's constant
in this dataset. While we're dropping columns, let's also drop
\texttt{iso2} and \texttt{iso3} since they're redundant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(new)}
\CommentTok{#> # A tibble: 1 × 2}
\CommentTok{#>     new     n}
\CommentTok{#>   <chr> <int>}
\CommentTok{#> 1   new 76046}
\NormalTok{who4 <-}\StringTok{ }\NormalTok{who3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(-new, -iso2, -iso3)}
\end{Highlighting}
\end{Shaded}

Next we'll separate \texttt{sexage} into \texttt{sex} and \texttt{age}
by splitting after the first character:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who5 <-}\StringTok{ }\NormalTok{who4 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(sexage, }\KeywordTok{c}\NormalTok{(}\StringTok{"sex"}\NormalTok{, }\StringTok{"age"}\NormalTok{), }\DataTypeTok{sep =} \DecValTok{1}\NormalTok{)}
\NormalTok{who5}
\CommentTok{#> # A tibble: 76,046 × 6}
\CommentTok{#>       country  year  type   sex   age cases}
\CommentTok{#> *       <chr> <int> <chr> <chr> <chr> <int>}
\CommentTok{#> 1 Afghanistan  1997    sp     m   014     0}
\CommentTok{#> 2 Afghanistan  1998    sp     m   014    30}
\CommentTok{#> 3 Afghanistan  1999    sp     m   014     8}
\CommentTok{#> 4 Afghanistan  2000    sp     m   014    52}
\CommentTok{#> 5 Afghanistan  2001    sp     m   014   129}
\CommentTok{#> 6 Afghanistan  2002    sp     m   014    90}
\CommentTok{#> # ... with 7.604e+04 more rows}
\end{Highlighting}
\end{Shaded}

The \texttt{who} dataset is now tidy!

I've shown you the code a piece at a time, assigning each interim result
to a new variable. This typically isn't how you'd work interactively.
Instead, you'd gradually build up a complex pipe:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{who %>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(code, value, new_sp_m014:newrel_f65, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{code =} \NormalTok{stringr::}\KeywordTok{str_replace}\NormalTok{(code, }\StringTok{"newrel"}\NormalTok{, }\StringTok{"new_rel"}\NormalTok{)) %>%}
\StringTok{  }\KeywordTok{separate}\NormalTok{(code, }\KeywordTok{c}\NormalTok{(}\StringTok{"new"}\NormalTok{, }\StringTok{"var"}\NormalTok{, }\StringTok{"sexage"}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(-new, -iso2, -iso3) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(sexage, }\KeywordTok{c}\NormalTok{(}\StringTok{"sex"}\NormalTok{, }\StringTok{"age"}\NormalTok{), }\DataTypeTok{sep =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In this case study I set \texttt{na.rm\ =\ TRUE} just to make it
  easier to check that we had the correct values. Is this reasonable?
  Think about how missing values are represented in this dataset. Are
  there implicit missing values? What's the difference between an
  \texttt{NA} and zero?
\item
  What happens if you neglect the \texttt{mutate()} step?
  (\texttt{mutate(key\ =\ stringr::str\_replace(key,\ "newrel",\ "new\_rel"))})
\item
  I claimed that \texttt{iso2} and \texttt{iso3} were redundant with
  \texttt{country}. Confirm this claim.
\item
  For each country, year, and sex compute the total number of cases of
  TB. Make an informative visualisation of the data.
\end{enumerate}

\section{Non-tidy data}\label{non-tidy-data}

Before we continue on to other topics, it's worth talking briefly about
non-tidy data. Earlier in the chapter, I used the pejorative term
``messy'' to refer to non-tidy data. That's an oversimplification: there
are lots of useful and well-founded data structures that are not tidy
data. There are two main reasons to use other data structures:

\begin{itemize}
\item
  Alternative representations may have substantial performance or space
  advantages.
\item
  Specialised fields have evolved their own conventions for storing data
  that may be quite different to the conventions of tidy data.
\end{itemize}

Either of these reasons means you'll need something other than a tibble
(or data frame). If your data does fit naturally into a rectangular
structure composed of observations and variables, I think tidy data
should be your default choice. But there are good reasons to use other
structures; tidy data is not the only way.

If you'd like to learn more about non-tidy data, I'd highly recommend
this thoughtful blog post by Jeff Leek:
\url{http://simplystatistics.org/2016/02/17/non-tidy-data/}

\hypertarget{relational-data}{\chapter{Relational
data}\label{relational-data}}

\section{Introduction}\label{introduction-7}

It's rare that a data analysis involves only a single table of data.
Typically you have many tables of data, and you must combine them to
answer the questions that you're interested in. Collectively, multiple
tables of data are called \textbf{relational data} because it is the
relations, not just the individual datasets, that are important.

Relations are always defined between a pair of tables. All other
relations are built up from this simple idea: the relations of three or
more tables are always a property of the relations between each pair.
Sometimes both elements of a pair can be the same table! This is needed
if, for example, you have a table of people, and each person has a
reference to their parents.

To work with relational data you need verbs that work with pairs of
tables. There are three families of verbs designed to work with
relational data:

\begin{itemize}
\item
  \textbf{Mutating joins}, which add new variables to one data frame
  from matching observations in another.
\item
  \textbf{Filtering joins}, which filter observations from one data
  frame based on whether or not they match an observation in the other
  table.
\item
  \textbf{Set operations}, which treat observations as if they were set
  elements.
\end{itemize}

The most common place to find relational data is in a \emph{relational}
database management system (or RDBMS), a term that encompasses almost
all modern databases. If you've used a database before, you've almost
certainly used SQL. If so, you should find the concepts in this chapter
familiar, although their expression in dplyr is a little different.
Generally, dplyr is a little easier to use than SQL because dplyr is
specialised to do data analysis: it makes common data analysis
operations easier, at the expense of making it more difficult to do
other things that aren't commonly needed for data analysis.

\subsection{Prerequisites}\label{prerequisites-7}

We will explore relational data from \texttt{nycflights13} using the
two-table verbs from dplyr.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(nycflights13)}
\end{Highlighting}
\end{Shaded}

\section{nycflights13}\label{nycflights13-relational}

We will use the nycflights13 package to learn about relational data.
nycflights13 contains four tibbles that are related to the
\texttt{flights} table that you used in
\protect\hyperlink{transform}{data transformation}:

\begin{itemize}
\item
  \texttt{airlines} lets you look up the full carrier name from its
  abbreviated code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airlines}
\CommentTok{#> # A tibble: 16 × 2}
\CommentTok{#>   carrier                     name}
\CommentTok{#>     <chr>                    <chr>}
\CommentTok{#> 1      9E        Endeavor Air Inc.}
\CommentTok{#> 2      AA   American Airlines Inc.}
\CommentTok{#> 3      AS     Alaska Airlines Inc.}
\CommentTok{#> 4      B6          JetBlue Airways}
\CommentTok{#> 5      DL     Delta Air Lines Inc.}
\CommentTok{#> 6      EV ExpressJet Airlines Inc.}
\CommentTok{#> # ... with 10 more rows}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{airports} gives information about each airport, identified by
  the \texttt{faa} airport code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airports}
\CommentTok{#> # A tibble: 1,396 × 7}
\CommentTok{#>     faa                           name   lat   lon   alt    tz   dst}
\CommentTok{#>   <chr>                          <chr> <dbl> <dbl> <int> <dbl> <chr>}
\CommentTok{#> 1   04G              Lansdowne Airport  41.1 -80.6  1044    -5     A}
\CommentTok{#> 2   06A  Moton Field Municipal Airport  32.5 -85.7   264    -5     A}
\CommentTok{#> 3   06C            Schaumburg Regional  42.0 -88.1   801    -6     A}
\CommentTok{#> 4   06N                Randall Airport  41.4 -74.4   523    -5     A}
\CommentTok{#> 5   09J          Jekyll Island Airport  31.1 -81.4    11    -4     A}
\CommentTok{#> 6   0A9 Elizabethton Municipal Airport  36.4 -82.2  1593    -4     A}
\CommentTok{#> # ... with 1,390 more rows}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{planes} gives information about each plane, identified by its
  \texttt{tailnum}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{planes}
\CommentTok{#> # A tibble: 3,322 × 9}
\CommentTok{#>   tailnum  year                    type     manufacturer     model engines}
\CommentTok{#>     <chr> <int>                   <chr>            <chr>     <chr>   <int>}
\CommentTok{#> 1  N10156  2004 Fixed wing multi engine          EMBRAER EMB-145XR       2}
\CommentTok{#> 2  N102UW  1998 Fixed wing multi engine AIRBUS INDUSTRIE  A320-214       2}
\CommentTok{#> 3  N103US  1999 Fixed wing multi engine AIRBUS INDUSTRIE  A320-214       2}
\CommentTok{#> 4  N104UW  1999 Fixed wing multi engine AIRBUS INDUSTRIE  A320-214       2}
\CommentTok{#> 5  N10575  2002 Fixed wing multi engine          EMBRAER EMB-145LR       2}
\CommentTok{#> 6  N105UW  1999 Fixed wing multi engine AIRBUS INDUSTRIE  A320-214       2}
\CommentTok{#> # ... with 3,316 more rows, and 3 more variables: seats <int>,}
\CommentTok{#> #   speed <int>, engine <chr>}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{weather} gives the weather at each NYC airport for each hour:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weather}
\CommentTok{#> # A tibble: 26,130 × 15}
\CommentTok{#>   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed}
\CommentTok{#>    <chr> <dbl> <dbl> <int> <int> <dbl> <dbl> <dbl>    <dbl>      <dbl>}
\CommentTok{#> 1    EWR  2013     1     1     0  37.0  21.9  54.0      230       10.4}
\CommentTok{#> 2    EWR  2013     1     1     1  37.0  21.9  54.0      230       13.8}
\CommentTok{#> 3    EWR  2013     1     1     2  37.9  21.9  52.1      230       12.7}
\CommentTok{#> 4    EWR  2013     1     1     3  37.9  23.0  54.5      230       13.8}
\CommentTok{#> 5    EWR  2013     1     1     4  37.9  24.1  57.0      240       15.0}
\CommentTok{#> 6    EWR  2013     1     1     6  39.0  26.1  59.4      270       10.4}
\CommentTok{#> # ... with 2.612e+04 more rows, and 5 more variables: wind_gust <dbl>,}
\CommentTok{#> #   precip <dbl>, pressure <dbl>, visib <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}
\end{itemize}

One way to show the relationships between the different tables is with a
drawing:

\begin{center}\includegraphics[width=0.7\linewidth]{diagrams/relational-nycflights} \end{center}

This diagram is a little overwhelming, but it's simple compared to some
you'll see in the wild! The key to understanding diagrams like this is
to remember each relation always concerns a pair of tables. You don't
need to understand the whole thing; you just need to understand the
chain of relations between the tables that you are interested in.

For nycflights13:

\begin{itemize}
\item
  \texttt{flights} connects to \texttt{planes} via a single variable,
  \texttt{tailnum}.
\item
  \texttt{flights} connects to \texttt{airlines} through the
  \texttt{carrier} variable.
\item
  \texttt{flights} connects to \texttt{airports} in two ways: via the
  \texttt{origin} and \texttt{dest} variables.
\item
  \texttt{flights} connects to \texttt{weather} via \texttt{origin} (the
  location), and \texttt{year}, \texttt{month}, \texttt{day} and
  \texttt{hour} (the time).
\end{itemize}

\subsection{Exercises}\label{exercises-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Imagine you wanted to draw (approximately) the route each plane flies
  from its origin to its destination. What variables would you need?
  What tables would you need to combine?
\item
  I forgot to draw the relationship between \texttt{weather} and
  \texttt{airports}. What is the relationship and how should it appear
  in the diagram?
\item
  \texttt{weather} only contains information for the origin (NYC)
  airports. If it contained weather records for all airports in the USA,
  what additional relation would it define with \texttt{flights}?
\item
  We know that some days of the year are ``special'', and fewer people
  than usual fly on them. How might you represent that data as a data
  frame? What would be the primary keys of that table? How would it
  connect to the existing tables?
\end{enumerate}

\section{Keys}\label{keys}

The variables used to connect each pair of tables are called
\textbf{keys}. A key is a variable (or set of variables) that uniquely
identifies an observation. In simple cases, a single variable is
sufficient to identify an observation. For example, each plane is
uniquely identified by its \texttt{tailnum}. In other cases, multiple
variables may be needed. For example, to identify an observation in
\texttt{weather} you need five variables: \texttt{year}, \texttt{month},
\texttt{day}, \texttt{hour}, and \texttt{origin}.

There are two types of keys:

\begin{itemize}
\item
  A \textbf{primary key} uniquely identifies an observation in its own
  table. For example, \texttt{planes\$tailnum} is a primary key because
  it uniquely identifies each plane in the \texttt{planes} table.
\item
  A \textbf{foreign key} uniquely identifies an observation in another
  table. For example, the \texttt{flights\$tailnum} is a foreign key
  because it appears in the \texttt{flights} table where it matches each
  flight to a unique plane.
\end{itemize}

A variable can be both a primary key \emph{and} a foreign key. For
example, \texttt{origin} is part of the \texttt{weather} primary key,
and is also a foreign key for the \texttt{airport} table.

Once you've identified the primary keys in your tables, it's good
practice to verify that they do indeed uniquely identify each
observation. One way to do that is to \texttt{count()} the primary keys
and look for entries where \texttt{n} is greater than one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{planes %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(tailnum) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n >}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> # A tibble: 0 × 2}
\CommentTok{#> # ... with 2 variables: tailnum <chr>, n <int>}

\NormalTok{weather %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(year, month, day, hour, origin) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n >}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> Source: local data frame [0 x 6]}
\CommentTok{#> Groups: year, month, day, hour [0]}
\CommentTok{#> }
\CommentTok{#> # ... with 6 variables: year <dbl>, month <dbl>, day <int>, hour <int>,}
\CommentTok{#> #   origin <chr>, n <int>}
\end{Highlighting}
\end{Shaded}

Sometimes a table doesn't have an explicit primary key: each row is an
observation, but no combination of variables reliably identifies it. For
example, what's the primary key in the \texttt{flights} table? You might
think it would be the date plus the flight or tail number, but neither
of those are unique:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(year, month, day, flight) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n >}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> Source: local data frame [29,768 x 5]}
\CommentTok{#> Groups: year, month, day [365]}
\CommentTok{#> }
\CommentTok{#>    year month   day flight     n}
\CommentTok{#>   <int> <int> <int>  <int> <int>}
\CommentTok{#> 1  2013     1     1      1     2}
\CommentTok{#> 2  2013     1     1      3     2}
\CommentTok{#> 3  2013     1     1      4     2}
\CommentTok{#> 4  2013     1     1     11     3}
\CommentTok{#> 5  2013     1     1     15     2}
\CommentTok{#> 6  2013     1     1     21     2}
\CommentTok{#> # ... with 2.976e+04 more rows}

\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(year, month, day, tailnum) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n >}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> Source: local data frame [64,928 x 5]}
\CommentTok{#> Groups: year, month, day [365]}
\CommentTok{#> }
\CommentTok{#>    year month   day tailnum     n}
\CommentTok{#>   <int> <int> <int>   <chr> <int>}
\CommentTok{#> 1  2013     1     1  N0EGMQ     2}
\CommentTok{#> 2  2013     1     1  N11189     2}
\CommentTok{#> 3  2013     1     1  N11536     2}
\CommentTok{#> 4  2013     1     1  N11544     3}
\CommentTok{#> 5  2013     1     1  N11551     2}
\CommentTok{#> 6  2013     1     1  N12540     2}
\CommentTok{#> # ... with 6.492e+04 more rows}
\end{Highlighting}
\end{Shaded}

When starting to work with this data, I had naively assumed that each
flight number would be only used once per day: that would make it much
easier to communicate problems with a specific flight. Unfortunately
that is not the case! If a table lacks a primary key, it's sometimes
useful to add one with \texttt{mutate()} and \texttt{row\_number()}.
That makes it easier to match observations if you've done some filtering
and want to check back in with the original data. This is called a
\textbf{surrogate key}.

A primary key and the corresponding foreign key in another table form a
\textbf{relation}. Relations are typically one-to-many. For example,
each flight has one plane, but each plane has many flights. In other
data, you'll occasionally see a 1-to-1 relationship. You can think of
this as a special case of 1-to-many. You can model many-to-many
relations with a many-to-1 relation plus a 1-to-many relation. For
example, in this data there's a many-to-many relationship between
airlines and airports: each airline flies to many airports; each airport
hosts many airlines.

\subsection{Exercises}\label{exercises-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Add a surrogate key to \texttt{flights}.
\item
  Identify the keys in the following datasets

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \texttt{Lahman::Batting},
  \item
    \texttt{babynames::babynames}
  \item
    \texttt{nasaweather::atmos}
  \item
    \texttt{fueleconomy::vehicles}
  \item
    \texttt{ggplot2::diamonds}
  \end{enumerate}

  (You might need to install some packages and read some documentation.)
\item
  Draw a diagram illustrating the connections between the
  \texttt{Batting}, \texttt{Master}, and \texttt{Salaries} tables in the
  Lahman package. Draw another diagram that shows the relationship
  between \texttt{Master}, \texttt{Managers}, \texttt{AwardsManagers}.

  How would you characterise the relationship between the
  \texttt{Batting}, \texttt{Pitching}, and \texttt{Fielding} tables?
\end{enumerate}

\section{Mutating joins}\label{mutating-joins}

The first tool we'll look at for combining a pair of tables is the
\textbf{mutating join}. A mutating join allows you to combine variables
from two tables. It first matches observations by their keys, then
copies across variables from one table to the other.

Like \texttt{mutate()}, the join functions add variables to the right,
so if you have a lot of variables already, the new variables won't get
printed out. For these examples, we'll make it easier to see what's
going on in the examples by creating a narrower dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights2 <-}\StringTok{ }\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(year:day, hour, origin, dest, tailnum, carrier)}
\NormalTok{flights2}
\CommentTok{#> # A tibble: 336,776 × 8}
\CommentTok{#>    year month   day  hour origin  dest tailnum carrier}
\CommentTok{#>   <int> <int> <int> <dbl>  <chr> <chr>   <chr>   <chr>}
\CommentTok{#> 1  2013     1     1     5    EWR   IAH  N14228      UA}
\CommentTok{#> 2  2013     1     1     5    LGA   IAH  N24211      UA}
\CommentTok{#> 3  2013     1     1     5    JFK   MIA  N619AA      AA}
\CommentTok{#> 4  2013     1     1     5    JFK   BQN  N804JB      B6}
\CommentTok{#> 5  2013     1     1     6    LGA   ATL  N668DN      DL}
\CommentTok{#> 6  2013     1     1     5    EWR   ORD  N39463      UA}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}

(Remember, when you're in RStudio, you can also use \texttt{View()} to
avoid this problem.)

Imagine you want to add the full airline name to the \texttt{flights2}
data. You can combine the \texttt{airlines} and \texttt{flights2} data
frames with \texttt{left\_join()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights2 %>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(-origin, -dest) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(airlines, }\DataTypeTok{by =} \StringTok{"carrier"}\NormalTok{)}
\CommentTok{#> # A tibble: 336,776 × 7}
\CommentTok{#>    year month   day  hour tailnum carrier                   name}
\CommentTok{#>   <int> <int> <int> <dbl>   <chr>   <chr>                  <chr>}
\CommentTok{#> 1  2013     1     1     5  N14228      UA  United Air Lines Inc.}
\CommentTok{#> 2  2013     1     1     5  N24211      UA  United Air Lines Inc.}
\CommentTok{#> 3  2013     1     1     5  N619AA      AA American Airlines Inc.}
\CommentTok{#> 4  2013     1     1     5  N804JB      B6        JetBlue Airways}
\CommentTok{#> 5  2013     1     1     6  N668DN      DL   Delta Air Lines Inc.}
\CommentTok{#> 6  2013     1     1     5  N39463      UA  United Air Lines Inc.}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}

The result of joining airlines to flights2 is an additional variable:
\texttt{name}. This is why I call this type of join a mutating join. In
this case, you could have got to the same place using \texttt{mutate()}
and R's base subsetting:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights2 %>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(-origin, -dest) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{name =} \NormalTok{airlines$name[}\KeywordTok{match}\NormalTok{(carrier, airlines$carrier)])}
\CommentTok{#> # A tibble: 336,776 × 7}
\CommentTok{#>    year month   day  hour tailnum carrier                   name}
\CommentTok{#>   <int> <int> <int> <dbl>   <chr>   <chr>                  <chr>}
\CommentTok{#> 1  2013     1     1     5  N14228      UA  United Air Lines Inc.}
\CommentTok{#> 2  2013     1     1     5  N24211      UA  United Air Lines Inc.}
\CommentTok{#> 3  2013     1     1     5  N619AA      AA American Airlines Inc.}
\CommentTok{#> 4  2013     1     1     5  N804JB      B6        JetBlue Airways}
\CommentTok{#> 5  2013     1     1     6  N668DN      DL   Delta Air Lines Inc.}
\CommentTok{#> 6  2013     1     1     5  N39463      UA  United Air Lines Inc.}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}

But this is hard to generalise when you need to match multiple
variables, and takes close reading to figure out the overall intent.

The following sections explain, in detail, how mutating joins work.
You'll start by learning a useful visual representation of joins. We'll
then use that to explain the four mutating join functions: the inner
join, and the three outer joins. When working with real data, keys don't
always uniquely identify observations, so next we'll talk about what
happens when there isn't a unique match. Finally, you'll learn how to
tell dplyr which variables are the keys for a given join.

\subsection{Understanding joins}\label{understanding-joins}

To help you learn how joins work, I'm going to use a visual
representation:

\begin{center}\includegraphics{diagrams/join-setup} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~key, ~val_x,}
     \DecValTok{1}\NormalTok{, }\StringTok{"x1"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"x2"}\NormalTok{,}
     \DecValTok{3}\NormalTok{, }\StringTok{"x3"}
\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~key, ~val_y,}
     \DecValTok{1}\NormalTok{, }\StringTok{"y1"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"y2"}\NormalTok{,}
     \DecValTok{4}\NormalTok{, }\StringTok{"y3"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The coloured column represents the ``key'' variable: these are used to
match the rows between the tables. The grey column represents the
``value'' column that is carried along for the ride. In these examples
I'll show a single key variable, but the idea generalises in a
straightforward way to multiple keys and multiple values.

A join is a way of connecting each row in \texttt{x} to zero, one, or
more rows in \texttt{y}. The following diagram shows each potential
match as an intersection of a pair of lines.

\begin{center}\includegraphics{diagrams/join-setup2} \end{center}

(If you look closely, you might notice that we've switched the order of
the key and value columns in \texttt{x}. This is to emphasise that joins
match based on the key; the value is just carried along for the ride.)

In an actual join, matches will be indicated with dots. The number of
dots = the number of matches = the number of rows in the output.

\begin{center}\includegraphics{diagrams/join-inner} \end{center}

\subsection{Inner join}\label{inner-join}

The simplest type of join is the \textbf{inner join}. An inner join
matches pairs of observations whenever their keys are equal:

\begin{center}\includegraphics{diagrams/join-inner} \end{center}

(To be precise, this is an inner \textbf{equijoin} because the keys are
matched using the equality operator. Since most joins are equijoins we
usually drop that specification.)

The output of an inner join is a new data frame that contains the key,
the x values, and the y values. We use \texttt{by} to tell dplyr which
variable is the key:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(y, }\DataTypeTok{by =} \StringTok{"key"}\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>     key val_x val_y}
\CommentTok{#>   <dbl> <chr> <chr>}
\CommentTok{#> 1     1    x1    y1}
\CommentTok{#> 2     2    x2    y2}
\end{Highlighting}
\end{Shaded}

The most important property of an inner join is that unmatched rows are
not included in the result. This means that generally inner joins are
usually not appropriate for use in analysis because it's too easy to
lose observations.

\subsection{Outer joins}\label{outer-join}

An inner join keeps observations that appear in both tables. An
\textbf{outer join} keeps observations that appear in at least one of
the tables. There are three types of outer joins:

\begin{itemize}
\tightlist
\item
  A \textbf{left join} keeps all observations in \texttt{x}.
\item
  A \textbf{right join} keeps all observations in \texttt{y}.
\item
  A \textbf{full join} keeps all observations in \texttt{x} and
  \texttt{y}.
\end{itemize}

These joins work by adding an additional ``virtual'' observation to each
table. This observation has a key that always matches (if no other key
matches), and a value filled with \texttt{NA}.

Graphically, that looks like:

\begin{center}\includegraphics{diagrams/join-outer} \end{center}

The most commonly used join is the left join: you use this whenever you
look up additional data from another table, because it preserves the
original observations even when there isn't a match. The left join
should be your default join: use it unless you have a strong reason to
prefer one of the others.

Another way to depict the different types of joins is with a Venn
diagram:

\begin{center}\includegraphics{diagrams/join-venn} \end{center}

However, this is not a great representation. It might jog your memory
about which join preserves the observations in which table, but it
suffers from a major limitation: a Venn diagram can't show what happens
when keys don't uniquely identify an observation.

\subsection{Duplicate keys}\label{join-matches}

So far all the diagrams have assumed that the keys are unique. But
that's not always the case. This section explains what happens when the
keys are not unique. There are two possibilities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  One table has duplicate keys. This is useful when you want to add in
  additional information as there is typically a one-to-many
  relationship.

  \begin{center}\includegraphics{diagrams/join-one-to-many} \end{center}

  Note that I've put the key column in a slightly different position in
  the output. This reflects that the key is a primary key in \texttt{y}
  and a foreign key in \texttt{x}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~key, ~val_x,}
     \DecValTok{1}\NormalTok{, }\StringTok{"x1"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"x2"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"x3"}\NormalTok{,}
     \DecValTok{1}\NormalTok{, }\StringTok{"x4"}
\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~key, ~val_y,}
     \DecValTok{1}\NormalTok{, }\StringTok{"y1"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"y2"}
\NormalTok{)}
\KeywordTok{left_join}\NormalTok{(x, y, }\DataTypeTok{by =} \StringTok{"key"}\NormalTok{)}
\CommentTok{#> # A tibble: 4 × 3}
\CommentTok{#>     key val_x val_y}
\CommentTok{#>   <dbl> <chr> <chr>}
\CommentTok{#> 1     1    x1    y1}
\CommentTok{#> 2     2    x2    y2}
\CommentTok{#> 3     2    x3    y2}
\CommentTok{#> 4     1    x4    y1}
\end{Highlighting}
\end{Shaded}
\item
  Both tables have duplicate keys. This is usually an error because in
  neither table do the keys uniquely identify an observation. When you
  join duplicated keys, you get all possible combinations, the Cartesian
  product:

  \begin{center}\includegraphics{diagrams/join-many-to-many} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~key, ~val_x,}
     \DecValTok{1}\NormalTok{, }\StringTok{"x1"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"x2"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"x3"}\NormalTok{,}
     \DecValTok{3}\NormalTok{, }\StringTok{"x4"}
\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~key, ~val_y,}
     \DecValTok{1}\NormalTok{, }\StringTok{"y1"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"y2"}\NormalTok{,}
     \DecValTok{2}\NormalTok{, }\StringTok{"y3"}\NormalTok{,}
     \DecValTok{3}\NormalTok{, }\StringTok{"y4"}
\NormalTok{)}
\KeywordTok{left_join}\NormalTok{(x, y, }\DataTypeTok{by =} \StringTok{"key"}\NormalTok{)}
\CommentTok{#> # A tibble: 6 × 3}
\CommentTok{#>     key val_x val_y}
\CommentTok{#>   <dbl> <chr> <chr>}
\CommentTok{#> 1     1    x1    y1}
\CommentTok{#> 2     2    x2    y2}
\CommentTok{#> 3     2    x2    y3}
\CommentTok{#> 4     2    x3    y2}
\CommentTok{#> 5     2    x3    y3}
\CommentTok{#> 6     3    x4    y4}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\subsection{Defining the key columns}\label{join-by}

So far, the pairs of tables have always been joined by a single
variable, and that variable has the same name in both tables. That
constraint was encoded by \texttt{by\ =\ "key"}. You can use other
values for \texttt{by} to connect the tables in other ways:

\begin{itemize}
\item
  The default, \texttt{by\ =\ NULL}, uses all variables that appear in
  both tables, the so called \textbf{natural} join. For example, the
  flights and weather tables match on their common variables:
  \texttt{year}, \texttt{month}, \texttt{day}, \texttt{hour} and
  \texttt{origin}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(weather)}
\CommentTok{#> Joining, by = c("year", "month", "day", "hour", "origin")}
\CommentTok{#> # A tibble: 336,776 × 18}
\CommentTok{#>    year month   day  hour origin  dest tailnum carrier  temp  dewp humid}
\CommentTok{#>   <dbl> <dbl> <int> <dbl>  <chr> <chr>   <chr>   <chr> <dbl> <dbl> <dbl>}
\CommentTok{#> 1  2013     1     1     5    EWR   IAH  N14228      UA    NA    NA    NA}
\CommentTok{#> 2  2013     1     1     5    LGA   IAH  N24211      UA    NA    NA    NA}
\CommentTok{#> 3  2013     1     1     5    JFK   MIA  N619AA      AA    NA    NA    NA}
\CommentTok{#> 4  2013     1     1     5    JFK   BQN  N804JB      B6    NA    NA    NA}
\CommentTok{#> 5  2013     1     1     6    LGA   ATL  N668DN      DL  39.9  26.1  57.3}
\CommentTok{#> 6  2013     1     1     5    EWR   ORD  N39463      UA    NA    NA    NA}
\CommentTok{#> # ... with 3.368e+05 more rows, and 7 more variables: wind_dir <dbl>,}
\CommentTok{#> #   wind_speed <dbl>, wind_gust <dbl>, precip <dbl>, pressure <dbl>,}
\CommentTok{#> #   visib <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}
\item
  A character vector, \texttt{by\ =\ "x"}. This is like a natural join,
  but uses only some of the common variables. For example,
  \texttt{flights} and \texttt{planes} have \texttt{year} variables, but
  they mean different things so we only want to join by
  \texttt{tailnum}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(planes, }\DataTypeTok{by =} \StringTok{"tailnum"}\NormalTok{)}
\CommentTok{#> # A tibble: 336,776 × 16}
\CommentTok{#>   year.x month   day  hour origin  dest tailnum carrier year.y}
\CommentTok{#>    <int> <int> <int> <dbl>  <chr> <chr>   <chr>   <chr>  <int>}
\CommentTok{#> 1   2013     1     1     5    EWR   IAH  N14228      UA   1999}
\CommentTok{#> 2   2013     1     1     5    LGA   IAH  N24211      UA   1998}
\CommentTok{#> 3   2013     1     1     5    JFK   MIA  N619AA      AA   1990}
\CommentTok{#> 4   2013     1     1     5    JFK   BQN  N804JB      B6   2012}
\CommentTok{#> 5   2013     1     1     6    LGA   ATL  N668DN      DL   1991}
\CommentTok{#> 6   2013     1     1     5    EWR   ORD  N39463      UA   2012}
\CommentTok{#> # ... with 3.368e+05 more rows, and 7 more variables: type <chr>,}
\CommentTok{#> #   manufacturer <chr>, model <chr>, engines <int>, seats <int>,}
\CommentTok{#> #   speed <int>, engine <chr>}
\end{Highlighting}
\end{Shaded}

  Note that the \texttt{year} variables (which appear in both input data
  frames, but are not constrained to be equal) are disambiguated in the
  output with a suffix.
\item
  A named character vector: \texttt{by\ =\ c("a"\ =\ "b")}. This will
  match variable \texttt{a} in table \texttt{x} to variable \texttt{b}
  in table \texttt{y}. The variables from \texttt{x} will be used in the
  output.

  For example, if we want to draw a map we need to combine the flights
  data with the airports data which contains the location (\texttt{lat}
  and \texttt{long}) of each airport. Each flight has an origin and
  destination \texttt{airport}, so we need to specify which one we want
  to join to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(airports, }\KeywordTok{c}\NormalTok{(}\StringTok{"dest"} \NormalTok{=}\StringTok{ "faa"}\NormalTok{))}
\CommentTok{#> # A tibble: 336,776 × 14}
\CommentTok{#>    year month   day  hour origin  dest tailnum carrier}
\CommentTok{#>   <int> <int> <int> <dbl>  <chr> <chr>   <chr>   <chr>}
\CommentTok{#> 1  2013     1     1     5    EWR   IAH  N14228      UA}
\CommentTok{#> 2  2013     1     1     5    LGA   IAH  N24211      UA}
\CommentTok{#> 3  2013     1     1     5    JFK   MIA  N619AA      AA}
\CommentTok{#> 4  2013     1     1     5    JFK   BQN  N804JB      B6}
\CommentTok{#> 5  2013     1     1     6    LGA   ATL  N668DN      DL}
\CommentTok{#> 6  2013     1     1     5    EWR   ORD  N39463      UA}
\CommentTok{#> # ... with 3.368e+05 more rows, and 6 more variables: name <chr>,}
\CommentTok{#> #   lat <dbl>, lon <dbl>, alt <int>, tz <dbl>, dst <chr>}

\NormalTok{flights2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(airports, }\KeywordTok{c}\NormalTok{(}\StringTok{"origin"} \NormalTok{=}\StringTok{ "faa"}\NormalTok{))}
\CommentTok{#> # A tibble: 336,776 × 14}
\CommentTok{#>    year month   day  hour origin  dest tailnum carrier                name}
\CommentTok{#>   <int> <int> <int> <dbl>  <chr> <chr>   <chr>   <chr>               <chr>}
\CommentTok{#> 1  2013     1     1     5    EWR   IAH  N14228      UA Newark Liberty Intl}
\CommentTok{#> 2  2013     1     1     5    LGA   IAH  N24211      UA          La Guardia}
\CommentTok{#> 3  2013     1     1     5    JFK   MIA  N619AA      AA John F Kennedy Intl}
\CommentTok{#> 4  2013     1     1     5    JFK   BQN  N804JB      B6 John F Kennedy Intl}
\CommentTok{#> 5  2013     1     1     6    LGA   ATL  N668DN      DL          La Guardia}
\CommentTok{#> 6  2013     1     1     5    EWR   ORD  N39463      UA Newark Liberty Intl}
\CommentTok{#> # ... with 3.368e+05 more rows, and 5 more variables: lat <dbl>,}
\CommentTok{#> #   lon <dbl>, alt <int>, tz <dbl>, dst <chr>}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsection{Exercises}\label{exercises-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the average delay by destination, then join on the
  \texttt{airports} data frame so you can show the spatial distribution
  of delays. Here's an easy way to draw a map of the United States:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airports %>%}
\StringTok{  }\KeywordTok{semi_join}\NormalTok{(flights, }\KeywordTok{c}\NormalTok{(}\StringTok{"faa"} \NormalTok{=}\StringTok{ "dest"}\NormalTok{)) %>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(lon, lat)) +}
\StringTok{    }\KeywordTok{borders}\NormalTok{(}\StringTok{"state"}\NormalTok{) +}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{    }\KeywordTok{coord_quickmap}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

  (Don't worry if you don't understand what \texttt{semi\_join()} does
  --- you'll learn about it next.)

  You might want to use the \texttt{size} or \texttt{colour} of the
  points to display the average delay for each airport.
\item
  Add the location of the origin \emph{and} destination (i.e.~the
  \texttt{lat} and \texttt{lon}) to \texttt{flights}.
\item
  Is there a relationship between the age of a plane and its delays?
\item
  What weather conditions make it more likely to see a delay?
\item
  What happened on June 13 2013? Display the spatial pattern of delays,
  and then use Google to cross-reference with the weather.
\end{enumerate}

\subsection{Other implementations}\label{other-implementations}

\texttt{base::merge()} can perform all four types of mutating join:

\begin{longtable}[]{@{}ll@{}}
\toprule
dplyr & merge\tabularnewline
\midrule
\endhead
\texttt{inner\_join(x,\ y)} & \texttt{merge(x,\ y)}\tabularnewline
\texttt{left\_join(x,\ y)} &
\texttt{merge(x,\ y,\ all.x\ =\ TRUE)}\tabularnewline
\texttt{right\_join(x,\ y)} &
\texttt{merge(x,\ y,\ all.y\ =\ TRUE)},\tabularnewline
\texttt{full\_join(x,\ y)} &
\texttt{merge(x,\ y,\ all.x\ =\ TRUE,\ all.y\ =\ TRUE)}\tabularnewline
\bottomrule
\end{longtable}

The advantages of the specific dplyr verbs is that they more clearly
convey the intent of your code: the difference between the joins is
really important but concealed in the arguments of \texttt{merge()}.
dplyr's joins are considerably faster and don't mess with the order of
the rows.

SQL is the inspiration for dplyr's conventions, so the translation is
straightforward:

\begin{longtable}[]{@{}ll@{}}
\toprule
dplyr & SQL\tabularnewline
\midrule
\endhead
\texttt{inner\_join(x,\ y,\ by\ =\ "z")} &
\texttt{SELECT\ *\ FROM\ x\ INNER\ JOIN\ y\ USING\ (z)}\tabularnewline
\texttt{left\_join(x,\ y,\ by\ =\ "z")} &
\texttt{SELECT\ *\ FROM\ x\ LEFT\ OUTER\ JOIN\ y\ USING\ (z)}\tabularnewline
\texttt{right\_join(x,\ y,\ by\ =\ "z")} &
\texttt{SELECT\ *\ FROM\ x\ RIGHT\ OUTER\ JOIN\ y\ USING\ (z)}\tabularnewline
\texttt{full\_join(x,\ y,\ by\ =\ "z")} &
\texttt{SELECT\ *\ FROM\ x\ FULL\ OUTER\ JOIN\ y\ USING\ (z)}\tabularnewline
\bottomrule
\end{longtable}

Note that ``INNER'' and ``OUTER'' are optional, and often omitted.

Joining different variables between the tables, e.g.
\texttt{inner\_join(x,\ y,\ by\ =\ c("a"\ =\ "b"))} uses a slightly
different syntax in SQL:
\texttt{SELECT\ *\ FROM\ x\ INNER\ JOIN\ y\ ON\ x.a\ =\ y.b}. As this
syntax suggests, SQL supports a wider range of join types than dplyr
because you can connect the tables using constraints other than equality
(sometimes called non-equijoins).

\section{Filtering joins}\label{filtering-joins}

Filtering joins match observations in the same way as mutating joins,
but affect the observations, not the variables. There are two types:

\begin{itemize}
\tightlist
\item
  \texttt{semi\_join(x,\ y)} \textbf{keeps} all observations in
  \texttt{x} that have a match in \texttt{y}.
\item
  \texttt{anti\_join(x,\ y)} \textbf{drops} all observations in
  \texttt{x} that have a match in \texttt{y}.
\end{itemize}

Semi-joins are useful for matching filtered summary tables back to the
original rows. For example, imagine you've found the top ten most
popular destinations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top_dest <-}\StringTok{ }\NormalTok{flights %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(dest, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{top_dest}
\CommentTok{#> # A tibble: 10 × 2}
\CommentTok{#>    dest     n}
\CommentTok{#>   <chr> <int>}
\CommentTok{#> 1   ORD 17283}
\CommentTok{#> 2   ATL 17215}
\CommentTok{#> 3   LAX 16174}
\CommentTok{#> 4   BOS 15508}
\CommentTok{#> 5   MCO 14082}
\CommentTok{#> 6   CLT 14064}
\CommentTok{#> # ... with 4 more rows}
\end{Highlighting}
\end{Shaded}

Now you want to find each flight that went to one of those destinations.
You could construct a filter yourself:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(dest %in%}\StringTok{ }\NormalTok{top_dest$dest)}
\CommentTok{#> # A tibble: 141,145 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      542            540         2      923}
\CommentTok{#> 2  2013     1     1      554            600        -6      812}
\CommentTok{#> 3  2013     1     1      554            558        -4      740}
\CommentTok{#> 4  2013     1     1      555            600        -5      913}
\CommentTok{#> 5  2013     1     1      557            600        -3      838}
\CommentTok{#> 6  2013     1     1      558            600        -2      753}
\CommentTok{#> # ... with 1.411e+05 more rows, and 12 more variables:}
\CommentTok{#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,}
\CommentTok{#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

But it's difficult to extend that approach to multiple variables. For
example, imagine that you'd found the 10 days with highest average
delays. How would you construct the filter statement that used
\texttt{year}, \texttt{month}, and \texttt{day} to match it back to
\texttt{flights}?

Instead you can use a semi-join, which connects the two tables like a
mutating join, but instead of adding new columns, only keeps the rows in
\texttt{x} that have a match in \texttt{y}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{semi_join}\NormalTok{(top_dest)}
\CommentTok{#> Joining, by = "dest"}
\CommentTok{#> # A tibble: 141,145 × 19}
\CommentTok{#>    year month   day dep_time sched_dep_time dep_delay arr_time}
\CommentTok{#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>}
\CommentTok{#> 1  2013     1     1      554            558        -4      740}
\CommentTok{#> 2  2013     1     1      558            600        -2      753}
\CommentTok{#> 3  2013     1     1      608            600         8      807}
\CommentTok{#> 4  2013     1     1      629            630        -1      824}
\CommentTok{#> 5  2013     1     1      656            700        -4      854}
\CommentTok{#> 6  2013     1     1      709            700         9      852}
\CommentTok{#> # ... with 1.411e+05 more rows, and 12 more variables:}
\CommentTok{#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,}
\CommentTok{#> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,}
\CommentTok{#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>}
\end{Highlighting}
\end{Shaded}

Graphically, a semi-join looks like this:

\begin{center}\includegraphics{diagrams/join-semi} \end{center}

Only the existence of a match is important; it doesn't matter which
observation is matched. This means that filtering joins never duplicate
rows like mutating joins do:

\begin{center}\includegraphics{diagrams/join-semi-many} \end{center}

The inverse of a semi-join is an anti-join. An anti-join keeps the rows
that \emph{don't} have a match:

\begin{center}\includegraphics{diagrams/join-anti} \end{center}

Anti-joins are useful for diagnosing join mismatches. For example, when
connecting \texttt{flights} and \texttt{planes}, you might be interested
to know that there are many \texttt{flights} that don't have a match in
\texttt{planes}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}
\StringTok{  }\KeywordTok{anti_join}\NormalTok{(planes, }\DataTypeTok{by =} \StringTok{"tailnum"}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(tailnum, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> # A tibble: 722 × 2}
\CommentTok{#>   tailnum     n}
\CommentTok{#>     <chr> <int>}
\CommentTok{#> 1    <NA>  2512}
\CommentTok{#> 2  N725MQ   575}
\CommentTok{#> 3  N722MQ   513}
\CommentTok{#> 4  N723MQ   507}
\CommentTok{#> 5  N713MQ   483}
\CommentTok{#> 6  N735MQ   396}
\CommentTok{#> # ... with 716 more rows}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What does it mean for a flight to have a missing \texttt{tailnum}?
  What do the tail numbers that don't have a matching record in
  \texttt{planes} have in common? (Hint: one variable explains
  \textasciitilde{}90\% of the problems.)
\item
  Filter flights to only show flights with planes that have flown at
  least 100 flights.
\item
  Combine \texttt{fueleconomy::vehicles} and
  \texttt{fueleconomy::common} to find only the records for the most
  common models.
\item
  Find the 48 hours (over the course of the whole year) that have the
  worst delays. Cross-reference it with the \texttt{weather} data. Can
  you see any patterns?
\item
  What does
  \texttt{anti\_join(flights,\ airports,\ by\ =\ c("dest"\ =\ "faa"))}
  tell you? What does
  \texttt{anti\_join(airports,\ flights,\ by\ =\ c("faa"\ =\ "dest"))}
  tell you?
\item
  You might expect that there's an implicit relationship between plane
  and airline, because each plane is flown by a single airline. Confirm
  or reject this hypothesis using the tools you've learned above.
\end{enumerate}

\section{Join problems}\label{join-problems}

The data you've been working with in this chapter has been cleaned up so
that you'll have as few problems as possible. Your own data is unlikely
to be so nice, so there are a few things that you should do with your
own data to make your joins go smoothly.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start by identifying the variables that form the primary key in each
  table. You should usually do this based on your understanding of the
  data, not empirically by looking for a combination of variables that
  give a unique identifier. If you just look for variables without
  thinking about what they mean, you might get (un)lucky and find a
  combination that's unique in your current data but the relationship
  might not be true in general.

  For example, the altitude and longitude uniquely identify each
  airport, but they are not good identifiers!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airports %>%}\StringTok{ }\KeywordTok{count}\NormalTok{(alt, lon) %>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(n >}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> Source: local data frame [0 x 3]}
\CommentTok{#> Groups: alt [0]}
\CommentTok{#> }
\CommentTok{#> # ... with 3 variables: alt <int>, lon <dbl>, n <int>}
\end{Highlighting}
\end{Shaded}
\item
  Check that none of the variables in the primary key are missing. If a
  value is missing then it can't identify an observation!
\item
  Check that your foreign keys match primary keys in another table. The
  best way to do this is with an \texttt{anti\_join()}. It's common for
  keys not to match because of data entry errors. Fixing these is often
  a lot of work.

  If you do have missing keys, you'll need to be thoughtful about your
  use of inner vs.~outer joins, carefully considering whether or not you
  want to drop rows that don't have a match.
\end{enumerate}

Be aware that simply checking the number of rows before and after the
join is not sufficient to ensure that your join has gone smoothly. If
you have an inner join with duplicate keys in both tables, you might get
unlucky as the number of dropped rows might exactly equal the number of
duplicated rows!

\section{Set operations}\label{set-operations}

The final type of two-table verb are the set operations. Generally, I
use these the least frequently, but they are occasionally useful when
you want to break a single complex filter into simpler pieces. All these
operations work with a complete row, comparing the values of every
variable. These expect the \texttt{x} and \texttt{y} inputs to have the
same variables, and treat the observations like sets:

\begin{itemize}
\tightlist
\item
  \texttt{intersect(x,\ y)}: return only observations in both \texttt{x}
  and \texttt{y}.
\item
  \texttt{union(x,\ y)}: return unique observations in \texttt{x} and
  \texttt{y}.
\item
  \texttt{setdiff(x,\ y)}: return observations in \texttt{x}, but not in
  \texttt{y}.
\end{itemize}

Given this simple data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x, ~y,}
   \DecValTok{1}\NormalTok{,  }\DecValTok{1}\NormalTok{,}
   \DecValTok{2}\NormalTok{,  }\DecValTok{1}
\NormalTok{)}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x, ~y,}
   \DecValTok{1}\NormalTok{,  }\DecValTok{1}\NormalTok{,}
   \DecValTok{1}\NormalTok{,  }\DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The four possibilities are:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{intersect}\NormalTok{(df1, df2)}
\CommentTok{#> # A tibble: 1 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <dbl> <dbl>}
\CommentTok{#> 1     1     1}

\CommentTok{# Note that we get 3 rows, not 4}
\KeywordTok{union}\NormalTok{(df1, df2)}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <dbl> <dbl>}
\CommentTok{#> 1     1     2}
\CommentTok{#> 2     2     1}
\CommentTok{#> 3     1     1}

\KeywordTok{setdiff}\NormalTok{(df1, df2)}
\CommentTok{#> # A tibble: 1 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <dbl> <dbl>}
\CommentTok{#> 1     2     1}

\KeywordTok{setdiff}\NormalTok{(df2, df1)}
\CommentTok{#> # A tibble: 1 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <dbl> <dbl>}
\CommentTok{#> 1     1     2}
\end{Highlighting}
\end{Shaded}

\hypertarget{strings}{\chapter{Strings}\label{strings}}

\section{Introduction}\label{introduction-8}

This chapter introduces you to string manipulation in R. You'll learn
the basics of how strings work and how to create them by hand, but the
focus of this chapter will be on regular expressions, or regexps for
short. Regular expressions are useful because strings usually contain
unstructured or semi-structured data, and regexps are a concise language
for describing patterns in strings. When you first look at a regexp,
you'll think a cat walked across your keyboard, but as your
understanding improves they will soon start to make sense.

\subsection{Prerequisites}\label{prerequisites-8}

This chapter will focus on the \textbf{stringr} package for string
manipulation. stringr is not part of the core tidyverse because you
don't always have textual data, so we need to load it explicitly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(stringr)}
\end{Highlighting}
\end{Shaded}

\hypertarget{string-basics}{\section{String
basics}\label{string-basics}}

You can create strings with either single quotes or double quotes.
Unlike other languages, there is no difference in behaviour. I recommend
always using \texttt{"}, unless you want to create a string that
contains multiple \texttt{"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string1 <-}\StringTok{ "This is a string"}
\NormalTok{string2 <-}\StringTok{ 'If I want to include a "quote" inside a string, I use single quotes'}
\end{Highlighting}
\end{Shaded}

If you forget to close a quote, you'll see \texttt{+}, the continuation
character:

\begin{verbatim}
> "This is a string without a closing quote
+ 
+ 
+ HELP I'M STUCK
\end{verbatim}

If this happen to you, press Escape and try again!

To include a literal single or double quote in a string you can use
\texttt{\textbackslash{}} to ``escape'' it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{double_quote <-}\StringTok{ "}\CharTok{\textbackslash{}"}\StringTok{"} \CommentTok{# or '"'}
\NormalTok{single_quote <-}\StringTok{ '}\CharTok{\textbackslash{}'}\StringTok{'} \CommentTok{# or "'"}
\end{Highlighting}
\end{Shaded}

That means if you want to include a literal backslash, you'll need to
double it up: \texttt{"\textbackslash{}\textbackslash{}"}.

Beware that the printed representation of a string is not the same as
string itself, because the printed representation shows the escapes. To
see the raw contents of the string, use \texttt{writeLines()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}"}\StringTok{"}\NormalTok{, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{"}\NormalTok{)}
\NormalTok{x}
\CommentTok{#> [1] "\textbackslash{}"" "\textbackslash{}\textbackslash{}"}
\KeywordTok{writeLines}\NormalTok{(x)}
\CommentTok{#> "}
\CommentTok{#> \textbackslash{}}
\end{Highlighting}
\end{Shaded}

There are a handful of other special characters. The most common are
\texttt{"\textbackslash{}n"}, newline, and \texttt{"\textbackslash{}t"},
tab, but you can see the complete list by requesting help on \texttt{"}:
\texttt{?\textquotesingle{}"\textquotesingle{}}, or
\texttt{?"\textquotesingle{}"}. You'll also sometimes see strings like
\texttt{"\textbackslash{}u00b5"}, this is a way of writing non-English
characters that works on all platforms:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "\textbackslash{}u00b5"}
\NormalTok{x}
\CommentTok{#> [1] "µ"}
\end{Highlighting}
\end{Shaded}

Multiple strings are often stored in a character vector, which you can
create with \texttt{c()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\StringTok{"one"}\NormalTok{, }\StringTok{"two"}\NormalTok{, }\StringTok{"three"}\NormalTok{)}
\CommentTok{#> [1] "one"   "two"   "three"}
\end{Highlighting}
\end{Shaded}

\subsection{String length}\label{string-length}

Base R contains many functions to work with strings but we'll avoid them
because they can be inconsistent, which makes them hard to remember.
Instead we'll use functions from stringr. These have more intuitive
names, and all start with \texttt{str\_}. For example,
\texttt{str\_length()} tells you the number of characters in a string:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_length}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"R for data science"}\NormalTok{, }\OtherTok{NA}\NormalTok{))}
\CommentTok{#> [1]  1 18 NA}
\end{Highlighting}
\end{Shaded}

The common \texttt{str\_} prefix is particularly useful if you use
RStudio, because typing \texttt{str\_} will trigger autocomplete,
allowing you to see all stringr functions:

\begin{center}\includegraphics[width=0.7\linewidth]{screenshots/stringr-autocomplete} \end{center}

\subsection{Combining strings}\label{combining-strings}

To combine two or more strings, use \texttt{str\_c()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{)}
\CommentTok{#> [1] "xy"}
\KeywordTok{str_c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, }\StringTok{"z"}\NormalTok{)}
\CommentTok{#> [1] "xyz"}
\end{Highlighting}
\end{Shaded}

Use the \texttt{sep} argument to control how they're separated:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{", "}\NormalTok{)}
\CommentTok{#> [1] "x, y"}
\end{Highlighting}
\end{Shaded}

Like most other functions in R, missing values are contagious. If you
want them to print as \texttt{"NA"}, use \texttt{str\_replace\_na()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"abc"}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\KeywordTok{str_c}\NormalTok{(}\StringTok{"|-"}\NormalTok{, x, }\StringTok{"-|"}\NormalTok{)}
\CommentTok{#> [1] "|-abc-|" NA}
\KeywordTok{str_c}\NormalTok{(}\StringTok{"|-"}\NormalTok{, }\KeywordTok{str_replace_na}\NormalTok{(x), }\StringTok{"-|"}\NormalTok{)}
\CommentTok{#> [1] "|-abc-|" "|-NA-|"}
\end{Highlighting}
\end{Shaded}

As shown above, \texttt{str\_c()} is vectorised, and it automatically
recycles shorter vectors to the same length as the longest:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_c}\NormalTok{(}\StringTok{"prefix-"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{), }\StringTok{"-suffix"}\NormalTok{)}
\CommentTok{#> [1] "prefix-a-suffix" "prefix-b-suffix" "prefix-c-suffix"}
\end{Highlighting}
\end{Shaded}

Objects of length 0 are silently dropped. This is particularly useful in
conjunction with \texttt{if}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{name <-}\StringTok{ "Hadley"}
\NormalTok{time_of_day <-}\StringTok{ "morning"}
\NormalTok{birthday <-}\StringTok{ }\OtherTok{FALSE}

\KeywordTok{str_c}\NormalTok{(}
  \StringTok{"Good "}\NormalTok{, time_of_day, }\StringTok{" "}\NormalTok{, name,}
  \NormalTok{if (birthday) }\StringTok{" and HAPPY BIRTHDAY"}\NormalTok{,}
  \StringTok{"."}
\NormalTok{)}
\CommentTok{#> [1] "Good morning Hadley."}
\end{Highlighting}
\end{Shaded}

To collapse a vector of strings into a single string, use
\texttt{collapse}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_c}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, }\StringTok{"z"}\NormalTok{), }\DataTypeTok{collapse =} \StringTok{", "}\NormalTok{)}
\CommentTok{#> [1] "x, y, z"}
\end{Highlighting}
\end{Shaded}

\subsection{Subsetting strings}\label{subsetting-strings}

You can extract parts of a string using \texttt{str\_sub()}. As well as
the string, \texttt{str\_sub()} takes \texttt{start} and \texttt{end}
arguments which give the (inclusive) position of the substring:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Apple"}\NormalTok{, }\StringTok{"Banana"}\NormalTok{, }\StringTok{"Pear"}\NormalTok{)}
\KeywordTok{str_sub}\NormalTok{(x, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\CommentTok{#> [1] "App" "Ban" "Pea"}
\CommentTok{# negative numbers count backwards from end}
\KeywordTok{str_sub}\NormalTok{(x, -}\DecValTok{3}\NormalTok{, -}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "ple" "ana" "ear"}
\end{Highlighting}
\end{Shaded}

Note that \texttt{str\_sub()} won't fail if the string is too short: it
will just return as much as possible:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_sub}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] "a"}
\end{Highlighting}
\end{Shaded}

You can also use the assignment form of \texttt{str\_sub()} to modify
strings:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_sub}\NormalTok{(x, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) <-}\StringTok{ }\KeywordTok{str_to_lower}\NormalTok{(}\KeywordTok{str_sub}\NormalTok{(x, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{x}
\CommentTok{#> [1] "apple"  "banana" "pear"}
\end{Highlighting}
\end{Shaded}

\subsection{Locales}\label{locales}

Above I used \texttt{str\_to\_lower()} to change the text to lower case.
You can also use \texttt{str\_to\_upper()} or \texttt{str\_to\_title()}.
However, changing case is more complicated than it might at first appear
because different languages have different rules for changing case. You
can pick which set of rules to use by specifying a locale:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Turkish has two i's: with and without a dot, and it}
\CommentTok{# has a different rule for capitalising them:}
\KeywordTok{str_to_upper}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"i"}\NormalTok{, }\StringTok{"ı"}\NormalTok{))}
\CommentTok{#> [1] "I" "I"}
\KeywordTok{str_to_upper}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"i"}\NormalTok{, }\StringTok{"ı"}\NormalTok{), }\DataTypeTok{locale =} \StringTok{"tr"}\NormalTok{)}
\CommentTok{#> [1] "İ" "I"}
\end{Highlighting}
\end{Shaded}

The locale is specified as a ISO 639 language code, which is a two or
three letter abbreviation. If you don't already know the code for your
language,
\href{https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes}{Wikipedia}
has a good list. If you leave the locale blank, it will use the current
locale, as provided by your operating system.

Another important operation that's affected by the locale is sorting.
The base R \texttt{order()} and \texttt{sort()} functions sort strings
using the current locale. If you want robust behaviour across different
computers, you may want to use \texttt{str\_sort()} and
\texttt{str\_order()} which take an additional \texttt{locale} argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"eggplant"}\NormalTok{, }\StringTok{"banana"}\NormalTok{)}

\KeywordTok{str_sort}\NormalTok{(x, }\DataTypeTok{locale =} \StringTok{"en"}\NormalTok{)  }\CommentTok{# English}
\CommentTok{#> [1] "apple"    "banana"   "eggplant"}

\KeywordTok{str_sort}\NormalTok{(x, }\DataTypeTok{locale =} \StringTok{"haw"}\NormalTok{) }\CommentTok{# Hawaiian}
\CommentTok{#> [1] "apple"    "eggplant" "banana"}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In code that doesn't use stringr, you'll often see \texttt{paste()}
  and \texttt{paste0()}. What's the difference between the two
  functions? What stringr function are they equivalent to? How do the
  functions differ in their handling of \texttt{NA}?
\item
  In your own words, describe the difference between the \texttt{sep}
  and \texttt{collapse} arguments to \texttt{str\_c()}.
\item
  Use \texttt{str\_length()} and \texttt{str\_sub()} to extract the
  middle character from a string. What will you do if the string has an
  even number of characters?
\item
  What does \texttt{str\_wrap()} do? When might you want to use it?
\item
  What does \texttt{str\_trim()} do? What's the opposite of
  \texttt{str\_trim()}?
\item
  Write a function that turns (e.g.) a vector
  \texttt{c("a",\ "b",\ "c")} into the string \texttt{a,\ b,\ and\ c}.
  Think carefully about what it should do if given a vector of length 0,
  1, or 2.
\end{enumerate}

\section{Matching patterns with regular
expressions}\label{matching-patterns-with-regular-expressions}

Regexps are a very terse language that allow you to describe patterns in
strings. They take a little while to get your head around, but once you
understand them, you'll find them extremely useful.

To learn regular expressions, we'll use \texttt{str\_view()} and
\texttt{str\_view\_all()}. These functions take a character vector and a
regular expression, and show you how they match. We'll start with very
simple regular expressions and then gradually get more and more
complicated. Once you've mastered pattern matching, you'll learn how to
apply those ideas with various stringr functions.

\subsection{Basic matches}\label{basic-matches}

The simplest patterns match exact strings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{, }\StringTok{"pear"}\NormalTok{)}
\CommentTok{# str_view(x, "an")}
\end{Highlighting}
\end{Shaded}

The next step up in complexity is \texttt{.}, which matches any
character (except a newline):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# str_view(x, ".a.")}
\end{Highlighting}
\end{Shaded}

But if ``\texttt{.}'' matches any character, how do you match the
character ``\texttt{.}''? You need to use an ``escape'' to tell the
regular expression you want to match it exactly, not use its special
behaviour. Like strings, regexps use the backslash,
\texttt{\textbackslash{}}, to escape special behaviour. So to match an
\texttt{.}, you need the regexp \texttt{\textbackslash{}.}.
Unfortunately this creates a problem. We use strings to represent
regular expressions, and \texttt{\textbackslash{}} is also used as an
escape symbol in strings. So to create the regular expression
\texttt{\textbackslash{}.} we need the string
\texttt{"\textbackslash{}\textbackslash{}."}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# To create the regular expression, we need \textbackslash{}\textbackslash{}}
\NormalTok{dot <-}\StringTok{ "}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{."}

\CommentTok{# But the expression itself only contains one:}
\KeywordTok{writeLines}\NormalTok{(dot)}
\CommentTok{#> \textbackslash{}.}

\CommentTok{# And this tells R to look for an explicit .}
\CommentTok{# str_view(c("abc", "a.c", "bef"), "a\textbackslash{}\textbackslash{}.c")}
\end{Highlighting}
\end{Shaded}

If \texttt{\textbackslash{}} is used as an escape character in regular
expressions, how do you match a literal \texttt{\textbackslash{}}? Well
you need to escape it, creating the regular expression
\texttt{\textbackslash{}\textbackslash{}}. To create that regular
expression, you need to use a string, which also needs to escape
\texttt{\textbackslash{}}. That means to match a literal
\texttt{\textbackslash{}} you need to write
\texttt{"\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}"}
--- you need four backslashes to match one!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "a}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}
\KeywordTok{writeLines}\NormalTok{(x)}
\CommentTok{#> a\textbackslash{}b}

\CommentTok{# str_view(x, "\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}")}
\end{Highlighting}
\end{Shaded}

In this book, I'll write regular expression as
\texttt{\textbackslash{}.} and strings that represent the regular
expression as \texttt{"\textbackslash{}\textbackslash{}."}.

\subsubsection{Exercises}\label{exercises-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explain why each of these strings don't match a
  \texttt{\textbackslash{}}: \texttt{"\textbackslash{}"},
  \texttt{"\textbackslash{}\textbackslash{}"},
  \texttt{"\textbackslash{}\textbackslash{}\textbackslash{}"}.
\item
  How would you match the sequence
  \texttt{"\textquotesingle{}\textbackslash{}}?
\item
  What patterns will the regular expression
  \texttt{\textbackslash{}..\textbackslash{}..\textbackslash{}..} match?
  How would you represent it as a string?
\end{enumerate}

\subsection{Anchors}\label{anchors}

By default, regular expressions will match any part of a string. It's
often useful to \emph{anchor} the regular expression so that it matches
from the start or end of the string. You can use:

\begin{itemize}
\tightlist
\item
  \texttt{\^{}} to match the start of the string.
\item
  \texttt{\$} to match the end of the string.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{, }\StringTok{"pear"}\NormalTok{)}
\CommentTok{# str_view(x, "^a")}
\CommentTok{# str_view(x, "a$")}
\end{Highlighting}
\end{Shaded}

To remember which is which, try this mnemonic which I learned from
\href{https://twitter.com/emisshula/status/323863393167613953}{Evan
Misshula}: if you begin with power (\texttt{\^{}}), you end up with
money (\texttt{\$}).

To force a regular expression to only match a complete string, anchor it
with both \texttt{\^{}} and \texttt{\$}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple pie"}\NormalTok{, }\StringTok{"apple"}\NormalTok{, }\StringTok{"apple cake"}\NormalTok{)}
\CommentTok{# str_view(x, "apple")}
\CommentTok{# str_view(x, "^apple$")}
\end{Highlighting}
\end{Shaded}

You can also match the boundary between words with
\texttt{\textbackslash{}b}. I don't often use this in R, but I will
sometimes use it when I'm doing a search in RStudio when I want to find
the name of a function that's a component of other functions. For
example, I'll search for \texttt{\textbackslash{}bsum\textbackslash{}b}
to avoid matching \texttt{summarise}, \texttt{summary}, \texttt{rowsum}
and so on.

\subsubsection{Exercises}\label{exercises-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How would you match the literal string \texttt{"\$\^{}\$"}?
\item
  Given the corpus of common words in \texttt{stringr::words}, create
  regular expressions that find all words that:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Start with ``y''.
  \item
    End with ``x''
  \item
    Are exactly three letters long. (Don't cheat by using
    \texttt{str\_length()}!)
  \item
    Have seven letters or more.
  \end{enumerate}

  Since this list is long, you might want to use the \texttt{match}
  argument to \texttt{\#\ str\_view()} to show only the matching or
  non-matching words.
\end{enumerate}

\subsection{Character classes and
alternatives}\label{character-classes-and-alternatives}

There are a number of special patterns that match more than one
character. You've already seen \texttt{.}, which matches any character
apart from a newline. There are four other useful tools:

\begin{itemize}
\tightlist
\item
  \texttt{\textbackslash{}d}: matches any digit.
\item
  \texttt{\textbackslash{}s}: matches any whitespace (e.g.~space, tab,
  newline).
\item
  \texttt{{[}abc{]}}: matches a, b, or c.
\item
  \texttt{{[}\^{}abc{]}}: matches anything except a, b, or c.
\end{itemize}

Remember, to create a regular expression containing
\texttt{\textbackslash{}d} or \texttt{\textbackslash{}s}, you'll need to
escape the \texttt{\textbackslash{}} for the string, so you'll type
\texttt{"\textbackslash{}\textbackslash{}d"} or
\texttt{"\textbackslash{}\textbackslash{}s"}.

You can use \emph{alternation} to pick between one or more alternative
patterns. For example, \texttt{abc\textbar{}d..f} will match either
`\,``abc''\,', or \texttt{"deaf"}. Note that the precedence for
\texttt{\textbar{}} is low, so that \texttt{abc\textbar{}xyz} matches
\texttt{abc} or \texttt{xyz} not \texttt{abcyz} or \texttt{abxyz}. Like
with mathematical expressions, if precedence ever gets confusing, use
parentheses to make it clear what you want:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# str_view(c("grey", "gray"), "gr(e|a)y")}
\end{Highlighting}
\end{Shaded}

\subsubsection{Exercises}\label{exercises-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create regular expressions to find all words that:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Start with a vowel.
  \item
    That only contain consonants. (Hint: thinking about matching
    ``not''-vowels.)
  \item
    End with \texttt{ed}, but not with \texttt{eed}.
  \item
    End with \texttt{ing} or \texttt{ise}.
  \end{enumerate}
\item
  Empirically verify the rule ``i before e except after c''.
\item
  Is ``q'' always followed by a ``u''?
\item
  Write a regular expression that matches a word if it's probably
  written in British English, not American English.
\item
  Create a regular expression that will match telephone numbers as
  commonly written in your country.
\end{enumerate}

\subsection{Repetition}\label{repetition}

The next step up in power involves controlling how many times a pattern
matches:

\begin{itemize}
\tightlist
\item
  \texttt{?}: 0 or 1
\item
  \texttt{+}: 1 or more
\item
  \texttt{*}: 0 or more
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "1888 is the longest year in Roman numerals: MDCCCLXXXVIII"}
\CommentTok{# str_view(x, "CC?")}
\CommentTok{# str_view(x, "CC+")}
\CommentTok{# str_view(x, 'C[LX]+')}
\end{Highlighting}
\end{Shaded}

Note that the precedence of these operators is high, so you can write:
\texttt{colou?r} to match either American or British spellings. That
means most uses will need parentheses, like \texttt{bana(na)+}.

You can also specify the number of matches precisely:

\begin{itemize}
\tightlist
\item
  \texttt{\{n\}}: exactly n
\item
  \texttt{\{n,\}}: n or more
\item
  \texttt{\{,m\}}: at most m
\item
  \texttt{\{n,m\}}: between n and m
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# str_view(x, "C\{2\}")}
\CommentTok{# str_view(x, "C\{2,\}")}
\CommentTok{# str_view(x, "C\{2,3\}")}
\end{Highlighting}
\end{Shaded}

By default these matches are ``greedy'': they will match the longest
string possible. You can make them ``lazy'', matching the shortest
string possible by putting a \texttt{?} after them. This is an advanced
feature of regular expressions, but it's useful to know that it exists:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# str_view(x, 'C\{2,3\}?')}
\CommentTok{# str_view(x, 'C[LX]+?')}
\end{Highlighting}
\end{Shaded}

\subsubsection{Exercises}\label{exercises-34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Describe the equivalents of \texttt{?}, \texttt{+}, \texttt{*} in
  \texttt{\{m,n\}} form.
\item
  Describe in words what these regular expressions match: (read
  carefully to see if I'm using a regular expression or a string that
  defines a regular expression.)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \texttt{\^{}.*\$}
  \item
    \texttt{"\textbackslash{}\textbackslash{}\{.+\textbackslash{}\textbackslash{}\}"}
  \item
    \texttt{\textbackslash{}d\{4\}-\textbackslash{}d\{2\}-\textbackslash{}d\{2\}}
  \item
    \texttt{"\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}\{4\}"}
  \end{enumerate}
\item
  Create regular expressions to find all words that:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Start with three consonants.
  \item
    Have three or more vowels in a row.
  \item
    Have two or more vowel-consonant pairs in a row.
  \end{enumerate}
\item
  Solve the beginner regexp crosswords at
  \url{https://regexcrossword.com/challenges/beginner}.
\end{enumerate}

\subsection{Grouping and
backreferences}\label{grouping-and-backreferences}

Earlier, you learned about parentheses as a way to disambiguate complex
expressions. They also define ``groups'' that you can refer to with
\emph{backreferences}, like \texttt{\textbackslash{}1},
\texttt{\textbackslash{}2} etc. For example, the following regular
expression finds all fruits that have a repeated pair of letters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# str_view(fruit, "(..)\textbackslash{}\textbackslash{}1", match = TRUE)}
\end{Highlighting}
\end{Shaded}

(Shortly, you'll also see how they're useful in conjunction with
\texttt{str\_match()}.)

\subsubsection{Exercises}\label{exercises-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Describe, in words, what these expressions will match:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \texttt{(.)\textbackslash{}1\textbackslash{}1}
  \item
    \texttt{"(.)(.)\textbackslash{}\textbackslash{}2\textbackslash{}\textbackslash{}1"}
  \item
    \texttt{(..)\textbackslash{}1}
  \item
    \texttt{"(.).\textbackslash{}\textbackslash{}1.\textbackslash{}\textbackslash{}1"}
  \item
    \texttt{"(.)(.)(.).*\textbackslash{}\textbackslash{}3\textbackslash{}\textbackslash{}2\textbackslash{}\textbackslash{}1"}
  \end{enumerate}
\item
  Construct regular expressions to match words that:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Start and end with the same character.
  \item
    Contain a repeated pair of letters (e.g. ``church'' contains ``ch''
    repeated twice.)
  \item
    Contain one letter repeated in at least three places (e.g.
    ``eleven'' contains three ``e''s.)
  \end{enumerate}
\end{enumerate}

\section{Tools}\label{tools}

Now that you've learned the basics of regular expressions, it's time to
learn how to apply them to real problems. In this section you'll learn a
wide array of stringr functions that let you:

\begin{itemize}
\tightlist
\item
  Determine which strings match a pattern.
\item
  Find the positions of matches.
\item
  Extract the content of matches.
\item
  Replace matches with new values.
\item
  Split a string based on a match.
\end{itemize}

A word of caution before we continue: because regular expressions are so
powerful, it's easy to try and solve every problem with a single regular
expression. In the words of Jamie Zawinski:

\begin{quote}
Some people, when confronted with a problem, think ``I know, I'll use
regular expressions.'' Now they have two problems.
\end{quote}

As a cautionary tale, check out this regular expression that checks if a
email address is valid:

\begin{verbatim}
(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:
\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(
?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ 
\t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\0
31]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\
](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+
(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:
(?:\r\n)?[ \t])*))*|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)
?[ \t])*)*\<(?:(?:\r\n)?[ \t])*(?:@(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\
r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[
 \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)
?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t]
)*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[
 \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*
)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)
*:(?:(?:\r\n)?[ \t])*)?(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+
|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r
\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:
\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t
]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031
]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](
?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?
:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?
:\r\n)?[ \t])*))*\>(?:(?:\r\n)?[ \t])*)|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?
:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?
[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()<>@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|
\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>
@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"
(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t]
)*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?
:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[
\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()<>@,;:\\".\[\] \000-
\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(
?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\<(?:(?:\r\n)?[ \t])*(?:@(?:[^()<>@,;
:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([
^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\"
.\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\
]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\
[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\
r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]
|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()<>@,;:\\".\[\] \0
00-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\
.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,
;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?
:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*
(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".
\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[
^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]
]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\>(?:(?:\r\n)?[ \t])*)(?:,\s*(
?:(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\
".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(
?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[
\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t
])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t
])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?
:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|
\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:
[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\
]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\<(?:(?:\r\n)
?[ \t])*(?:@(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["
()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)
?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>
@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[
 \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,
;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t]
)*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?
(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".
\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:
\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\[
"()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])
*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])
+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\
.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\>(?:(
?:\r\n)?[ \t])*))*)?;\s*)
\end{verbatim}

This is a somewhat pathological example (because email addresses are
actually suprisingly complex), but is used in real code. See the
stackoverflow discussion at \url{http://stackoverflow.com/a/201378} for
more details.

Don't forget that you're in a programming language and you have other
tools at your disposal. Instead of creating one complex regular
expression, it's often easier to write a series of simpler regexps. If
you get stuck trying to create a single regexp that solves your problem,
take a step back and think if you could break the problem down into
smaller pieces, solving each challenge before moving onto the next one.

\subsection{Detect matches}\label{detect-matches}

To determine if a character vector matches a pattern, use
\texttt{str\_detect()}. It returns a logical vector the same length as
the input:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{, }\StringTok{"pear"}\NormalTok{)}
\KeywordTok{str_detect}\NormalTok{(x, }\StringTok{"e"}\NormalTok{)}
\CommentTok{#> [1]  TRUE FALSE  TRUE}
\end{Highlighting}
\end{Shaded}

Remember that when you use a logical vector in a numeric context,
\texttt{FALSE} becomes 0 and \texttt{TRUE} becomes 1. That makes
\texttt{sum()} and \texttt{mean()} useful if you want to answer
questions about matches across a larger vector:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# How many common words start with t?}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(words, }\StringTok{"^t"}\NormalTok{))}
\CommentTok{#> [1] 65}
\CommentTok{# What proportion of common words end with a vowel?}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(words, }\StringTok{"[aeiou]$"}\NormalTok{))}
\CommentTok{#> [1] 0.277}
\end{Highlighting}
\end{Shaded}

When you have complex logical conditions (e.g.~match a or b but not c
unless d) it's often easier to combine multiple \texttt{str\_detect()}
calls with logical operators, rather than trying to create a single
regular expression. For example, here are two ways to find all words
that don't contain any vowels:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Find all words containing at least one vowel, and negate}
\NormalTok{no_vowels_1 <-}\StringTok{ }\NormalTok{!}\KeywordTok{str_detect}\NormalTok{(words, }\StringTok{"[aeiou]"}\NormalTok{)}
\CommentTok{# Find all words consisting only of consonants (non-vowels)}
\NormalTok{no_vowels_2 <-}\StringTok{ }\KeywordTok{str_detect}\NormalTok{(words, }\StringTok{"^[^aeiou]+$"}\NormalTok{)}
\KeywordTok{identical}\NormalTok{(no_vowels_1, no_vowels_2)}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}

The results are identical, but I think the first approach is
significantly easier to understand. If your regular expression gets
overly complicated, try breaking it up into smaller pieces, giving each
piece a name, and then combining the pieces with logical operations.

A common use of \texttt{str\_detect()} is to select the elements that
match a pattern. You can do this with logical subsetting, or the
convenient \texttt{str\_subset()} wrapper:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{words[}\KeywordTok{str_detect}\NormalTok{(words, }\StringTok{"x$"}\NormalTok{)]}
\CommentTok{#> [1] "box" "sex" "six" "tax"}
\KeywordTok{str_subset}\NormalTok{(words, }\StringTok{"x$"}\NormalTok{)}
\CommentTok{#> [1] "box" "sex" "six" "tax"}
\end{Highlighting}
\end{Shaded}

Typically, however, your strings will be one column of a data frame, and
you'll want to use filter instead:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{word =} \NormalTok{words, }
  \DataTypeTok{i =} \KeywordTok{seq_along}\NormalTok{(word)}
\NormalTok{)}
\NormalTok{df %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(words, }\StringTok{"x$"}\NormalTok{))}
\CommentTok{#> # A tibble: 4 × 2}
\CommentTok{#>    word     i}
\CommentTok{#>   <chr> <int>}
\CommentTok{#> 1   box   108}
\CommentTok{#> 2   sex   747}
\CommentTok{#> 3   six   772}
\CommentTok{#> 4   tax   841}
\end{Highlighting}
\end{Shaded}

A variation on \texttt{str\_detect()} is \texttt{str\_count()}: rather
than a simple yes or no, it tells you how many matches there are in a
string:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{, }\StringTok{"pear"}\NormalTok{)}
\KeywordTok{str_count}\NormalTok{(x, }\StringTok{"a"}\NormalTok{)}
\CommentTok{#> [1] 1 3 1}

\CommentTok{# On average, how many vowels per word?}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{str_count}\NormalTok{(words, }\StringTok{"[aeiou]"}\NormalTok{))}
\CommentTok{#> [1] 1.99}
\end{Highlighting}
\end{Shaded}

It's natural to use \texttt{str\_count()} with \texttt{mutate()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{vowels =} \KeywordTok{str_count}\NormalTok{(word, }\StringTok{"[aeiou]"}\NormalTok{),}
    \DataTypeTok{consonants =} \KeywordTok{str_count}\NormalTok{(word, }\StringTok{"[^aeiou]"}\NormalTok{)}
  \NormalTok{)}
\CommentTok{#> # A tibble: 980 × 4}
\CommentTok{#>       word     i vowels consonants}
\CommentTok{#>      <chr> <int>  <int>      <int>}
\CommentTok{#> 1        a     1      1          0}
\CommentTok{#> 2     able     2      2          2}
\CommentTok{#> 3    about     3      3          2}
\CommentTok{#> 4 absolute     4      4          4}
\CommentTok{#> 5   accept     5      2          4}
\CommentTok{#> 6  account     6      3          4}
\CommentTok{#> # ... with 974 more rows}
\end{Highlighting}
\end{Shaded}

Note that matches never overlap. For example, in \texttt{"abababa"}, how
many times will the pattern \texttt{"aba"} match? Regular expressions
say two, not three:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_count}\NormalTok{(}\StringTok{"abababa"}\NormalTok{, }\StringTok{"aba"}\NormalTok{)}
\CommentTok{#> [1] 2}
\CommentTok{# str_view_all("abababa", "aba")}
\end{Highlighting}
\end{Shaded}

Note the use of \texttt{str\_view\_all()}. As you'll shortly learn, many
stringr functions come in pairs: one function works with a single match,
and the other works with all matches. The second function will have the
suffix \texttt{\_all}.

\subsection{Exercises}\label{exercises-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For each of the following challenges, try solving it by using both a
  single regular expression, and a combination of multiple
  \texttt{str\_detect()} calls.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Find all words that start or end with \texttt{x}.
  \item
    Find all words that start with a vowel and end with a consonant.
  \item
    Are there any words that contain at least one of each different
    vowel?
  \end{enumerate}
\item
  What word has the highest number of vowels? What word has the highest
  proportion of vowels? (Hint: what is the denominator?)
\end{enumerate}

\subsection{Extract matches}\label{extract-matches}

To extract the actual text of a match, use \texttt{str\_extract()}. To
show that off, we're going to need a more complicated example. I'm going
to use the
\href{https://en.wikipedia.org/wiki/Harvard_sentences}{Harvard
sentences}, which were designed to test VOIP systems, but are also
useful for practicing regexps. These are provided in
\texttt{stringr::sentences}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(sentences)}
\CommentTok{#> [1] 720}
\KeywordTok{head}\NormalTok{(sentences)}
\CommentTok{#> [1] "The birch canoe slid on the smooth planks." }
\CommentTok{#> [2] "Glue the sheet to the dark blue background."}
\CommentTok{#> [3] "It's easy to tell the depth of a well."     }
\CommentTok{#> [4] "These days a chicken leg is a rare dish."   }
\CommentTok{#> [5] "Rice is often served in round bowls."       }
\CommentTok{#> [6] "The juice of lemons makes fine punch."}
\end{Highlighting}
\end{Shaded}

Imagine we want to find all sentences that contain a colour. We first
create a vector of colour names, and then turn it into a single regular
expression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{colours <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"purple"}\NormalTok{)}
\NormalTok{colour_match <-}\StringTok{ }\KeywordTok{str_c}\NormalTok{(colours, }\DataTypeTok{collapse =} \StringTok{"|"}\NormalTok{)}
\NormalTok{colour_match}
\CommentTok{#> [1] "red|orange|yellow|green|blue|purple"}
\end{Highlighting}
\end{Shaded}

Now we can select the sentences that contain a colour, and then extract
the colour to figure out which one it is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{has_colour <-}\StringTok{ }\KeywordTok{str_subset}\NormalTok{(sentences, colour_match)}
\NormalTok{matches <-}\StringTok{ }\KeywordTok{str_extract}\NormalTok{(has_colour, colour_match)}
\KeywordTok{head}\NormalTok{(matches)}
\CommentTok{#> [1] "blue" "blue" "red"  "red"  "red"  "blue"}
\end{Highlighting}
\end{Shaded}

Note that \texttt{str\_extract()} only extracts the first match. We can
see that most easily by first selecting all the sentences that have more
than 1 match:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more <-}\StringTok{ }\NormalTok{sentences[}\KeywordTok{str_count}\NormalTok{(sentences, colour_match) >}\StringTok{ }\DecValTok{1}\NormalTok{]}
\CommentTok{# str_view_all(more, colour_match)}

\KeywordTok{str_extract}\NormalTok{(more, colour_match)}
\CommentTok{#> [1] "blue"   "green"  "orange"}
\end{Highlighting}
\end{Shaded}

This is a common pattern for stringr functions, because working with a
single match allows you to use much simpler data structures. To get all
matches, use \texttt{str\_extract\_all()}. It returns a list:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_extract_all}\NormalTok{(more, colour_match)}
\CommentTok{#> [[1]]}
\CommentTok{#> [1] "blue" "red" }
\CommentTok{#> }
\CommentTok{#> [[2]]}
\CommentTok{#> [1] "green" "red"  }
\CommentTok{#> }
\CommentTok{#> [[3]]}
\CommentTok{#> [1] "orange" "red"}
\end{Highlighting}
\end{Shaded}

You'll learn more about lists in \protect\hyperlink{lists}{lists} and
\protect\hyperlink{iteration}{iteration}.

If you use \texttt{simplify\ =\ TRUE}, \texttt{str\_extract\_all()} will
return a matrix with short matches expanded to the same length as the
longest:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_extract_all}\NormalTok{(more, colour_match, }\DataTypeTok{simplify =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>      [,1]     [,2] }
\CommentTok{#> [1,] "blue"   "red"}
\CommentTok{#> [2,] "green"  "red"}
\CommentTok{#> [3,] "orange" "red"}

\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"a b"}\NormalTok{, }\StringTok{"a b c"}\NormalTok{)}
\KeywordTok{str_extract_all}\NormalTok{(x, }\StringTok{"[a-z]"}\NormalTok{, }\DataTypeTok{simplify =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>      [,1] [,2] [,3]}
\CommentTok{#> [1,] "a"  ""   ""  }
\CommentTok{#> [2,] "a"  "b"  ""  }
\CommentTok{#> [3,] "a"  "b"  "c"}
\end{Highlighting}
\end{Shaded}

\subsubsection{Exercises}\label{exercises-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In the previous example, you might have noticed that the regular
  expression matched ``flickered'', which is not a colour. Modify the
  regex to fix the problem.
\item
  From the Harvard sentences data, extract:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    The first word from each sentence.
  \item
    All words ending in \texttt{ing}.
  \item
    All plurals.
  \end{enumerate}
\end{enumerate}

\subsection{Grouped matches}\label{grouped-matches}

Earlier in this chapter we talked about the use of parentheses for
clarifying precedence and for backreferences when matching. You can also
use parentheses to extract parts of a complex match. For example,
imagine we want to extract nouns from the sentences. As a heuristic,
we'll look for any word that comes after ``a'' or ``the''. Defining a
``word'' in a regular expression is a little tricky, so here I use a
simple approximation: a sequence of at least one character that isn't a
space.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noun <-}\StringTok{ "(a|the) ([^ ]+)"}

\NormalTok{has_noun <-}\StringTok{ }\NormalTok{sentences %>%}
\StringTok{  }\KeywordTok{str_subset}\NormalTok{(noun) %>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{has_noun %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_extract}\NormalTok{(noun)}
\CommentTok{#>  [1] "the smooth" "the sheet"  "the depth"  "a chicken"  "the parked"}
\CommentTok{#>  [6] "the sun"    "the huge"   "the ball"   "the woman"  "a helps"}
\end{Highlighting}
\end{Shaded}

\texttt{str\_extract()} gives us the complete match;
\texttt{str\_match()} gives each individual component. Instead of a
character vector, it returns a matrix, with one column for the complete
match followed by one column for each group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{has_noun %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_match}\NormalTok{(noun)}
\CommentTok{#>       [,1]         [,2]  [,3]     }
\CommentTok{#>  [1,] "the smooth" "the" "smooth" }
\CommentTok{#>  [2,] "the sheet"  "the" "sheet"  }
\CommentTok{#>  [3,] "the depth"  "the" "depth"  }
\CommentTok{#>  [4,] "a chicken"  "a"   "chicken"}
\CommentTok{#>  [5,] "the parked" "the" "parked" }
\CommentTok{#>  [6,] "the sun"    "the" "sun"    }
\CommentTok{#>  [7,] "the huge"   "the" "huge"   }
\CommentTok{#>  [8,] "the ball"   "the" "ball"   }
\CommentTok{#>  [9,] "the woman"  "the" "woman"  }
\CommentTok{#> [10,] "a helps"    "a"   "helps"}
\end{Highlighting}
\end{Shaded}

(Unsurprisingly, our heuristic for detecting nouns is poor, and also
picks up adjectives like smooth and parked.)

If your data is in a tibble, it's often easier to use
\texttt{tidyr::extract()}. It works like \texttt{str\_match()} but
requires you to name the matches, which are then placed in new columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{sentence =} \NormalTok{sentences) %>%}\StringTok{ }
\StringTok{  }\NormalTok{tidyr::}\KeywordTok{extract}\NormalTok{(}
    \NormalTok{sentence, }\KeywordTok{c}\NormalTok{(}\StringTok{"article"}\NormalTok{, }\StringTok{"noun"}\NormalTok{), }\StringTok{"(a|the) ([^ ]+)"}\NormalTok{, }
    \DataTypeTok{remove =} \OtherTok{FALSE}
  \NormalTok{)}
\CommentTok{#> # A tibble: 720 × 3}
\CommentTok{#>                                      sentence article    noun}
\CommentTok{#> *                                       <chr>   <chr>   <chr>}
\CommentTok{#> 1  The birch canoe slid on the smooth planks.     the  smooth}
\CommentTok{#> 2 Glue the sheet to the dark blue background.     the   sheet}
\CommentTok{#> 3      It's easy to tell the depth of a well.     the   depth}
\CommentTok{#> 4    These days a chicken leg is a rare dish.       a chicken}
\CommentTok{#> 5        Rice is often served in round bowls.    <NA>    <NA>}
\CommentTok{#> 6       The juice of lemons makes fine punch.    <NA>    <NA>}
\CommentTok{#> # ... with 714 more rows}
\end{Highlighting}
\end{Shaded}

Like \texttt{str\_extract()}, if you want all matches for each string,
you'll need \texttt{str\_match\_all()}.

\subsubsection{Exercises}\label{exercises-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find all words that come after a ``number'' like ``one'', ``two'',
  ``three'' etc. Pull out both the number and the word.
\item
  Find all contractions. Separate out the pieces before and after the
  apostrophe.
\end{enumerate}

\subsection{Replacing matches}\label{replacing-matches}

\texttt{str\_replace()} and \texttt{str\_replace\_all()} allow you to
replace matches with new strings. The simplest use is to replace a
pattern with a fixed string:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"pear"}\NormalTok{, }\StringTok{"banana"}\NormalTok{)}
\KeywordTok{str_replace}\NormalTok{(x, }\StringTok{"[aeiou]"}\NormalTok{, }\StringTok{"-"}\NormalTok{)}
\CommentTok{#> [1] "-pple"  "p-ar"   "b-nana"}
\KeywordTok{str_replace_all}\NormalTok{(x, }\StringTok{"[aeiou]"}\NormalTok{, }\StringTok{"-"}\NormalTok{)}
\CommentTok{#> [1] "-ppl-"  "p--r"   "b-n-n-"}
\end{Highlighting}
\end{Shaded}

With \texttt{str\_replace\_all()} you can perform multiple replacements
by supplying a named vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"1 house"}\NormalTok{, }\StringTok{"2 cars"}\NormalTok{, }\StringTok{"3 people"}\NormalTok{)}
\KeywordTok{str_replace_all}\NormalTok{(x, }\KeywordTok{c}\NormalTok{(}\StringTok{"1"} \NormalTok{=}\StringTok{ "one"}\NormalTok{, }\StringTok{"2"} \NormalTok{=}\StringTok{ "two"}\NormalTok{, }\StringTok{"3"} \NormalTok{=}\StringTok{ "three"}\NormalTok{))}
\CommentTok{#> [1] "one house"    "two cars"     "three people"}
\end{Highlighting}
\end{Shaded}

Instead of replacing with a fixed string you can use backreferences to
insert components of the match. In the following code, I flip the order
of the second and third words.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentences %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_replace}\NormalTok{(}\StringTok{"([^ ]+) ([^ ]+) ([^ ]+)"}\NormalTok{, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{1 }\CharTok{\textbackslash{}\textbackslash{}}\StringTok{3 }\CharTok{\textbackslash{}\textbackslash{}}\StringTok{2"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] "The canoe birch slid on the smooth planks." }
\CommentTok{#> [2] "Glue sheet the to the dark blue background."}
\CommentTok{#> [3] "It's to easy tell the depth of a well."     }
\CommentTok{#> [4] "These a days chicken leg is a rare dish."   }
\CommentTok{#> [5] "Rice often is served in round bowls."}
\end{Highlighting}
\end{Shaded}

\subsubsection{Exercises}\label{exercises-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Replace all forward slashes in a string with backslashes.
\item
  Implement a simple version of \texttt{str\_to\_lower()} using
  \texttt{replace\_all()}.
\item
  Switch the first and last letters in \texttt{words}. Which of those
  strings are still words?
\end{enumerate}

\subsection{Splitting}\label{splitting}

Use \texttt{str\_split()} to split a string up into pieces. For example,
we could split sentences into words:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentences %>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{5}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_split}\NormalTok{(}\StringTok{" "}\NormalTok{)}
\CommentTok{#> [[1]]}
\CommentTok{#> [1] "The"     "birch"   "canoe"   "slid"    "on"      "the"     "smooth" }
\CommentTok{#> [8] "planks."}
\CommentTok{#> }
\CommentTok{#> [[2]]}
\CommentTok{#> [1] "Glue"        "the"         "sheet"       "to"          "the"        }
\CommentTok{#> [6] "dark"        "blue"        "background."}
\CommentTok{#> }
\CommentTok{#> [[3]]}
\CommentTok{#> [1] "It's"  "easy"  "to"    "tell"  "the"   "depth" "of"    "a"     "well."}
\CommentTok{#> }
\CommentTok{#> [[4]]}
\CommentTok{#> [1] "These"   "days"    "a"       "chicken" "leg"     "is"      "a"      }
\CommentTok{#> [8] "rare"    "dish."  }
\CommentTok{#> }
\CommentTok{#> [[5]]}
\CommentTok{#> [1] "Rice"   "is"     "often"  "served" "in"     "round"  "bowls."}
\end{Highlighting}
\end{Shaded}

Because each component might contain a different number of pieces, this
returns a list. If you're working with a length-1 vector, the easiest
thing is to just extract the first element of the list:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"a|b|c|d"} \NormalTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_split}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{|"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\NormalTok{.[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{#> [1] "a" "b" "c" "d"}
\end{Highlighting}
\end{Shaded}

Otherwise, like the other stringr functions that return a list, you can
use \texttt{simplify\ =\ TRUE} to return a matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentences %>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{5}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_split}\NormalTok{(}\StringTok{" "}\NormalTok{, }\DataTypeTok{simplify =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>      [,1]    [,2]    [,3]    [,4]      [,5]  [,6]    [,7]    }
\CommentTok{#> [1,] "The"   "birch" "canoe" "slid"    "on"  "the"   "smooth"}
\CommentTok{#> [2,] "Glue"  "the"   "sheet" "to"      "the" "dark"  "blue"  }
\CommentTok{#> [3,] "It's"  "easy"  "to"    "tell"    "the" "depth" "of"    }
\CommentTok{#> [4,] "These" "days"  "a"     "chicken" "leg" "is"    "a"     }
\CommentTok{#> [5,] "Rice"  "is"    "often" "served"  "in"  "round" "bowls."}
\CommentTok{#>      [,8]          [,9]   }
\CommentTok{#> [1,] "planks."     ""     }
\CommentTok{#> [2,] "background." ""     }
\CommentTok{#> [3,] "a"           "well."}
\CommentTok{#> [4,] "rare"        "dish."}
\CommentTok{#> [5,] ""            ""}
\end{Highlighting}
\end{Shaded}

You can also request a maximum number of pieces:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fields <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Name: Hadley"}\NormalTok{, }\StringTok{"Country: NZ"}\NormalTok{, }\StringTok{"Age: 35"}\NormalTok{)}
\NormalTok{fields %>%}\StringTok{ }\KeywordTok{str_split}\NormalTok{(}\StringTok{": "}\NormalTok{, }\DataTypeTok{n =} \DecValTok{2}\NormalTok{, }\DataTypeTok{simplify =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>      [,1]      [,2]    }
\CommentTok{#> [1,] "Name"    "Hadley"}
\CommentTok{#> [2,] "Country" "NZ"    }
\CommentTok{#> [3,] "Age"     "35"}
\end{Highlighting}
\end{Shaded}

Instead of splitting up strings by patterns, you can also split up by
character, line, sentence and word \texttt{boundary()}s:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "This is a sentence.  This is another sentence."}
\CommentTok{# str_view_all(x, boundary("word"))}

\KeywordTok{str_split}\NormalTok{(x, }\StringTok{" "}\NormalTok{)[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{#> [1] "This"      "is"        "a"         "sentence." ""          "This"     }
\CommentTok{#> [7] "is"        "another"   "sentence."}
\KeywordTok{str_split}\NormalTok{(x, }\KeywordTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{))[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{#> [1] "This"     "is"       "a"        "sentence" "This"     "is"      }
\CommentTok{#> [7] "another"  "sentence"}
\end{Highlighting}
\end{Shaded}

\subsubsection{Exercises}\label{exercises-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Split up a string like \texttt{"apples,\ pears,\ and\ bananas"} into
  individual components.
\item
  Why is it better to split up by \texttt{boundary("word")} than
  \texttt{"\ "}?
\item
  What does splitting with an empty string (\texttt{""}) do? Experiment,
  and then read the documentation.
\end{enumerate}

\subsection{Find matches}\label{find-matches}

\texttt{str\_locate()} and \texttt{str\_locate\_all()} give you the
starting and ending positions of each match. These are particularly
useful when none of the other functions does exactly what you want. You
can use \texttt{str\_locate()} to find the matching pattern,
\texttt{str\_sub()} to extract and/or modify them.

\section{Other types of pattern}\label{other-types-of-pattern}

When you use a pattern that's a string, it's automatically wrapped into
a call to \texttt{regex()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The regular call:}
\CommentTok{# str_view(fruit, "nana")}
\CommentTok{# Is shorthand for}
\CommentTok{# str_view(fruit, regex("nana"))}
\end{Highlighting}
\end{Shaded}

You can use the other arguments of \texttt{regex()} to control details
of the match:

\begin{itemize}
\item
  \texttt{ignore\_case\ =\ TRUE} allows characters to match either their
  uppercase or lowercase forms. This always uses the current locale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bananas <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"banana"}\NormalTok{, }\StringTok{"Banana"}\NormalTok{, }\StringTok{"BANANA"}\NormalTok{)}
\CommentTok{# str_view(bananas, "banana")}
\CommentTok{# str_view(bananas, regex("banana", ignore_case = TRUE))}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{multiline\ =\ TRUE} allows \texttt{\^{}} and \texttt{\$} to
  match the start and end of each line rather than the start and end of
  the complete string.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "Line 1}\CharTok{\textbackslash{}n}\StringTok{Line 2}\CharTok{\textbackslash{}n}\StringTok{Line 3"}
\KeywordTok{str_extract_all}\NormalTok{(x, }\StringTok{"^Line"}\NormalTok{)[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{#> [1] "Line"}
\KeywordTok{str_extract_all}\NormalTok{(x, }\KeywordTok{regex}\NormalTok{(}\StringTok{"^Line"}\NormalTok{, }\DataTypeTok{multiline =} \OtherTok{TRUE}\NormalTok{))[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{#> [1] "Line" "Line" "Line"}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{comments\ =\ TRUE} allows you to use comments and white space
  to make complex regular expressions more understandable. Spaces are
  ignored, as is everything after \texttt{\#}. To match a literal space,
  you'll need to escape it:
  \texttt{"\textbackslash{}\textbackslash{}\ "}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phone <-}\StringTok{ }\KeywordTok{regex}\NormalTok{(}\StringTok{"}
\StringTok{  }\CharTok{\textbackslash{}\textbackslash{}}\StringTok{(?     # optional opening parens}
\StringTok{  (}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{d\{3\}) # area code}
\StringTok{  [)- ]?   # optional closing parens, dash, or space}
\StringTok{  (}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{d\{3\}) # another three numbers}
\StringTok{  [ -]?    # optional space or dash}
\StringTok{  (}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{d\{3\}) # three more numbers}
\StringTok{  "}\NormalTok{, }\DataTypeTok{comments =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{str_match}\NormalTok{(}\StringTok{"514-791-8141"}\NormalTok{, phone)}
\CommentTok{#>      [,1]          [,2]  [,3]  [,4] }
\CommentTok{#> [1,] "514-791-814" "514" "791" "814"}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{dotall\ =\ TRUE} allows \texttt{.} to match everything,
  including \texttt{\textbackslash{}n}.
\end{itemize}

There are three other functions you can use instead of \texttt{regex()}:

\begin{itemize}
\item
  \texttt{fixed()}: matches exactly the specified sequence of bytes. It
  ignores all special regular expressions and operates at a very low
  level. This allows you to avoid complex escaping and can be much
  faster than regular expressions. The following microbenchmark shows
  that it's about 3x faster for a simple example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{microbenchmark::}\KeywordTok{microbenchmark}\NormalTok{(}
  \DataTypeTok{fixed =} \KeywordTok{str_detect}\NormalTok{(sentences, }\KeywordTok{fixed}\NormalTok{(}\StringTok{"the"}\NormalTok{)),}
  \DataTypeTok{regex =} \KeywordTok{str_detect}\NormalTok{(sentences, }\StringTok{"the"}\NormalTok{),}
  \DataTypeTok{times =} \DecValTok{20}
\NormalTok{)}
\CommentTok{#> Unit: microseconds}
\CommentTok{#>   expr min  lq mean median  uq max neval cld}
\CommentTok{#>  fixed 113 115  130    119 126 302    20  a }
\CommentTok{#>  regex 368 370  378    371 374 461    20   b}
\end{Highlighting}
\end{Shaded}

  Beware using \texttt{fixed()} with non-English data. It is problematic
  because there are often multiple ways of representing the same
  character. For example, there are two ways to define ``á'': either as
  a single character or as an ``a'' plus an accent:


  They render identically, but because they're defined differently,
  \texttt{fixed()} doesn't find a match. Instead, you can use
  \texttt{coll()}, defined next, to respect human character comparison
  rules:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str_detect}\NormalTok{(a1, }\KeywordTok{fixed}\NormalTok{(a2))}
\CommentTok{#> [1] FALSE}
\KeywordTok{str_detect}\NormalTok{(a1, }\KeywordTok{coll}\NormalTok{(a2))}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{coll()}: compare strings using standard \textbf{coll}ation
  rules. This is useful for doing case insensitive matching. Note that
  \texttt{coll()} takes a \texttt{locale} parameter that controls which
  rules are used for comparing characters. Unfortunately different parts
  of the world use different rules!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# That means you also need to be aware of the difference}
\CommentTok{# when doing case insensitive matches:}
\NormalTok{i <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"I"}\NormalTok{, }\StringTok{"İ"}\NormalTok{, }\StringTok{"i"}\NormalTok{, }\StringTok{"ı"}\NormalTok{)}
\NormalTok{i}
\CommentTok{#> [1] "I" "İ" "i" "ı"}

\KeywordTok{str_subset}\NormalTok{(i, }\KeywordTok{coll}\NormalTok{(}\StringTok{"i"}\NormalTok{, }\DataTypeTok{ignore_case =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> [1] "I" "i"}
\KeywordTok{str_subset}\NormalTok{(i, }\KeywordTok{coll}\NormalTok{(}\StringTok{"i"}\NormalTok{, }\DataTypeTok{ignore_case =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{locale =} \StringTok{"tr"}\NormalTok{))}
\CommentTok{#> [1] "İ" "i"}
\end{Highlighting}
\end{Shaded}

  Both \texttt{fixed()} and \texttt{regex()} have \texttt{ignore\_case}
  arguments, but they do not allow you to pick the locale: they always
  use the default locale. You can see what that is with the following
  code; more on stringi later.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stringi::}\KeywordTok{stri_locale_info}\NormalTok{()}
\CommentTok{#> $Language}
\CommentTok{#> [1] "en"}
\CommentTok{#> }
\CommentTok{#> $Country}
\CommentTok{#> [1] "US"}
\CommentTok{#> }
\CommentTok{#> $Variant}
\CommentTok{#> [1] ""}
\CommentTok{#> }
\CommentTok{#> $Name}
\CommentTok{#> [1] "en_US"}
\end{Highlighting}
\end{Shaded}

  The downside of \texttt{coll()} is speed; because the rules for
  recognising which characters are the same are complicated,
  \texttt{coll()} is relatively slow compared to \texttt{regex()} and
  \texttt{fixed()}.
\item
  As you saw with \texttt{str\_split()} you can use \texttt{boundary()}
  to match boundaries. You can also use it with the other functions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "This is a sentence."}
\CommentTok{# str_view_all(x, boundary("word"))}
\KeywordTok{str_extract_all}\NormalTok{(x, }\KeywordTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{))}
\CommentTok{#> [[1]]}
\CommentTok{#> [1] "This"     "is"       "a"        "sentence"}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsection{Exercises}\label{exercises-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How would you find all strings containing \texttt{\textbackslash{}}
  with \texttt{regex()} vs.~with \texttt{fixed()}?
\item
  What are the five most common words in \texttt{sentences}?
\end{enumerate}

\section{Other uses of regular
expressions}\label{other-uses-of-regular-expressions}

There are two useful function in base R that also use regular
expressions:

\begin{itemize}
\item
  \texttt{apropos()} searches all objects available from the global
  environment. This is useful if you can't quite remember the name of
  the function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apropos}\NormalTok{(}\StringTok{"replace"}\NormalTok{)}
\CommentTok{#> [1] "%+replace%"      "replace"         "replace_na"      "str_replace"    }
\CommentTok{#> [5] "str_replace_all" "str_replace_na"  "theme_replace"}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{dir()} lists all the files in a directory. The
  \texttt{pattern} argument takes a regular expression and only returns
  file names that match the pattern. For example, you can find all the R
  Markdown files in the current directory with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{dir}\NormalTok{(}\DataTypeTok{pattern =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{.Rmd$"}\NormalTok{))}
\CommentTok{#> [1] "communicate-plots.Rmd" "communicate.Rmd"       "datetimes.Rmd"        }
\CommentTok{#> [4] "EDA.Rmd"               "explore.Rmd"           "factors.Rmd"}
\end{Highlighting}
\end{Shaded}

  (If you're more comfortable with ``globs'' like \texttt{*.Rmd}, you
  can convert them to regular expressions with \texttt{glob2rx()}):
\end{itemize}

\section{stringi}\label{stringi}

stringr is built on top of the \textbf{stringi} package. stringr is
useful when you're learning because it exposes a minimal set of
functions, which have been carefully picked to handle the most common
string manipulation functions. stringi, on the other hand, is designed
to be comprehensive. It contains almost every function you might ever
need: stringi has 234 functions to stringr's 42.

If you find yourself struggling to do something in stringr, it's worth
taking a look at stringi. The packages work very similarly, so you
should be able to translate your stringr knowledge in a natural way. The
main difference is the prefix: \texttt{str\_} vs. \texttt{stri\_}.

\subsection{Exercises}\label{exercises-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find the stringi functions that:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Count the number of words.
  \item
    Find duplicated strings.
  \item
    Generate random text.
  \end{enumerate}
\item
  How do you control the language that \texttt{stri\_sort()} uses for
  sorting?
\end{enumerate}

\chapter{Factors}\label{factors}

\section{Introduction}\label{introduction-9}

In R, factors are used to work with categorical variables, variables
that have a fixed and known set of possible values. They are also useful
when you want to display character vectors in a non-alphabetical order.

Historically, factors were much easier to work with than characters. As
a result, many of the functions in base R automatically convert
characters to factors. This means that factors often crop up in places
where they're not actually helpful. Fortunately, you don't need to worry
about that in the tidyverse, and can focus on situations where factors
are genuinely useful.

For more historical context on factors, I recommend
\href{http://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/}{\emph{stringsAsFactors:
An unauthorized biography}} by Roger Peng, and
\href{http://notstatschat.tumblr.com/post/124987394001/stringsasfactors-sigh}{\emph{stringsAsFactors
= \textless{}sigh\textgreater{}}} by Thomas Lumley.

\subsection{Prerequisites}\label{prerequisites-9}

To work with factors, we'll use the \textbf{forcats} package, which
provides tools for dealing with \textbf{cat}egorical variables (and it's
an anagram of factors!). It provides a wide range of helpers for working
with factors. forcats is not part of the core tidyverse, so we need to
load it explicitly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(forcats)}
\end{Highlighting}
\end{Shaded}

\section{Creating factors}\label{creating-factors}

Imagine that you have a variable that records month:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Dec"}\NormalTok{, }\StringTok{"Apr"}\NormalTok{, }\StringTok{"Jan"}\NormalTok{, }\StringTok{"Mar"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Using a string to record this variable has two problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  There are only twelve possible months, and there's nothing saving you
  from typos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Dec"}\NormalTok{, }\StringTok{"Apr"}\NormalTok{, }\StringTok{"Jam"}\NormalTok{, }\StringTok{"Mar"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  It doesn't sort in a useful way:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sort}\NormalTok{(x1)}
\CommentTok{#> [1] "Apr" "Dec" "Jan" "Mar"}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

You can fix both of these problems with a factor. To create a factor you
must start by creating a list of the valid \textbf{levels}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{month_levels <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
  \StringTok{"Jan"}\NormalTok{, }\StringTok{"Feb"}\NormalTok{, }\StringTok{"Mar"}\NormalTok{, }\StringTok{"Apr"}\NormalTok{, }\StringTok{"May"}\NormalTok{, }\StringTok{"Jun"}\NormalTok{, }
  \StringTok{"Jul"}\NormalTok{, }\StringTok{"Aug"}\NormalTok{, }\StringTok{"Sep"}\NormalTok{, }\StringTok{"Oct"}\NormalTok{, }\StringTok{"Nov"}\NormalTok{, }\StringTok{"Dec"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now you can create a factor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y1 <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(x1, }\DataTypeTok{levels =} \NormalTok{month_levels)}
\NormalTok{y1}
\CommentTok{#> [1] Dec Apr Jan Mar}
\CommentTok{#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec}
\KeywordTok{sort}\NormalTok{(y1)}
\CommentTok{#> [1] Jan Mar Apr Dec}
\CommentTok{#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec}
\end{Highlighting}
\end{Shaded}

And any values not in the set will be silently converted to NA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y2 <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(x2, }\DataTypeTok{levels =} \NormalTok{month_levels)}
\NormalTok{y2}
\CommentTok{#> [1] Dec  Apr  <NA> Mar }
\CommentTok{#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec}
\end{Highlighting}
\end{Shaded}

If you want a warning, you can use \texttt{readr::parse\_factor()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y2 <-}\StringTok{ }\KeywordTok{parse_factor}\NormalTok{(x2, }\DataTypeTok{levels =} \NormalTok{month_levels)}
\CommentTok{#> Warning: 1 parsing failure.}
\CommentTok{#> row col           expected actual}
\CommentTok{#>   3  -- value in level set    Jam}
\end{Highlighting}
\end{Shaded}

If you omit the levels, they'll be taken from the data in alphabetical
order:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{factor}\NormalTok{(x1)}
\CommentTok{#> [1] Dec Apr Jan Mar}
\CommentTok{#> Levels: Apr Dec Jan Mar}
\end{Highlighting}
\end{Shaded}

Sometimes you'd prefer that the order of the levels match the order of
the first appearance in the data. You can do that when creating the
factor by setting levels to \texttt{unique(x)}, or after the fact, with
\texttt{fct\_inorder()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f1 <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(x1, }\DataTypeTok{levels =} \KeywordTok{unique}\NormalTok{(x1))}
\NormalTok{f1}
\CommentTok{#> [1] Dec Apr Jan Mar}
\CommentTok{#> Levels: Dec Apr Jan Mar}

\NormalTok{f2 <-}\StringTok{ }\NormalTok{x1 %>%}\StringTok{ }\KeywordTok{factor}\NormalTok{() %>%}\StringTok{ }\KeywordTok{fct_inorder}\NormalTok{()}
\NormalTok{f2}
\CommentTok{#> [1] Dec Apr Jan Mar}
\CommentTok{#> Levels: Dec Apr Jan Mar}
\end{Highlighting}
\end{Shaded}

If you ever need to access the set of valid levels directly, you can do
so with \texttt{levels()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(f2)}
\CommentTok{#> [1] "Dec" "Apr" "Jan" "Mar"}
\end{Highlighting}
\end{Shaded}

\section{General Social Survey}\label{general-social-survey}

For the rest of this chapter, we're going to focus on
\texttt{forcats::gss\_cat}. It's a sample of data from the
\href{http://gss.norc.org}{General Social Survey}, which is a
long-running US survey conducted by the independent research
organization NORC at the University of Chicago. The survey has thousands
of questions, so in \texttt{gss\_cat} I've selected a handful that will
illustrate some common challenges you'll encounter when working with
factors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat}
\CommentTok{#> # A tibble: 21,483 × 9}
\CommentTok{#>    year       marital   age   race        rincome            partyid}
\CommentTok{#>   <int>        <fctr> <int> <fctr>         <fctr>             <fctr>}
\CommentTok{#> 1  2000 Never married    26  White  $8000 to 9999       Ind,near rep}
\CommentTok{#> 2  2000      Divorced    48  White  $8000 to 9999 Not str republican}
\CommentTok{#> 3  2000       Widowed    67  White Not applicable        Independent}
\CommentTok{#> 4  2000 Never married    39  White Not applicable       Ind,near rep}
\CommentTok{#> 5  2000      Divorced    25  White Not applicable   Not str democrat}
\CommentTok{#> 6  2000       Married    25  White $20000 - 24999    Strong democrat}
\CommentTok{#> # ... with 2.148e+04 more rows, and 3 more variables: relig <fctr>,}
\CommentTok{#> #   denom <fctr>, tvhours <int>}
\end{Highlighting}
\end{Shaded}

(Remember, since this dataset is provided by a package, you can get more
information about the variables with \texttt{?gss\_cat}.)

When factors are stored in a tibble, you can't see their levels so
easily. One way to see them is with \texttt{count()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(race)}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>     race     n}
\CommentTok{#>   <fctr> <int>}
\CommentTok{#> 1  Other  1959}
\CommentTok{#> 2  Black  3129}
\CommentTok{#> 3  White 16395}
\end{Highlighting}
\end{Shaded}

Or with a bar chart:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gss_cat, }\KeywordTok{aes}\NormalTok{(race)) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-14-1} \end{center}

By default, ggplot2 will drop levels that don't have any values. You can
force them to display with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gss_cat, }\KeywordTok{aes}\NormalTok{(race)) +}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{drop =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-15-1} \end{center}

These levels represent valid values that simply did not occur in this
dataset. Unfortunately, dplyr doesn't yet have a \texttt{drop} option,
but it will in the future.

When working with factors, the two most common operations are changing
the order of the levels, and changing the values of the levels. Those
operations are described in the sections below.

\subsection{Exercise}\label{exercise}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explore the distribution of \texttt{rincome} (reported income). What
  makes the default bar chart hard to understand? How could you improve
  the plot?
\item
  What is the most common \texttt{relig} in this survey? What's the most
  common \texttt{partyid}?
\item
  Which \texttt{relig} does \texttt{denom} (denomination) apply to? How
  can you find out with a table? How can you find out with a
  visualisation?
\end{enumerate}

\section{Modifying factor order}\label{modifying-factor-order}

It's often useful to change the order of the factor levels in a
visualisation. For example, imagine you want to explore the average
number of hours spent watching TV per day across religions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{relig <-}\StringTok{ }\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(relig) %>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{age =} \KeywordTok{mean}\NormalTok{(age, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{tvhours =} \KeywordTok{mean}\NormalTok{(tvhours, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{()}
  \NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(relig, }\KeywordTok{aes}\NormalTok{(tvhours, relig)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-16-1} \end{center}

It is difficult to interpret this plot because there's no overall
pattern. We can improve it by reordering the levels of \texttt{relig}
using \texttt{fct\_reorder()}. \texttt{fct\_reorder()} takes three
arguments:

\begin{itemize}
\tightlist
\item
  \texttt{f}, the factor whose levels you want to modify.
\item
  \texttt{x}, a numeric vector that you want to use to reorder the
  levels.
\item
  Optionally, \texttt{fun}, a function that's used if there are multiple
  values of \texttt{x} for each value of \texttt{f}. The default value
  is \texttt{median}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(relig, }\KeywordTok{aes}\NormalTok{(tvhours, }\KeywordTok{fct_reorder}\NormalTok{(relig, tvhours))) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-17-1} \end{center}

Reordering religion makes it much easier to see that people in the
``Don't know'' category watch much more TV, and Hinduism \& Other
Eastern religions watch much less.

As you start making more complicated transformations, I'd recommend
moving them out of \texttt{aes()} and into a separate \texttt{mutate()}
step. For example, you could rewrite the plot above as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{relig %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{relig =} \KeywordTok{fct_reorder}\NormalTok{(relig, tvhours)) %>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(tvhours, relig)) +}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

What if we create a similar plot looking at how average age varies
across reported income level?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rincome <-}\StringTok{ }\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(rincome) %>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{age =} \KeywordTok{mean}\NormalTok{(age, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{tvhours =} \KeywordTok{mean}\NormalTok{(tvhours, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{()}
  \NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(rincome, }\KeywordTok{aes}\NormalTok{(age, }\KeywordTok{fct_reorder}\NormalTok{(rincome, age))) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-19-1} \end{center}

Here, arbitrarily reordering the levels isn't a good idea! That's
because \texttt{rincome} already has a principled order that we
shouldn't mess with. Reserve \texttt{fct\_reorder()} for factors whose
levels are arbitrarily ordered.

However, it does make sense to pull ``Not applicable'' to the front with
the other special levels. You can use \texttt{fct\_relevel()}. It takes
a factor, \texttt{f}, and then any number of levels that you want to
move to the front of the line.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(rincome, }\KeywordTok{aes}\NormalTok{(age, }\KeywordTok{fct_relevel}\NormalTok{(rincome, }\StringTok{"Not applicable"}\NormalTok{))) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-20-1} \end{center}

Why do you think the average age for ``Not applicable'' is so high?

Another type of reordering is useful when you are colouring the lines on
a plot. \texttt{fct\_reorder2()} reorders the factor by the \texttt{y}
values associated with the largest \texttt{x} values. This makes the
plot easier to read because the line colours line up with the legend.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_age <-}\StringTok{ }\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(!}\KeywordTok{is.na}\NormalTok{(age)) %>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(age, marital) %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{() %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop =} \NormalTok{n /}\StringTok{ }\KeywordTok{sum}\NormalTok{(n))}

\KeywordTok{ggplot}\NormalTok{(by_age, }\KeywordTok{aes}\NormalTok{(age, prop, }\DataTypeTok{colour =} \NormalTok{marital)) +}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(by_age, }\KeywordTok{aes}\NormalTok{(age, prop, }\DataTypeTok{colour =} \KeywordTok{fct_reorder2}\NormalTok{(marital, age, prop))) +}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() +}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{colour =} \StringTok{"marital"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-21-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-21-2}

Finally, for bar plots, you can use \texttt{fct\_infreq()} to order
levels in increasing frequency: this is the simplest type of reordering
because it doesn't need any extra variables. You may want to combine
with \texttt{fct\_rev()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{marital =} \NormalTok{marital %>%}\StringTok{ }\KeywordTok{fct_infreq}\NormalTok{() %>%}\StringTok{ }\KeywordTok{fct_rev}\NormalTok{()) %>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(marital)) +}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/factors_files/figure-latex/unnamed-chunk-22-1} \end{center}

\subsection{Exercises}\label{exercises-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  There are some suspiciously high numbers in \texttt{tvhours}. Is the
  mean a good summary?
\item
  For each factor in \texttt{gss\_cat} identify whether the order of the
  levels is arbitrary or principled.
\item
  Why did moving ``Not applicable'' to the front of the levels move it
  to the bottom of the plot?
\end{enumerate}

\section{Modifying factor levels}\label{modifying-factor-levels}

More powerful than changing the orders of the levels is changing their
values. This allows you to clarify labels for publication, and collapse
levels for high-level displays. The most general and powerful tool is
\texttt{fct\_recode()}. It allows you to recode, or change, the value of
each level. For example, take the \texttt{gss\_cat\$partyid}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}\StringTok{ }\KeywordTok{count}\NormalTok{(partyid)}
\CommentTok{#> # A tibble: 10 × 2}
\CommentTok{#>              partyid     n}
\CommentTok{#>               <fctr> <int>}
\CommentTok{#> 1          No answer   154}
\CommentTok{#> 2         Don't know     1}
\CommentTok{#> 3        Other party   393}
\CommentTok{#> 4  Strong republican  2314}
\CommentTok{#> 5 Not str republican  3032}
\CommentTok{#> 6       Ind,near rep  1791}
\CommentTok{#> # ... with 4 more rows}
\end{Highlighting}
\end{Shaded}

The levels are terse and inconsistent. Let's tweak them to be longer and
use a parallel construction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{partyid =} \KeywordTok{fct_recode}\NormalTok{(partyid,}
    \StringTok{"Republican, strong"}    \NormalTok{=}\StringTok{ "Strong republican"}\NormalTok{,}
    \StringTok{"Republican, weak"}      \NormalTok{=}\StringTok{ "Not str republican"}\NormalTok{,}
    \StringTok{"Independent, near rep"} \NormalTok{=}\StringTok{ "Ind,near rep"}\NormalTok{,}
    \StringTok{"Independent, near dem"} \NormalTok{=}\StringTok{ "Ind,near dem"}\NormalTok{,}
    \StringTok{"Democrat, weak"}        \NormalTok{=}\StringTok{ "Not str democrat"}\NormalTok{,}
    \StringTok{"Democrat, strong"}      \NormalTok{=}\StringTok{ "Strong democrat"}
  \NormalTok{)) %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(partyid)}
\CommentTok{#> # A tibble: 10 × 2}
\CommentTok{#>                 partyid     n}
\CommentTok{#>                  <fctr> <int>}
\CommentTok{#> 1             No answer   154}
\CommentTok{#> 2            Don't know     1}
\CommentTok{#> 3           Other party   393}
\CommentTok{#> 4    Republican, strong  2314}
\CommentTok{#> 5      Republican, weak  3032}
\CommentTok{#> 6 Independent, near rep  1791}
\CommentTok{#> # ... with 4 more rows}
\end{Highlighting}
\end{Shaded}

\texttt{fct\_recode()} will leave levels that aren't explicitly
mentioned as is, and will warn you if you accidentally refer to a level
that doesn't exist.

To combine groups, you can assign multiple old levels to the same new
level:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{partyid =} \KeywordTok{fct_recode}\NormalTok{(partyid,}
    \StringTok{"Republican, strong"}    \NormalTok{=}\StringTok{ "Strong republican"}\NormalTok{,}
    \StringTok{"Republican, weak"}      \NormalTok{=}\StringTok{ "Not str republican"}\NormalTok{,}
    \StringTok{"Independent, near rep"} \NormalTok{=}\StringTok{ "Ind,near rep"}\NormalTok{,}
    \StringTok{"Independent, near dem"} \NormalTok{=}\StringTok{ "Ind,near dem"}\NormalTok{,}
    \StringTok{"Democrat, weak"}        \NormalTok{=}\StringTok{ "Not str democrat"}\NormalTok{,}
    \StringTok{"Democrat, strong"}      \NormalTok{=}\StringTok{ "Strong democrat"}\NormalTok{,}
    \StringTok{"Other"}                 \NormalTok{=}\StringTok{ "No answer"}\NormalTok{,}
    \StringTok{"Other"}                 \NormalTok{=}\StringTok{ "Don't know"}\NormalTok{,}
    \StringTok{"Other"}                 \NormalTok{=}\StringTok{ "Other party"}
  \NormalTok{)) %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(partyid)}
\CommentTok{#> # A tibble: 8 × 2}
\CommentTok{#>                 partyid     n}
\CommentTok{#>                  <fctr> <int>}
\CommentTok{#> 1                 Other   548}
\CommentTok{#> 2    Republican, strong  2314}
\CommentTok{#> 3      Republican, weak  3032}
\CommentTok{#> 4 Independent, near rep  1791}
\CommentTok{#> 5           Independent  4119}
\CommentTok{#> 6 Independent, near dem  2499}
\CommentTok{#> # ... with 2 more rows}
\end{Highlighting}
\end{Shaded}

You must use this technique with care: if you group together categories
that are truly different you will end up with misleading results.

If you want to collapse a lot of levels, \texttt{fct\_collapse()} is a
useful variant of \texttt{fct\_recode()}. For each new variable, you can
provide a vector of old levels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{partyid =} \KeywordTok{fct_collapse}\NormalTok{(partyid,}
    \DataTypeTok{other =} \KeywordTok{c}\NormalTok{(}\StringTok{"No answer"}\NormalTok{, }\StringTok{"Don't know"}\NormalTok{, }\StringTok{"Other party"}\NormalTok{),}
    \DataTypeTok{rep =} \KeywordTok{c}\NormalTok{(}\StringTok{"Strong republican"}\NormalTok{, }\StringTok{"Not str republican"}\NormalTok{),}
    \DataTypeTok{ind =} \KeywordTok{c}\NormalTok{(}\StringTok{"Ind,near rep"}\NormalTok{, }\StringTok{"Independent"}\NormalTok{, }\StringTok{"Ind,near dem"}\NormalTok{),}
    \DataTypeTok{dem =} \KeywordTok{c}\NormalTok{(}\StringTok{"Not str democrat"}\NormalTok{, }\StringTok{"Strong democrat"}\NormalTok{)}
  \NormalTok{)) %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(partyid)}
\CommentTok{#> # A tibble: 4 × 2}
\CommentTok{#>   partyid     n}
\CommentTok{#>    <fctr> <int>}
\CommentTok{#> 1   other   548}
\CommentTok{#> 2     rep  5346}
\CommentTok{#> 3     ind  8409}
\CommentTok{#> 4     dem  7180}
\end{Highlighting}
\end{Shaded}

Sometimes you just want to lump together all the small groups to make a
plot or table simpler. That's the job of \texttt{fct\_lump()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{relig =} \KeywordTok{fct_lump}\NormalTok{(relig)) %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(relig)}
\CommentTok{#> # A tibble: 2 × 2}
\CommentTok{#>        relig     n}
\CommentTok{#>       <fctr> <int>}
\CommentTok{#> 1 Protestant 10846}
\CommentTok{#> 2      Other 10637}
\end{Highlighting}
\end{Shaded}

The default behaviour is to progressively lump together the smallest
groups, ensuring that the aggregate is still the smallest group. In this
case it's not very helpful: it is true that the majority of Americans in
this survey are Protestant, but we've probably over collapsed.

Instead, we can use the \texttt{n} parameter to specify how many groups
(excluding other) we want to keep:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_cat %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{relig =} \KeywordTok{fct_lump}\NormalTok{(relig, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{)) %>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(relig, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{print}\NormalTok{(}\DataTypeTok{n =} \OtherTok{Inf}\NormalTok{)}
\CommentTok{#> # A tibble: 10 × 2}
\CommentTok{#>                      relig     n}
\CommentTok{#>                     <fctr> <int>}
\CommentTok{#> 1               Protestant 10846}
\CommentTok{#> 2                 Catholic  5124}
\CommentTok{#> 3                     None  3523}
\CommentTok{#> 4                Christian   689}
\CommentTok{#> 5                    Other   458}
\CommentTok{#> 6                   Jewish   388}
\CommentTok{#> 7                 Buddhism   147}
\CommentTok{#> 8  Inter-nondenominational   109}
\CommentTok{#> 9             Moslem/islam   104}
\CommentTok{#> 10      Orthodox-christian    95}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How have the proportions of people identifying as Democrat,
  Republican, and Independent changed over time?
\item
  How could you collapse \texttt{rincome} into a small set of
  categories?
\end{enumerate}

\hypertarget{dates-and-times}{\chapter{Dates and
times}\label{dates-and-times}}

\section{Introduction}\label{introduction-10}

This chapter will show you how to work with dates and times in R. At
first glance, dates and times seem simple. You use them all the time in
your regular life, and they don't seem to cause much confusion. However,
the more you learn about dates and times, the more complicated they seem
to get. To warm up, try these three seemingly simple questions:

\begin{itemize}
\tightlist
\item
  Does every year have 365 days?
\item
  Does every day have 24 hours?
\item
  Does every minute have 60 seconds?
\end{itemize}

I'm sure you know that not every year has 365 days, but do you know the
full rule for determining if a year is a leap year? (It has three
parts.) You might have remembered that many parts of the world use
daylight savings time (DST), so that some days have 23 hours, and others
have 25. You might not have known that some minutes have 61 seconds
because every now and then leap seconds are added because the Earth's
rotation is gradually slowing down.

Dates and times are hard because they have to reconcile two physical
phenomena (the rotation of the Earth and its orbit around the sun) with
a whole raft of geopolitical phenomena including months, time zones, and
DST. This chapter won't teach you every last detail about dates and
times, but it will give you a solid grounding of practical skills that
will help you with common data analysis challenges.

\subsection{Prerequisites}\label{prerequisites-10}

This chapter will focus on the \textbf{lubridate} package, which makes
it easier to work with dates and times in R. lubridate is not part of
core tidyverse because you only need it when you're working with
dates/times. We will also need nycflights13 for practice data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{library}\NormalTok{(lubridate)}
\KeywordTok{library}\NormalTok{(nycflights13)}
\end{Highlighting}
\end{Shaded}

\section{Creating date/times}\label{creating-datetimes}

There are three types of date/time data that refer to an instant in
time:

\begin{itemize}
\item
  A \textbf{date}. Tibbles print this as
  \texttt{\textless{}date\textgreater{}}.
\item
  A \textbf{time} within a day. Tibbles print this as
  \texttt{\textless{}time\textgreater{}}.
\item
  A \textbf{date-time} is a date plus a time: it uniquely identifies an
  instant in time (typically to the nearest second). Tibbles print this
  as \texttt{\textless{}dttm\textgreater{}}. Elsewhere in R these are
  called POSIXct, but I don't think that's a very useful name.
\end{itemize}

In this chapter we are only going to focus on dates and date-times as R
doesn't have a native class for storing times. If you need one, you can
use the \textbf{hms} package.

You should always use the simplest possible data type that works for
your needs. That means if you can use a date instead of a date-time, you
should. Date-times are substantially more complicated because of the
need to handle time zones, which we'll come back to at the end of the
chapter.

To get the current date or date-time you can use \texttt{today()} or
\texttt{now()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{today}\NormalTok{()}
\CommentTok{#> [1] "2016-11-09"}
\KeywordTok{now}\NormalTok{()}
\CommentTok{#> [1] "2016-11-09 15:45:45 EST"}
\end{Highlighting}
\end{Shaded}

Otherwise, there are three ways you're likely to create a date/time:

\begin{itemize}
\tightlist
\item
  From a string.
\item
  From individual date-time components.
\item
  From an existing date/time object.
\end{itemize}

They work as follows.

\subsection{From strings}\label{from-strings}

Date/time data often comes as strings. You've seen one approach to
parsing strings into date-times in
\protect\hyperlink{readr-datetimes}{date-times}. Another approach is to
use the helpers provided by lubridate. They automatically work out the
format once you specify the order of the component. To use them,
identify the order in which year, month, and day appear in your dates,
then arrange ``y'', ``m'', and ``d'' in the same order. That gives you
the name of the lubridate function that will parse your date. For
example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\StringTok{"2017-01-31"}\NormalTok{)}
\CommentTok{#> [1] "2017-01-31"}
\KeywordTok{mdy}\NormalTok{(}\StringTok{"January 31st, 2017"}\NormalTok{)}
\CommentTok{#> [1] "2017-01-31"}
\KeywordTok{dmy}\NormalTok{(}\StringTok{"31-Jan-2017"}\NormalTok{)}
\CommentTok{#> [1] "2017-01-31"}
\end{Highlighting}
\end{Shaded}

These functions also take unquoted numbers. This is the most concise way
to create a single date/time object, as you might need when filtering
date/time data. \texttt{ymd()} is short and unambiguous:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\DecValTok{20170131}\NormalTok{)}
\CommentTok{#> [1] "2017-01-31"}
\end{Highlighting}
\end{Shaded}

\texttt{ymd()} and friends create dates. To create a date-time, add an
underscore and one or more of ``h'', ``m'', and ``s'' to the name of the
parsing function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd_hms}\NormalTok{(}\StringTok{"2017-01-31 20:11:59"}\NormalTok{)}
\CommentTok{#> [1] "2017-01-31 20:11:59 UTC"}
\KeywordTok{mdy_hm}\NormalTok{(}\StringTok{"01/31/2017 08:01"}\NormalTok{)}
\CommentTok{#> [1] "2017-01-31 08:01:00 UTC"}
\end{Highlighting}
\end{Shaded}

You can also force the creation of a date-time from a date by supplying
a timezone:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\DecValTok{20170131}\NormalTok{, }\DataTypeTok{tz =} \StringTok{"UTC"}\NormalTok{)}
\CommentTok{#> [1] "2017-01-31 UTC"}
\end{Highlighting}
\end{Shaded}

\subsection{From individual
components}\label{from-individual-components}

Instead of a single string, sometimes you'll have the individual
components of the date-time spread across multiple columns. This is what
we have in the flights data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(year, month, day, hour, minute)}
\CommentTok{#> # A tibble: 336,776 × 5}
\CommentTok{#>    year month   day  hour minute}
\CommentTok{#>   <int> <int> <int> <dbl>  <dbl>}
\CommentTok{#> 1  2013     1     1     5     15}
\CommentTok{#> 2  2013     1     1     5     29}
\CommentTok{#> 3  2013     1     1     5     40}
\CommentTok{#> 4  2013     1     1     5     45}
\CommentTok{#> 5  2013     1     1     6      0}
\CommentTok{#> 6  2013     1     1     5     58}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}

To create a date/time from this sort of input, use \texttt{make\_date()}
for dates, or \texttt{make\_datetime()} for date-times:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(year, month, day, hour, minute) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{departure =} \KeywordTok{make_datetime}\NormalTok{(year, month, day, hour, minute))}
\CommentTok{#> # A tibble: 336,776 × 6}
\CommentTok{#>    year month   day  hour minute           departure}
\CommentTok{#>   <int> <int> <int> <dbl>  <dbl>              <dttm>}
\CommentTok{#> 1  2013     1     1     5     15 2013-01-01 05:15:00}
\CommentTok{#> 2  2013     1     1     5     29 2013-01-01 05:29:00}
\CommentTok{#> 3  2013     1     1     5     40 2013-01-01 05:40:00}
\CommentTok{#> 4  2013     1     1     5     45 2013-01-01 05:45:00}
\CommentTok{#> 5  2013     1     1     6      0 2013-01-01 06:00:00}
\CommentTok{#> 6  2013     1     1     5     58 2013-01-01 05:58:00}
\CommentTok{#> # ... with 3.368e+05 more rows}
\end{Highlighting}
\end{Shaded}

Let's do the same thing for each of the four time columns in
\texttt{flights}. The times are represented in a slightly odd format, so
we use modulus arithmetic to pull out the hour and minute components.
Once I've created the date-time variables, I focus in on the variables
we'll explore in the rest of the chapter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make_datetime_100 <-}\StringTok{ }\NormalTok{function(year, month, day, time) \{}
  \KeywordTok{make_datetime}\NormalTok{(year, month, day, time %/%}\StringTok{ }\DecValTok{100}\NormalTok{, time %%}\StringTok{ }\DecValTok{100}\NormalTok{)}
\NormalTok{\}}

\NormalTok{flights_dt <-}\StringTok{ }\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(!}\KeywordTok{is.na}\NormalTok{(dep_time), !}\KeywordTok{is.na}\NormalTok{(arr_time)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{dep_time =} \KeywordTok{make_datetime_100}\NormalTok{(year, month, day, dep_time),}
    \DataTypeTok{arr_time =} \KeywordTok{make_datetime_100}\NormalTok{(year, month, day, arr_time),}
    \DataTypeTok{sched_dep_time =} \KeywordTok{make_datetime_100}\NormalTok{(year, month, day, sched_dep_time),}
    \DataTypeTok{sched_arr_time =} \KeywordTok{make_datetime_100}\NormalTok{(year, month, day, sched_arr_time)}
  \NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(origin, dest, }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"delay"}\NormalTok{), }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"time"}\NormalTok{))}

\NormalTok{flights_dt}
\CommentTok{#> # A tibble: 328,063 × 9}
\CommentTok{#>   origin  dest dep_delay arr_delay            dep_time      sched_dep_time}
\CommentTok{#>    <chr> <chr>     <dbl>     <dbl>              <dttm>              <dttm>}
\CommentTok{#> 1    EWR   IAH         2        11 2013-01-01 05:17:00 2013-01-01 05:15:00}
\CommentTok{#> 2    LGA   IAH         4        20 2013-01-01 05:33:00 2013-01-01 05:29:00}
\CommentTok{#> 3    JFK   MIA         2        33 2013-01-01 05:42:00 2013-01-01 05:40:00}
\CommentTok{#> 4    JFK   BQN        -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00}
\CommentTok{#> 5    LGA   ATL        -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00}
\CommentTok{#> 6    EWR   ORD        -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00}
\CommentTok{#> # ... with 3.281e+05 more rows, and 3 more variables: arr_time <dttm>,}
\CommentTok{#> #   sched_arr_time <dttm>, air_time <dbl>}
\end{Highlighting}
\end{Shaded}

With this data, I can visualise the distribution of departure times
across the year:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(dep_time)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{86400}\NormalTok{) }\CommentTok{# 86400 seconds = 1 day}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-10-1} \end{center}

Or within a single day:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(dep_time <}\StringTok{ }\KeywordTok{ymd}\NormalTok{(}\DecValTok{20130102}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(dep_time)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{600}\NormalTok{) }\CommentTok{# 600 s = 10 minutes}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-11-1} \end{center}

Note that when you use date-times in a numeric context (like in a
histogram), 1 means 1 second, so a binwidth of 86400 means one day. For
dates, 1 means 1 day.

\subsection{From other types}\label{from-other-types}

You may want to switch between a date-time and a date. That's the job of
\texttt{as\_datetime()} and \texttt{as\_date()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as_datetime}\NormalTok{(}\KeywordTok{today}\NormalTok{())}
\CommentTok{#> [1] "2016-11-09 UTC"}
\KeywordTok{as_date}\NormalTok{(}\KeywordTok{now}\NormalTok{())}
\CommentTok{#> [1] "2016-11-09"}
\end{Highlighting}
\end{Shaded}

Sometimes you'll get date/times as numeric offsets from the ``Unix
Epoch'', 1970-01-01. If the offset is in seconds, use
\texttt{as\_datetime()}; if it's in days, use \texttt{as\_date()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as_datetime}\NormalTok{(}\DecValTok{60} \NormalTok{*}\StringTok{ }\DecValTok{60} \NormalTok{*}\StringTok{ }\DecValTok{10}\NormalTok{)}
\CommentTok{#> [1] "1970-01-01 10:00:00 UTC"}
\KeywordTok{as_date}\NormalTok{(}\DecValTok{365} \NormalTok{*}\StringTok{ }\DecValTok{10} \NormalTok{+}\StringTok{ }\DecValTok{2}\NormalTok{)}
\CommentTok{#> [1] "1980-01-01"}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What happens if you parse a string that contains invalid dates?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"2010-10-10"}\NormalTok{, }\StringTok{"bananas"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\item
  What does the \texttt{tzone} argument to \texttt{today()} do? Why is
  it important?
\item
  Use the appropriate lubridate function to parse each of the following
  dates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d1 <-}\StringTok{ "January 1, 2010"}
\NormalTok{d2 <-}\StringTok{ "2015-Mar-07"}
\NormalTok{d3 <-}\StringTok{ "06-Jun-2017"}
\NormalTok{d4 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"August 19 (2015)"}\NormalTok{, }\StringTok{"July 1 (2015)"}\NormalTok{)}
\NormalTok{d5 <-}\StringTok{ "12/30/14"} \CommentTok{# Dec 30, 2014}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{Date-time components}\label{date-time-components}

Now that you know how to get date-time data into R's date-time data
structures, let's explore what you can do with them. This section will
focus on the accessor functions that let you get and set individual
components. The next section will look at how arithmetic works with
date-times.

\subsection{Getting components}\label{getting-components}

You can pull out individual parts of the date with the accessor
functions \texttt{year()}, \texttt{month()}, \texttt{mday()} (day of the
month), \texttt{yday()} (day of the year), \texttt{wday()} (day of the
week), \texttt{hour()}, \texttt{minute()}, and \texttt{second()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datetime <-}\StringTok{ }\KeywordTok{ymd_hms}\NormalTok{(}\StringTok{"2016-07-08 12:34:56"}\NormalTok{)}

\KeywordTok{year}\NormalTok{(datetime)}
\CommentTok{#> [1] 2016}
\KeywordTok{month}\NormalTok{(datetime)}
\CommentTok{#> [1] 7}
\KeywordTok{mday}\NormalTok{(datetime)}
\CommentTok{#> [1] 8}

\KeywordTok{yday}\NormalTok{(datetime)}
\CommentTok{#> [1] 190}
\KeywordTok{wday}\NormalTok{(datetime)}
\CommentTok{#> [1] 6}
\end{Highlighting}
\end{Shaded}

For \texttt{month()} and \texttt{wday()} you can set
\texttt{label\ =\ TRUE} to return the abbreviated name of the month or
day of the week. Set \texttt{abbr\ =\ FALSE} to return the full name.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{month}\NormalTok{(datetime, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] Jul}
\CommentTok{#> 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec}
\KeywordTok{wday}\NormalTok{(datetime, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{abbr =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> [1] Friday}
\CommentTok{#> 7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday}
\end{Highlighting}
\end{Shaded}

We can use \texttt{wday()} to see that more flights depart during the
week than on the weekend:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{wday =} \KeywordTok{wday}\NormalTok{(dep_time, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{wday)) +}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-18-1} \end{center}

There's an interesting pattern if we look at the average departure delay
by minute within the hour. It looks like flights leaving in minutes
20-30 and 50-60 have much lower delays than the rest of the hour!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{minute =} \KeywordTok{minute}\NormalTok{(dep_time)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(minute) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{avg_delay =} \KeywordTok{mean}\NormalTok{(arr_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{()) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(minute, avg_delay)) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-19-1} \end{center}

Interestingly, if we look at the \emph{scheduled} departure time we
don't see such a strong pattern:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sched_dep <-}\StringTok{ }\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{minute =} \KeywordTok{minute}\NormalTok{(sched_dep_time)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(minute) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{avg_delay =} \KeywordTok{mean}\NormalTok{(arr_delay, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}

\KeywordTok{ggplot}\NormalTok{(sched_dep, }\KeywordTok{aes}\NormalTok{(minute, avg_delay)) +}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-20-1} \end{center}

So why do we see that pattern with the actual departure times? Well,
like much data collected by humans, there's a strong bias towards
flights leaving at ``nice'' departure times. Always be alert for this
sort of pattern whenever you work with data that involves human
judgement!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sched_dep, }\KeywordTok{aes}\NormalTok{(minute, n)) +}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-21-1} \end{center}

\subsection{Rounding}\label{rounding}

An alternative approach to plotting individual components is to round
the date to a nearby unit of time, with \texttt{floor\_date()},
\texttt{round\_date()}, and \texttt{ceiling\_date()}. Each function
takes a vector of dates to adjust and then the name of the unit round
down (floor), round up (ceiling), or round to. This, for example, allows
us to plot the number of flights per week:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(}\DataTypeTok{week =} \KeywordTok{floor_date}\NormalTok{(dep_time, }\StringTok{"week"}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(week, n)) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-22-1} \end{center}

Computing the difference between a rounded and unrounded date can be
particularly useful.

\subsection{Setting components}\label{setting-components}

You can also use each accessor function to set the components of a
date/time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(datetime <-}\StringTok{ }\KeywordTok{ymd_hms}\NormalTok{(}\StringTok{"2016-07-08 12:34:56"}\NormalTok{))}
\CommentTok{#> [1] "2016-07-08 12:34:56 UTC"}

\KeywordTok{year}\NormalTok{(datetime) <-}\StringTok{ }\DecValTok{2020}
\NormalTok{datetime}
\CommentTok{#> [1] "2020-07-08 12:34:56 UTC"}
\KeywordTok{month}\NormalTok{(datetime) <-}\StringTok{ }\DecValTok{01}
\NormalTok{datetime}
\CommentTok{#> [1] "2020-01-08 12:34:56 UTC"}
\KeywordTok{hour}\NormalTok{(datetime) <-}\StringTok{ }\KeywordTok{hour}\NormalTok{(datetime) +}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

Alternatively, rather than modifying in place, you can create a new
date-time with \texttt{update()}. This also allows you to set multiple
values at once.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{update}\NormalTok{(datetime, }\DataTypeTok{year =} \DecValTok{2020}\NormalTok{, }\DataTypeTok{month =} \DecValTok{2}\NormalTok{, }\DataTypeTok{mday =} \DecValTok{2}\NormalTok{, }\DataTypeTok{hour =} \DecValTok{2}\NormalTok{)}
\CommentTok{#> [1] "2020-02-02 02:34:56 UTC"}
\end{Highlighting}
\end{Shaded}

If values are too big, they will roll-over:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\StringTok{"2015-02-01"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{update}\NormalTok{(}\DataTypeTok{mday =} \DecValTok{30}\NormalTok{)}
\CommentTok{#> [1] "2015-03-02"}
\KeywordTok{ymd}\NormalTok{(}\StringTok{"2015-02-01"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{update}\NormalTok{(}\DataTypeTok{hour =} \DecValTok{400}\NormalTok{)}
\CommentTok{#> [1] "2015-02-17 16:00:00 UTC"}
\end{Highlighting}
\end{Shaded}

You can use \texttt{update()} to show the distribution of flights across
the course of the day for every day of the year:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{dep_hour =} \KeywordTok{update}\NormalTok{(dep_time, }\DataTypeTok{yday =} \DecValTok{1}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(dep_hour)) +}
\StringTok{    }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{300}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/datetimes_files/figure-latex/unnamed-chunk-26-1} \end{center}

Setting larger components of a date to a constant is a powerful
technique that allows you to explore patterns in the smaller components.

\subsection{Exercises}\label{exercises-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How does the distribution of flight times within a day change over the
  course of the year?
\item
  Compare \texttt{dep\_time}, \texttt{sched\_dep\_time} and
  \texttt{dep\_delay}. Are they consistent? Explain your findings.
\item
  Compare \texttt{air\_time} with the duration between the departure and
  arrival. Explain your findings. (Hint: consider the location of the
  airport.)
\item
  How does the average delay time change over the course of a day?
  Should you use \texttt{dep\_time} or \texttt{sched\_dep\_time}? Why?
\item
  On what day of the week should you leave if you want to minimise the
  chance of a delay?
\item
  What makes the distribution of \texttt{diamonds\$carat} and
  \texttt{flights\$sched\_dep\_time} similar?
\item
  Confirm my hypothesis that the early departures of flights in minutes
  20-30 and 50-60 are caused by scheduled flights that leave early.
  Hint: create a binary variable that tells you whether or not a flight
  was delayed.
\end{enumerate}

\section{Time spans}\label{time-spans}

Next you'll learn about how arithmetic with dates works, including
subtraction, addition, and division. Along the way, you'll learn about
three important classes that represent time spans:

\begin{itemize}
\tightlist
\item
  \textbf{durations}, which represent an exact number of seconds.
\item
  \textbf{periods}, which represent human units like weeks and months.
\item
  \textbf{intervals}, which represent a starting and ending point.
\end{itemize}

\subsection{Durations}\label{durations}

In R, when you subtract two dates, you get a difftime object:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# How old is Hadley?}
\NormalTok{h_age <-}\StringTok{ }\KeywordTok{today}\NormalTok{() -}\StringTok{ }\KeywordTok{ymd}\NormalTok{(}\DecValTok{19791014}\NormalTok{)}
\NormalTok{h_age}
\CommentTok{#> Time difference of 13541 days}
\end{Highlighting}
\end{Shaded}

A difftime class object records a time span of seconds, minutes, hours,
days, or weeks. This ambiguity can make difftimes a little painful to
work with, so lubridate provides an alternative which always uses
seconds: the \textbf{duration}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.duration}\NormalTok{(h_age)}
\CommentTok{#> [1] "1169942400s (~37.07 years)"}
\end{Highlighting}
\end{Shaded}

Durations come with a bunch of convenient constructors:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dseconds}\NormalTok{(}\DecValTok{15}\NormalTok{)}
\CommentTok{#> [1] "15s"}
\KeywordTok{dminutes}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{#> [1] "600s (~10 minutes)"}
\KeywordTok{dhours}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{24}\NormalTok{))}
\CommentTok{#> [1] "43200s (~12 hours)" "86400s (~1 days)"}
\KeywordTok{ddays}\NormalTok{(}\DecValTok{0}\NormalTok{:}\DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] "0s"                "86400s (~1 days)"  "172800s (~2 days)"}
\CommentTok{#> [4] "259200s (~3 days)" "345600s (~4 days)" "432000s (~5 days)"}
\KeywordTok{dweeks}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\CommentTok{#> [1] "1814400s (~3 weeks)"}
\KeywordTok{dyears}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "31536000s (~52.14 weeks)"}
\end{Highlighting}
\end{Shaded}

Durations always record the time span in seconds. Larger units are
created by converting minutes, hours, days, weeks, and years to seconds
at the standard rate (60 seconds in a minute, 60 minutes in an hour, 24
hours in day, 7 days in a week, 365 days in a year).

You can add and multiply durations:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \NormalTok{*}\StringTok{ }\KeywordTok{dyears}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "63072000s (~2 years)"}
\KeywordTok{dyears}\NormalTok{(}\DecValTok{1}\NormalTok{) +}\StringTok{ }\KeywordTok{dweeks}\NormalTok{(}\DecValTok{12}\NormalTok{) +}\StringTok{ }\KeywordTok{dhours}\NormalTok{(}\DecValTok{15}\NormalTok{)}
\CommentTok{#> [1] "38847600s (~1.23 years)"}
\end{Highlighting}
\end{Shaded}

You can add and subtract durations to and from days:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tomorrow <-}\StringTok{ }\KeywordTok{today}\NormalTok{() +}\StringTok{ }\KeywordTok{ddays}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{last_year <-}\StringTok{ }\KeywordTok{today}\NormalTok{() -}\StringTok{ }\KeywordTok{dyears}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

However, because durations represent an exact number of seconds,
sometimes you might get an unexpected result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one_pm <-}\StringTok{ }\KeywordTok{ymd_hms}\NormalTok{(}\StringTok{"2016-03-12 13:00:00"}\NormalTok{, }\DataTypeTok{tz =} \StringTok{"America/New_York"}\NormalTok{)}

\NormalTok{one_pm}
\CommentTok{#> [1] "2016-03-12 13:00:00 EST"}
\NormalTok{one_pm +}\StringTok{ }\KeywordTok{ddays}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "2016-03-13 14:00:00 EDT"}
\end{Highlighting}
\end{Shaded}

Why is one day after 1pm on March 12, 2pm on March 13?! If you look
carefully at the date you might also notice that the time zones have
changed. Because of DST, March 12 only has 23 hours, so if add a full
days worth of seconds we end up with a different time.

\subsection{Periods}\label{periods}

To solve this problem, lubridate provides \textbf{periods}. Periods are
time spans but don't have a fixed length in seconds, instead they work
with ``human'' times, like days and months. That allows them work in a
more intuitive way:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one_pm}
\CommentTok{#> [1] "2016-03-12 13:00:00 EST"}
\NormalTok{one_pm +}\StringTok{ }\KeywordTok{days}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "2016-03-13 13:00:00 EDT"}
\end{Highlighting}
\end{Shaded}

Like durations, periods can be created with a number of friendly
constructor functions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seconds}\NormalTok{(}\DecValTok{15}\NormalTok{)}
\CommentTok{#> [1] "15S"}
\KeywordTok{minutes}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{#> [1] "10M 0S"}
\KeywordTok{hours}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{24}\NormalTok{))}
\CommentTok{#> [1] "12H 0M 0S" "24H 0M 0S"}
\KeywordTok{days}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\CommentTok{#> [1] "7d 0H 0M 0S"}
\KeywordTok{months}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{6}\NormalTok{)}
\CommentTok{#> [1] "1m 0d 0H 0M 0S" "2m 0d 0H 0M 0S" "3m 0d 0H 0M 0S" "4m 0d 0H 0M 0S"}
\CommentTok{#> [5] "5m 0d 0H 0M 0S" "6m 0d 0H 0M 0S"}
\KeywordTok{weeks}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\CommentTok{#> [1] "21d 0H 0M 0S"}
\KeywordTok{years}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "1y 0m 0d 0H 0M 0S"}
\end{Highlighting}
\end{Shaded}

You can add and multiply periods:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10} \NormalTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{months}\NormalTok{(}\DecValTok{6}\NormalTok{) +}\StringTok{ }\KeywordTok{days}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\CommentTok{#> [1] "60m 10d 0H 0M 0S"}
\KeywordTok{days}\NormalTok{(}\DecValTok{50}\NormalTok{) +}\StringTok{ }\KeywordTok{hours}\NormalTok{(}\DecValTok{25}\NormalTok{) +}\StringTok{ }\KeywordTok{minutes}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\CommentTok{#> [1] "50d 25H 2M 0S"}
\end{Highlighting}
\end{Shaded}

And of course, add them to dates. Compared to durations, periods are
more likely to do what you expect:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# A leap year}
\KeywordTok{ymd}\NormalTok{(}\StringTok{"2016-01-01"}\NormalTok{) +}\StringTok{ }\KeywordTok{dyears}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "2016-12-31"}
\KeywordTok{ymd}\NormalTok{(}\StringTok{"2016-01-01"}\NormalTok{) +}\StringTok{ }\KeywordTok{years}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "2017-01-01"}

\CommentTok{# Daylight Savings Time}
\NormalTok{one_pm +}\StringTok{ }\KeywordTok{ddays}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "2016-03-13 14:00:00 EDT"}
\NormalTok{one_pm +}\StringTok{ }\KeywordTok{days}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "2016-03-13 13:00:00 EDT"}
\end{Highlighting}
\end{Shaded}

Let's use periods to fix an oddity related to our flight dates. Some
planes appear to have arrived at their destination \emph{before} they
departed from New York City.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(arr_time <}\StringTok{ }\NormalTok{dep_time) }
\CommentTok{#> # A tibble: 10,633 × 9}
\CommentTok{#>   origin  dest dep_delay arr_delay            dep_time      sched_dep_time}
\CommentTok{#>    <chr> <chr>     <dbl>     <dbl>              <dttm>              <dttm>}
\CommentTok{#> 1    EWR   BQN         9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00}
\CommentTok{#> 2    JFK   DFW        59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00}
\CommentTok{#> 3    EWR   TPA        -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00}
\CommentTok{#> 4    EWR   SJU        -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00}
\CommentTok{#> 5    EWR   SFO        11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00}
\CommentTok{#> 6    LGA   FLL       -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00}
\CommentTok{#> # ... with 1.063e+04 more rows, and 3 more variables: arr_time <dttm>,}
\CommentTok{#> #   sched_arr_time <dttm>, air_time <dbl>}
\end{Highlighting}
\end{Shaded}

These are overnight flights. We used the same date information for both
the departure and the arrival times, but these flights arrived on the
following day. We can fix this by adding \texttt{days(1)} to the arrival
time of each overnight flight.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt <-}\StringTok{ }\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{overnight =} \NormalTok{arr_time <}\StringTok{ }\NormalTok{dep_time,}
    \DataTypeTok{arr_time =} \NormalTok{arr_time +}\StringTok{ }\KeywordTok{days}\NormalTok{(overnight *}\StringTok{ }\DecValTok{1}\NormalTok{),}
    \DataTypeTok{sched_arr_time =} \NormalTok{sched_arr_time +}\StringTok{ }\KeywordTok{days}\NormalTok{(overnight *}\StringTok{ }\DecValTok{1}\NormalTok{)}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now all of our flights obey the laws of physics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights_dt %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(overnight, arr_time <}\StringTok{ }\NormalTok{dep_time) }
\CommentTok{#> # A tibble: 0 × 10}
\CommentTok{#> # ... with 10 variables: origin <chr>, dest <chr>, dep_delay <dbl>,}
\CommentTok{#> #   arr_delay <dbl>, dep_time <dttm>, sched_dep_time <dttm>,}
\CommentTok{#> #   arr_time <dttm>, sched_arr_time <dttm>, air_time <dbl>,}
\CommentTok{#> #   overnight <lgl>}
\end{Highlighting}
\end{Shaded}

\subsection{Intervals}\label{intervals}

It's obvious what \texttt{dyears(1)\ /\ ddays(365)} should return: one,
because durations are always represented by a number of seconds, and a
duration of a year is defined as 365 days worth of seconds.

What should \texttt{years(1)\ /\ days(1)} return? Well, if the year was
2015 it should return 365, but if it was 2016, it should return 366!
There's not quite enough information for lubridate to give a single
clear answer. What it does instead is give an estimate, with a warning:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{years}\NormalTok{(}\DecValTok{1}\NormalTok{) /}\StringTok{ }\KeywordTok{days}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> estimate only: convert to intervals for accuracy}
\CommentTok{#> [1] 365}
\end{Highlighting}
\end{Shaded}

If you want a more accurate measurement, you'll have to use an
\textbf{interval}. An interval is a duration with a starting point: that
makes it precise so you can determine exactly how long it is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{next_year <-}\StringTok{ }\KeywordTok{today}\NormalTok{() +}\StringTok{ }\KeywordTok{years}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{(}\KeywordTok{today}\NormalTok{() %--%}\StringTok{ }\NormalTok{next_year) /}\StringTok{ }\KeywordTok{ddays}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] 365}
\end{Highlighting}
\end{Shaded}

To find out how many periods fall into an interval, you need to use
integer division:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\KeywordTok{today}\NormalTok{() %--%}\StringTok{ }\NormalTok{next_year) %/%}\StringTok{ }\KeywordTok{days}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> Note: method with signature 'Timespan#Timespan' chosen for function '%/%',}
\CommentTok{#>  target signature 'Interval#Period'.}
\CommentTok{#>  "Interval#ANY", "ANY#Period" would also be valid}
\CommentTok{#> [1] 365}
\end{Highlighting}
\end{Shaded}

\subsection{Summary}\label{summary-1}

How do you pick between duration, periods, and intervals? As always,
pick the simplest data structure that solves your problem. If you only
care about physical time, use a duration; if you need to add human
times, use a period; if you need to figure out how long a span is in
human units, use an interval.

Figure \ref{fig:dt-algebra} summarises permitted arithmetic operations
between the different data types.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{diagrams/datetimes-arithmetic} 

}

\caption{The allowed arithmetic operations between pairs of date/time classes.}\label{fig:dt-algebra}
\end{figure}

\subsection{Exercises}\label{exercises-47}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is there \texttt{months()} but no \texttt{dmonths()}?
\item
  Explain \texttt{days(overnight\ *\ 1)} to someone who has just started
  learning R. How does it work?
\item
  Create a vector of dates giving the first day of every month in 2015.
  Create a vector of dates giving the first day of every month in the
  \emph{current} year.
\item
  Write a function that given your birthday (as a date), returns how old
  you are in years.
\item
  Why can't
  \texttt{(today()\ \%-\/-\%\ (today()\ +\ years(1))\ /\ months(1)}
  work?
\end{enumerate}

\hypertarget{time-zones}{\section{Time zones}\label{time-zones}}

Time zones are an enormously complicated topic because of their
interaction with geopolitical entities. Fortunately we don't need to dig
into all the details as they're not all important for data analysis, but
there are a few challenges we'll need to tackle head on.

The first challenge is that everyday names of time zones tend to be
ambiguous. For example, if you're American you're probably familiar with
EST, or Eastern Standard Time. However, both Australia and Canada also
have EST! To avoid confusion, R uses the international standard IANA
time zones. These use a consistent naming scheme ``/'', typically in the
form
``\textless{}continent\textgreater{}/\textless{}city\textgreater{}''
(there are a few exceptions because not every country lies on a
continent). Examples include ``America/New\_York'', ``Europe/Paris'',
and ``Pacific/Auckland''.

You might wonder why the time zone uses a city, when typically you think
of time zones as associated with a country or region within a country.
This is because the IANA database has to record decades worth of time
zone rules. In the course of decades, countries change names (or break
apart) fairly frequently, but city names tend to stay the same. Another
problem is that name needs to reflect not only to the current behaviour,
but also the complete history. For example, there are time zones for
both ``America/New\_York'' and ``America/Detroit''. These cities both
currently use Eastern Standard Time but in 1969-1972 Michigan (the state
in which Detroit is located), did not follow DST, so it needs a
different name. It's worth reading the raw time zone database (available
at \url{http://www.iana.org/time-zones}) just to read some of these
stories!

You can find out what R thinks your current time zone is with
\texttt{Sys.timezone()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{Sys.timezone}\NormalTok{()}
\CommentTok{#> [1] "America/New_York"}
\end{Highlighting}
\end{Shaded}

(If R doesn't know, you'll get an \texttt{NA}.)

And see the complete list of all time zone names with
\texttt{OlsonNames()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(}\KeywordTok{OlsonNames}\NormalTok{())}
\CommentTok{#> [1] 589}
\KeywordTok{head}\NormalTok{(}\KeywordTok{OlsonNames}\NormalTok{())}
\CommentTok{#> [1] "Africa/Abidjan"     "Africa/Accra"       "Africa/Addis_Ababa"}
\CommentTok{#> [4] "Africa/Algiers"     "Africa/Asmara"      "Africa/Asmera"}
\end{Highlighting}
\end{Shaded}

In R, the time zone is an attribute of the date-time that only controls
printing. For example, these three objects represent the same instant in
time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x1 <-}\StringTok{ }\KeywordTok{ymd_hms}\NormalTok{(}\StringTok{"2015-06-01 12:00:00"}\NormalTok{, }\DataTypeTok{tz =} \StringTok{"America/New_York"}\NormalTok{))}
\CommentTok{#> [1] "2015-06-01 12:00:00 EDT"}
\NormalTok{(x2 <-}\StringTok{ }\KeywordTok{ymd_hms}\NormalTok{(}\StringTok{"2015-06-01 18:00:00"}\NormalTok{, }\DataTypeTok{tz =} \StringTok{"Europe/Copenhagen"}\NormalTok{))}
\CommentTok{#> [1] "2015-06-01 18:00:00 CEST"}
\NormalTok{(x3 <-}\StringTok{ }\KeywordTok{ymd_hms}\NormalTok{(}\StringTok{"2015-06-02 04:00:00"}\NormalTok{, }\DataTypeTok{tz =} \StringTok{"Pacific/Auckland"}\NormalTok{))}
\CommentTok{#> [1] "2015-06-02 04:00:00 NZST"}
\end{Highlighting}
\end{Shaded}

You can verify that they're the same time using subtraction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 -}\StringTok{ }\NormalTok{x2}
\CommentTok{#> Time difference of 0 secs}
\NormalTok{x1 -}\StringTok{ }\NormalTok{x3}
\CommentTok{#> Time difference of 0 secs}
\end{Highlighting}
\end{Shaded}

Unless other specified, lubridate always uses UTC. UTC (Coordinated
Universal Time) is the standard time zone used by the scientific
community and roughly equivalent to its predecessor GMT (Greenwich Mean
Time). It does not have DST, which makes a convenient representation for
computation. Operations that combine date-times, like \texttt{c()}, will
often drop the time zone. In that case, the date-times will display in
your local time zone:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x4 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(x1, x2, x3)}
\NormalTok{x4}
\CommentTok{#> [1] "2015-06-01 12:00:00 EDT" "2015-06-01 12:00:00 EDT"}
\CommentTok{#> [3] "2015-06-01 12:00:00 EDT"}
\end{Highlighting}
\end{Shaded}

You can change the time zone in two ways:

\begin{itemize}
\item
  Keep the instant in time the same, and change how it's displayed. Use
  this when the instant is correct, but you want a more natural display.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x4a <-}\StringTok{ }\KeywordTok{with_tz}\NormalTok{(x4, }\DataTypeTok{tzone =} \StringTok{"Australia/Lord_Howe"}\NormalTok{)}
\NormalTok{x4a}
\CommentTok{#> [1] "2015-06-02 02:30:00 LHST" "2015-06-02 02:30:00 LHST"}
\CommentTok{#> [3] "2015-06-02 02:30:00 LHST"}
\NormalTok{x4a -}\StringTok{ }\NormalTok{x4}
\CommentTok{#> Time differences in secs}
\CommentTok{#> [1] 0 0 0}
\end{Highlighting}
\end{Shaded}

  (This also illustrates another challenge of times zones: they're not
  all integer hour offsets!)
\item
  Change the underlying instant in time. Use this when you have an
  instant that has been labelled with the incorrect time zone, and you
  need to fix it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x4b <-}\StringTok{ }\KeywordTok{force_tz}\NormalTok{(x4, }\DataTypeTok{tzone =} \StringTok{"Australia/Lord_Howe"}\NormalTok{)}
\NormalTok{x4b}
\CommentTok{#> [1] "2015-06-01 12:00:00 LHST" "2015-06-01 12:00:00 LHST"}
\CommentTok{#> [3] "2015-06-01 12:00:00 LHST"}
\NormalTok{x4b -}\StringTok{ }\NormalTok{x4}
\CommentTok{#> Time differences in hours}
\CommentTok{#> [1] -14.5 -14.5 -14.5}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\part{Program}\label{part-program}


\chapter{Introduction}\label{program-intro}

In this part of the book, you'll improve your programming skills.
Programming is a cross-cutting skill needed for all data science work:
you must use a computer to do data science; you cannot do it in your
head, or with pencil and paper.

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/data-science-program} \end{center}

Programming produces code, and code is a tool of communication.
Obviously code tells the computer what you want it to do. But it also
communicates meaning to other humans. Thinking about code as a vehicle
for communication is important because every project you do is
fundamentally collaborative. Even if you're not working with other
people, you'll definitely be working with future-you! Writing clear code
is important so that others (like future-you) can understand why you
tackled an analysis in the way you did. That means getting better at
programming also involves getting better at communicating. Over time,
you want your code to become not just easier to write, but easier for
others to read.

Writing code is similar in many ways to writing prose. One parallel
which I find particularly useful is that in both cases rewriting is the
key to clarity. The first expression of your ideas is unlikely to be
particularly clear, and you may need to rewrite multiple times. After
solving a data analysis challenge, it's often worth looking at your code
and thinking about whether or not it's obvious what you've done. If you
spend a little time rewriting your code while the ideas are fresh, you
can save a lot of time later trying to recreate what your code did. But
this doesn't mean you should rewrite every function: you need to balance
what you need to achieve now with saving time in the long run. (But the
more you rewrite your functions the more likely your first attempt will
be clear.)

In the following four chapters, you'll learn skills that will allow you
to both tackle new programs and to solve existing problems with greater
clarity and ease:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In \protect\hyperlink{pipes}{pipes}, you will dive deep into the
  \textbf{pipe}, \texttt{\%\textgreater{}\%}, and learn more about how
  it works, what the alternatives are, and when not to use it.
\item
  Copy-and-paste is a powerful tool, but you should avoid doing it more
  than twice. Repeating yourself in code is dangerous because it can
  easily lead to errors and inconsistencies. Instead, in
  \protect\hyperlink{functions}{functions}, you'll learn how to write
  \textbf{functions} which let you extract out repeated code so that it
  can be easily reused.
\item
  As you start to write more powerful functions, you'll need a solid
  grounding in R's \textbf{data structures}, provided by
  \protect\hyperlink{vectors}{vectors}. You must master the four common
  atomic vectors, the three important S3 classes built on top of them,
  and understand the mysteries of the list and data frame.
\item
  Functions extract out repeated code, but you often need to repeat the
  same actions on different inputs. You need tools for
  \textbf{iteration} that let you do similar things again and again.
  These tools include for loops and functional programming, which you'll
  learn about in \protect\hyperlink{iteration}{iteration}.
\end{enumerate}

\section{Learning more}\label{learning-more-1}

The goal of these chapters is to teach you the minimum about programming
that you need to practice data science, which turns out to be a
reasonable amount. Once you have mastered the material in this book, I
strongly believe you should invest further in your programming skills.
Learning more about programming is a long-term investment: it won't pay
off immediately, but in the long term it will allow you to solve new
problems more quickly, and let you reuse your insights from previous
problems in new scenarios.

To learn more you need to study R as a programming language, not just an
interactive environment for data science. We have written two books that
will help you do so:

\begin{itemize}
\item
  \href{https://amzn.com/1449359019}{\emph{Hands on Programming with
  R}}, by Garrett Grolemund. This is an introduction to R as a
  programming language and is a great place to start if R is your first
  programming language. It covers similar material to these chapters,
  but with a different style and different motivation examples (based in
  the casino). It's a useful complement if you find that these four
  chapters go by too quickly.
\item
  \href{https://amzn.com/1466586966}{\emph{Advanced R}} by Hadley
  Wickham. This dives into the details of R the programming language.
  This is a great place to start if you have existing programming
  experience. It's also a great next step once you've internalised the
  ideas in these chapters. You can read it online at
  \url{http://adv-r.had.co.nz}.
\end{itemize}

\hypertarget{pipes}{\chapter{Pipes}\label{pipes}}

\section{Introduction}\label{introduction-11}

Pipes are a powerful tool for clearly expressing a sequence of multiple
operations. So far, you've been using them without knowing how they
work, or what the alternatives are. Now, in this chapter, it's time to
explore the pipe in more detail. You'll learn the alternatives to the
pipe, when you shouldn't use the pipe, and some useful related tools.

\subsection{Prerequisites}\label{prerequisites-11}

The pipe, \texttt{\%\textgreater{}\%}, comes from the \textbf{magrittr}
package by Stefan Milton Bache. Packages in the tidyverse load
\texttt{\%\textgreater{}\%} for you automatically, so you don't usually
load magrittr explicitly. Here, however, we're focussing on piping, and
we aren't loading any other packages, so we will load it explicitly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(magrittr)}
\end{Highlighting}
\end{Shaded}

\section{Piping alternatives}\label{piping-alternatives}

The point of the pipe is to help you write code in a way that easier to
read and understand. To see why the pipe is so useful, we're going to
explore a number of ways of writing the same code. Let's use code to
tell a story about a little bunny named Foo Foo:

\begin{quote}
Little bunny Foo Foo\\
Went hopping through the forest\\
Scooping up the field mice\\
And bopping them on the head
\end{quote}

This is a popular Children's poem that is accompanied by hand actions.

We'll start by defining an object to represent little bunny Foo Foo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo_foo <-}\StringTok{ }\KeywordTok{little_bunny}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

And we'll use a function for each key verb: \texttt{hop()},
\texttt{scoop()}, and \texttt{bop()}. Using this object and these verbs,
there are (at least) four ways we could retell the story in code:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Save each intermediate step as a new object.
\item
  Overwrite the original object many times.
\item
  Compose functions.
\item
  Use the pipe.
\end{enumerate}

We'll work through each approach, showing you the code and talking about
the advantages and disadvantages.

\subsection{Intermediate steps}\label{intermediate-steps}

The simplest approach is to save each step as a new object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo_foo_1 <-}\StringTok{ }\KeywordTok{hop}\NormalTok{(foo_foo, }\DataTypeTok{through =} \NormalTok{forest)}
\NormalTok{foo_foo_2 <-}\StringTok{ }\KeywordTok{scoop}\NormalTok{(foo_foo_1, }\DataTypeTok{up =} \NormalTok{field_mice)}
\NormalTok{foo_foo_3 <-}\StringTok{ }\KeywordTok{bop}\NormalTok{(foo_foo_2, }\DataTypeTok{on =} \NormalTok{head)}
\end{Highlighting}
\end{Shaded}

The main downside of this form is that it forces you to name each
intermediate element. If there are natural names, this is a good idea,
and you should do it. But many times, like this in this example, there
aren't natural names, and you add numeric suffixes to make the names
unique. That leads to two problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The code is cluttered with unimportant names
\item
  You have to carefully increment the suffix on each line.
\end{enumerate}

Whenever I write code like this, I invariably use the wrong number on
one line and then spend 10 minutes scratching my head and trying to
figure out what went wrong with my code.

You may also worry that this form creates many copies of your data and
takes up a lot of memory. Surprisingly, that's not the case. First, note
that proactively worrying about memory is not a useful way to spend your
time: worry about it when it becomes a problem (i.e.~you run out of
memory), not before. Second, R isn't stupid, and it will share columns
across data frames, where possible. Let's take a look at an actual data
manipulation pipeline where we add a new column to
\texttt{ggplot2::diamonds}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds <-}\StringTok{ }\NormalTok{ggplot2::diamonds}
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{price_per_carat =} \NormalTok{price /}\StringTok{ }\NormalTok{carat)}

\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(diamonds)}
\CommentTok{#> 3.46 MB}
\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(diamonds2)}
\CommentTok{#> 3.89 MB}
\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(diamonds, diamonds2)}
\CommentTok{#> 3.89 MB}
\end{Highlighting}
\end{Shaded}

\texttt{pryr::object\_size()} gives the memory occupied by all of its
arguments. The results seem counterintuitive at first:

\begin{itemize}
\tightlist
\item
  \texttt{diamonds} takes up 3.46 MB,
\item
  \texttt{diamonds2} takes up 3.89 MB,
\item
  \texttt{diamonds} and \texttt{diamonds2} together take up 3.89 MB!
\end{itemize}

How can that work? Well, \texttt{diamonds2} has 10 columns in common
with \texttt{diamonds}: there's no need to duplicate all that data, so
the two data frames have variables in common. These variables will only
get copied if you modify one of them. In the following example, we
modify a single value in \texttt{diamonds\$carat}. That means the
\texttt{carat} variable can no longer be shared between the two data
frames, and a copy must be made. The size of each data frame is
unchanged, but the collective size increases:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds$carat[}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(diamonds)}
\CommentTok{#> 3.46 MB}
\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(diamonds2)}
\CommentTok{#> 3.89 MB}
\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(diamonds, diamonds2)}
\CommentTok{#> 4.32 MB}
\end{Highlighting}
\end{Shaded}

(Note that we use \texttt{pryr::object\_size()} here, not the built-in
\texttt{object.size()}. \texttt{object.size()} only takes a single
object so it can't compute how data is shared across multiple objects.)

\subsection{Overwrite the original}\label{overwrite-the-original}

Instead of creating intermediate objects at each step, we could
overwrite the original object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo_foo <-}\StringTok{ }\KeywordTok{hop}\NormalTok{(foo_foo, }\DataTypeTok{through =} \NormalTok{forest)}
\NormalTok{foo_foo <-}\StringTok{ }\KeywordTok{scoop}\NormalTok{(foo_foo, }\DataTypeTok{up =} \NormalTok{field_mice)}
\NormalTok{foo_foo <-}\StringTok{ }\KeywordTok{bop}\NormalTok{(foo_foo, }\DataTypeTok{on =} \NormalTok{head)}
\end{Highlighting}
\end{Shaded}

This is less typing (and less thinking), so you're less likely to make
mistakes. However, there are two problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Debugging is painful: if you make a mistake you'll need to re-run the
  complete pipeline from the beginning.
\item
  The repetition of the object being transformed (we've written
  \texttt{foo\_foo} six times!) obscures what's changing on each line.
\end{enumerate}

\subsection{Function composition}\label{function-composition}

Another approach is to abandon assignment and just string the function
calls together:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bop}\NormalTok{(}
  \KeywordTok{scoop}\NormalTok{(}
    \KeywordTok{hop}\NormalTok{(foo_foo, }\DataTypeTok{through =} \NormalTok{forest),}
    \DataTypeTok{up =} \NormalTok{field_mice}
  \NormalTok{), }
  \DataTypeTok{on =} \NormalTok{head}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here the disadvantage is that you have to read from inside-out, from
right-to-left, and that the arguments end up spread far apart
(evocatively called the
\href{https://en.wikipedia.org/wiki/Dagwood_sandwich}{dagwood sandwhich}
problem). In short, this code is hard for a human to consume.

\subsection{Use the pipe}\label{use-the-pipe}

Finally, we can use the pipe:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foo_foo %>%}
\StringTok{  }\KeywordTok{hop}\NormalTok{(}\DataTypeTok{through =} \NormalTok{forest) %>%}
\StringTok{  }\KeywordTok{scoop}\NormalTok{(}\DataTypeTok{up =} \NormalTok{field_mouse) %>%}
\StringTok{  }\KeywordTok{bop}\NormalTok{(}\DataTypeTok{on =} \NormalTok{head)}
\end{Highlighting}
\end{Shaded}

This is my favourite form, because it focusses on verbs, not nouns. You
can read this series of function compositions like it's a set of
imperative actions. Foo Foo hops, then scoops, then bops. The downside,
of course, is that you need to be familiar with the pipe. If you've
never seen \texttt{\%\textgreater{}\%} before, you'll have no idea what
this code does. Fortunately, most people pick up the idea very quickly,
so when you share you code with others who aren't familiar with the
pipe, you can easily teach them.

The pipe works by performing a ``lexical transformation'': behind the
scenes, magrittr reassembles the code in the pipe to a form that works
by overwriting an intermediate object. When you run a pipe like the one
above, magrittr does something like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_pipe <-}\StringTok{ }\NormalTok{function(.) \{}
  \NormalTok{. <-}\StringTok{ }\KeywordTok{hop}\NormalTok{(., }\DataTypeTok{through =} \NormalTok{forest)}
  \NormalTok{. <-}\StringTok{ }\KeywordTok{scoop}\NormalTok{(., }\DataTypeTok{up =} \NormalTok{field_mice)}
  \KeywordTok{bop}\NormalTok{(., }\DataTypeTok{on =} \NormalTok{head)}
\NormalTok{\}}
\KeywordTok{my_pipe}\NormalTok{(foo_foo)}
\end{Highlighting}
\end{Shaded}

This means that the pipe won't work for two classes of functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Functions that use the current environment. For example,
  \texttt{assign()} will create a new variable with the given name in
  the current environment:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{assign}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{x}
\CommentTok{#> [1] 10}

\StringTok{"x"} \NormalTok{%>%}\StringTok{ }\KeywordTok{assign}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{x}
\CommentTok{#> [1] 10}
\end{Highlighting}
\end{Shaded}

  The use of assign with the pipe does not work because it assigns it to
  a temporary environment used by \texttt{\%\textgreater{}\%}. If you do
  want to use assign with the pipe, you must be explicit about the
  environment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{env <-}\StringTok{ }\KeywordTok{environment}\NormalTok{()}
\StringTok{"x"} \NormalTok{%>%}\StringTok{ }\KeywordTok{assign}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DataTypeTok{envir =} \NormalTok{env)}
\NormalTok{x}
\CommentTok{#> [1] 100}
\end{Highlighting}
\end{Shaded}

  Other functions with this problem include \texttt{get()} and
  \texttt{load()}.
\item
  Functions that use lazy evaluation. In R, function arguments are only
  computed when the function uses them, not prior to calling the
  function. The pipe computes each element in turn, so you can't rely on
  this behaviour.

  One place that this is a problem is \texttt{tryCatch()}, which lets
  you capture and handle errors:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tryCatch}\NormalTok{(}\KeywordTok{stop}\NormalTok{(}\StringTok{"!"}\NormalTok{), }\DataTypeTok{error =} \NormalTok{function(e) }\StringTok{"An error"}\NormalTok{)}
\CommentTok{#> [1] "An error"}

\KeywordTok{stop}\NormalTok{(}\StringTok{"!"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{tryCatch}\NormalTok{(}\DataTypeTok{error =} \NormalTok{function(e) }\StringTok{"An error"}\NormalTok{)}
\CommentTok{#> Error in eval(expr, envir, enclos): !}
\end{Highlighting}
\end{Shaded}

  There are a relatively wide class of functions with this behaviour,
  including \texttt{try()}, \texttt{suppressMessages()}, and
  \texttt{suppressWarnings()} in base R.
\end{enumerate}

\section{When not to use the pipe}\label{when-not-to-use-the-pipe}

The pipe is a powerful tool, but it's not the only tool at your
disposal, and it doesn't solve every problem! Pipes are most useful for
rewriting a fairly short linear sequence of operations. I think you
should reach for another tool when:

\begin{itemize}
\item
  Your pipes are longer than (say) ten steps. In that case, create
  intermediate objects with meaningful names. That will make debugging
  easier, because you can more easily check the intermediate results,
  and it makes it easier to understand your code, because the variable
  names can help communicate intent.
\item
  You have multiple inputs or outputs. If there isn't one primary object
  being transformed, but two or more objects being combined together,
  don't use the pipe.
\item
  You are starting to think about a directed graph with a complex
  dependency structure. Pipes are fundamentally linear and expressing
  complex relationships with them will typically yield confusing code.
\end{itemize}

\section{Other tools from magrittr}\label{other-tools-from-magrittr}

All packages in the tidyverse automatically make
\texttt{\%\textgreater{}\%} available for you, so you don't normally
load magrittr explicitly. However, there are some other useful tools
inside magrittr that you might want to try out:

\begin{itemize}
\item
  When working with more complex pipes, it's sometimes useful to call a
  function for its side-effects. Maybe you want to print out the current
  object, or plot it, or save it to disk. Many times, such functions
  don't return anything, effectively terminating the pipe.

  To work around this problem, you can use the ``tee'' pipe.
  \texttt{\%T\textgreater{}\%} works like \texttt{\%\textgreater{}\%}
  except that it returns the left-hand side instead of the right-hand
  side. It's called ``tee'' because it's like a literal T-shaped pipe.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{plot}\NormalTok{() %>%}
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\CommentTok{#>  NULL}

\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{) %T>%}
\StringTok{  }\KeywordTok{plot}\NormalTok{() %>%}
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\CommentTok{#>  num [1:50, 1:2] -0.387 -0.785 -1.057 -0.796 -1.756 ...}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/pipes_files/figure-latex/unnamed-chunk-13-1} \includegraphics[width=0.7\linewidth]{_bookdown_files/pipes_files/figure-latex/unnamed-chunk-13-2} \end{center}
\item
  If you're working with functions that don't have a data frame based
  API\\
  (i.e.~you pass them individual vectors, not a data frame and
  expressions to be evaluated in the context of that data frame), you
  might find \texttt{\%\$\%} useful. It ``explodes'' out the variables
  in a data frame so that you can refer to them explicitly. This is
  useful when working with many functions in base R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars %$%}
\StringTok{  }\KeywordTok{cor}\NormalTok{(disp, mpg)}
\CommentTok{#> [1] -0.848}
\end{Highlighting}
\end{Shaded}
\item
  For assignment magrittr provides the
  \texttt{\%\textless{}\textgreater{}\%} operator which allows you to
  replace code like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars <-}\StringTok{ }\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{transform}\NormalTok{(}\DataTypeTok{cyl =} \NormalTok{cyl *}\StringTok{ }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  with

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars %<>%}\StringTok{ }\KeywordTok{transform}\NormalTok{(}\DataTypeTok{cyl =} \NormalTok{cyl *}\StringTok{ }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  I'm not a fan of this operator because I think assignment is such a
  special operation that it should always be clear when it's occurring.
  In my opinion, a little bit of duplication (i.e.~repeating the name of
  the object twice) is fine in return for making assignment more
  explicit.
\end{itemize}

\hypertarget{functions}{\chapter{Functions}\label{functions}}

\section{Introduction}\label{introduction-12}

One of the best ways to improve your reach as a data scientist is to
write functions. Functions allow you to automate common tasks in a more
powerful and general way than copy-and-pasting. Writing a function has
three big advantages over using copy-and-paste:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You can give a function an evocative name that makes your code easier
  to understand.
\item
  As requirements change, you only need to update code in one place,
  instead of many.
\item
  You eliminate the chance of making incidental mistakes when you copy
  and paste (i.e.~updating a variable name in one place, but not in
  another).
\end{enumerate}

Writing good functions is a lifetime journey. Even after using R for
many years I still learn new techniques and better ways of approaching
old problems. The goal of this chapter is not to teach you every
esoteric detail of functions but to get you started with some pragmatic
advice that you can apply immediately.

As well as practical advice for writing functions, this chapter also
gives you some suggestions for how to style your code. Good code style
is like correct punctuation. Youcanmanagewithoutit, but it sure makes
things easier to read! As with styles of punctuation, there are many
possible variations. Here we present the style we use in our code, but
the most important thing is to be consistent.

\subsection{Prerequisites}\label{prerequisites-12}

The focus of this chapter is on writing functions in base R, so you
won't need any extra packages.

\section{When should you write a
function?}\label{when-should-you-write-a-function}

You should consider writing a function whenever you've copied and pasted
a block of code more than twice (i.e.~you now have three copies of the
same code). For example, take a look at this code. What does it do?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\NormalTok{tibble::}\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{a =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{b =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{c =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{d =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{)}

\NormalTok{df$a <-}\StringTok{ }\NormalTok{(df$a -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$a, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) /}\StringTok{ }
\StringTok{  }\NormalTok{(}\KeywordTok{max}\NormalTok{(df$a, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$a, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{df$b <-}\StringTok{ }\NormalTok{(df$b -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$b, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) /}\StringTok{ }
\StringTok{  }\NormalTok{(}\KeywordTok{max}\NormalTok{(df$b, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$a, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{df$c <-}\StringTok{ }\NormalTok{(df$c -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$c, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) /}\StringTok{ }
\StringTok{  }\NormalTok{(}\KeywordTok{max}\NormalTok{(df$c, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$c, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{df$d <-}\StringTok{ }\NormalTok{(df$d -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$d, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) /}\StringTok{ }
\StringTok{  }\NormalTok{(}\KeywordTok{max}\NormalTok{(df$d, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$d, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

You might be able to puzzle out that this rescales each column to have a
range from 0 to 1. But did you spot the mistake? I made an error when
copying-and-pasting the code for \texttt{df\$b}: I forgot to change an
\texttt{a} to a \texttt{b}. Extracting repeated code out into a function
is a good idea because it prevents you from making this type of mistake.

To write a function you need to first analyse the code. How many inputs
does it have?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(df$a -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$a, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) /}
\StringTok{  }\NormalTok{(}\KeywordTok{max}\NormalTok{(df$a, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) -}\StringTok{ }\KeywordTok{min}\NormalTok{(df$a, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This code only has one input: \texttt{df\$a}. (If you're surprised that
\texttt{TRUE} is not an input, you can explore why in the exercise
below.) To make the inputs more clear, it's a good idea to rewrite the
code using temporary variables with general names. Here this code only
requires a single numeric vector, so I'll call it \texttt{x}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{df$a}
\NormalTok{(x -}\StringTok{ }\KeywordTok{min}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) /}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) -}\StringTok{ }\KeywordTok{min}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#>  [1] 0.289 0.751 0.000 0.678 0.853 1.000 0.172 0.611 0.612 0.601}
\end{Highlighting}
\end{Shaded}

There is some duplication in this code. We're computing the range of the
data three times, but it makes sense to do it in one step:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rng <-}\StringTok{ }\KeywordTok{range}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{(x -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{]) /}\StringTok{ }\NormalTok{(rng[}\DecValTok{2}\NormalTok{] -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{])}
\CommentTok{#>  [1] 0.289 0.751 0.000 0.678 0.853 1.000 0.172 0.611 0.612 0.601}
\end{Highlighting}
\end{Shaded}

Pulling out intermediate calculations into named variables is a good
practice because it makes it more clear what the code is doing. Now that
I've simplified the code, and checked that it still works, I can turn it
into a function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rescale01 <-}\StringTok{ }\NormalTok{function(x) \{}
  \NormalTok{rng <-}\StringTok{ }\KeywordTok{range}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
  \NormalTok{(x -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{]) /}\StringTok{ }\NormalTok{(rng[}\DecValTok{2}\NormalTok{] -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{])}
\NormalTok{\}}
\KeywordTok{rescale01}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\CommentTok{#> [1] 0.0 0.5 1.0}
\end{Highlighting}
\end{Shaded}

There are three key steps to creating a new function:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You need to pick a \textbf{name} for the function. Here I've used
  \texttt{rescale01} because this function rescales a vector to lie
  between 0 and 1.
\item
  You list the inputs, or \textbf{arguments}, to the function inside
  \texttt{function}. Here we have just one argument. If we had more the
  call would look like \texttt{function(x,\ y,\ z)}.
\item
  You place the code you have developed in \textbf{body} of the
  function, a \texttt{\{} block that immediately follows
  \texttt{function(...)}.
\end{enumerate}

Note the overall process: I only made the function after I'd figured out
how to make it work with a simple input. It's easier to start with
working code and turn it into a function; it's harder to create a
function and then try to make it work.

At this point it's a good idea to check your function with a few
different inputs:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rescale01}\NormalTok{(}\KeywordTok{c}\NormalTok{(-}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\CommentTok{#> [1] 0.0 0.5 1.0}
\KeywordTok{rescale01}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\CommentTok{#> [1] 0.00 0.25 0.50   NA 1.00}
\end{Highlighting}
\end{Shaded}

As you write more and more functions you'll eventually want to convert
these informal, interactive tests into formal, automated tests. That
process is called unit testing. Unfortunately, it's beyond the scope of
this book, but you can learn about it in
\url{http://r-pkgs.had.co.nz/tests.html}.

We can simplify the original example now that we have a function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df$a <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$a)}
\NormalTok{df$b <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$b)}
\NormalTok{df$c <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$c)}
\NormalTok{df$d <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$d)}
\end{Highlighting}
\end{Shaded}

Compared to the original, this code is easier to understand and we've
eliminated one class of copy-and-paste errors. There is still quite a
bit of duplication since we're doing the same thing to multiple columns.
We'll learn how to eliminate that duplication in
\protect\hyperlink{iteration}{iteration}, once you've learned more about
R's data structures in \protect\hyperlink{vectors}{vectors}.

Another advantage of functions is that if our requirements change, we
only need to make the change in one place. For example, we might
discover that some of our variables include infinite values, and
\texttt{rescale01()} fails:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{, }\OtherTok{Inf}\NormalTok{)}
\KeywordTok{rescale01}\NormalTok{(x)}
\CommentTok{#>  [1]   0   0   0   0   0   0   0   0   0   0 NaN}
\end{Highlighting}
\end{Shaded}

Because we've extracted the code into a function, we only need to make
the fix in one place:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rescale01 <-}\StringTok{ }\NormalTok{function(x) \{}
  \NormalTok{rng <-}\StringTok{ }\KeywordTok{range}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{finite =} \OtherTok{TRUE}\NormalTok{)}
  \NormalTok{(x -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{]) /}\StringTok{ }\NormalTok{(rng[}\DecValTok{2}\NormalTok{] -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{])}
\NormalTok{\}}
\KeywordTok{rescale01}\NormalTok{(x)}
\CommentTok{#>  [1] 0.000 0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000   Inf}
\end{Highlighting}
\end{Shaded}

This is an important part of the ``do not repeat yourself'' (or DRY)
principle. The more repetition you have in your code, the more places
you need to remember to update when things change (and they always do!),
and the more likely you are to create bugs over time.

\subsection{Practice}\label{practice-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is \texttt{TRUE} not a parameter to \texttt{rescale01()}? What
  would happen if \texttt{x} contained a single missing value, and
  \texttt{na.rm} was \texttt{FALSE}?
\item
  In the second variant of \texttt{rescale01()}, infinite values are
  left unchanged. Rewrite \texttt{rescale01()} so that \texttt{-Inf} is
  mapped to 0, and \texttt{Inf} is mapped to 1.
\item
  Practice turning the following code snippets into functions. Think
  about what each function does. What would you call it? How many
  arguments does it need? Can you rewrite it to be more expressive or
  less duplicative?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x))}

\NormalTok{x /}\StringTok{ }\KeywordTok{sum}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{sd}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) /}\StringTok{ }\KeywordTok{mean}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  Follow \url{http://nicercode.github.io/intro/writing-functions.html}
  to write your own functions to compute the variance and skew of a
  numeric vector.
\item
  Write \texttt{both\_na()}, a function that takes two vectors of the
  same length and returns the number of positions that have an
  \texttt{NA} in both vectors.
\item
  What do the following functions do? Why are they useful even though
  they are so short?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{is_directory <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{file.info}\NormalTok{(x)$isdir}
\NormalTok{is_readable <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{file.access}\NormalTok{(x, }\DecValTok{4}\NormalTok{) ==}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}
\item
  Read the
  \href{https://en.wikipedia.org/wiki/Little_Bunny_Foo_Foo}{complete
  lyrics} to ``Little Bunny Foo Foo''. There's a lot of duplication in
  this song. Extend the initial piping example to recreate the complete
  song, and use functions to reduce the duplication.
\end{enumerate}

\section{Functions are for humans and
computers}\label{functions-are-for-humans-and-computers}

It's important to remember that functions are not just for the computer,
but are also for humans. R doesn't care what your function is called, or
what comments it contains, but these are important for human readers.
This section discusses some things that you should bear in mind when
writing functions that humans can understand.

The name of a function is important. Ideally, the name of your function
will be short, but clearly evoke what the function does. That's hard!
But it's better to be clear than short, as RStudio's autocomplete makes
it easy to type long names.

Generally, function names should be verbs, and arguments should be
nouns. There are some exceptions: nouns are ok if the function computes
a very well known noun (i.e. \texttt{mean()} is better than
\texttt{compute\_mean()}), or accessing some property of an object (i.e.
\texttt{coef()} is better than \texttt{get\_coefficients()}). A good
sign that a noun might be a better choice is if you're using a very
broad verb like ``get'', ``compute'', ``calculate'', or ``determine''.
Use your best judgement and don't be afraid to rename a function if you
figure out a better name later.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Too short}
\KeywordTok{f}\NormalTok{()}

\CommentTok{# Not a verb, or descriptive}
\KeywordTok{my_awesome_function}\NormalTok{()}

\CommentTok{# Long, but clear}
\KeywordTok{impute_missing}\NormalTok{()}
\KeywordTok{collapse_years}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

If your function name is composed of multiple words, I recommend using
``snake\_case'', where each lowercase word is separated by an
underscore. camelCase is a popular alternative. It doesn't really matter
which one you pick, the important thing is to be consistent: pick one or
the other and stick with it. R itself is not very consistent, but
there's nothing you can do about that. Make sure you don't fall into the
same trap by making your code as consistent as possible.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Never do this!}
\NormalTok{col_mins <-}\StringTok{ }\NormalTok{function(x, y) \{\}}
\NormalTok{rowMaxes <-}\StringTok{ }\NormalTok{function(y, x) \{\}}
\end{Highlighting}
\end{Shaded}

If you have a family of functions that do similar things, make sure they
have consistent names and arguments. Use a common prefix to indicate
that they are connected. That's better than a common suffix because
autocomplete allows you to type the prefix and see all the members of
the family.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Good}
\KeywordTok{input_select}\NormalTok{()}
\KeywordTok{input_checkbox}\NormalTok{()}
\KeywordTok{input_text}\NormalTok{()}

\CommentTok{# Not so good}
\KeywordTok{select_input}\NormalTok{()}
\KeywordTok{checkbox_input}\NormalTok{()}
\KeywordTok{text_input}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

A good example of this design is the stringr package: if you don't
remember exactly which function you need, you can type \texttt{str\_}
and jog your memory.

Where possible, avoid overriding existing functions and variables. It's
impossible to do in general because so many good names are already taken
by other packages, but avoiding the most common names from base R will
avoid confusion.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Don't do this!}
\NormalTok{T <-}\StringTok{ }\OtherTok{FALSE}
\NormalTok{c <-}\StringTok{ }\DecValTok{10}
\NormalTok{mean <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{sum}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

Use comments, lines starting with \texttt{\#}, to explain the ``why'' of
your code. You generally should avoid comments that explain the ``what''
or the ``how''. If you can't understand what the code does from reading
it, you should think about how to rewrite it to be more clear. Do you
need to add some intermediate variables with useful names? Do you need
to break out a subcomponent of a large function so you can name it?
However, your code can never capture the reasoning behind your
decisions: why did you choose this approach instead of an alternative?
What else did you try that didn't work? It's a great idea to capture
that sort of thinking in a comment.

Another important use of comments is to break up your file into easily
readable chunks. Use long lines of \texttt{-} and \texttt{=} to make it
easy to spot the breaks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load data --------------------------------------}

\CommentTok{# Plot data --------------------------------------}
\end{Highlighting}
\end{Shaded}

RStudio provides a keyboard shortcut to create these headers (Cmd/Ctrl +
Shift + R), and will display them in the code navigation drop-down at
the bottom-left of the editor:

\begin{center}\includegraphics{screenshots/rstudio-nav} \end{center}

\subsection{Exercises}\label{exercises-48}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Read the source code for each of the following three functions, puzzle
  out what they do, and then brainstorm better names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f1 <-}\StringTok{ }\NormalTok{function(string, prefix) \{}
  \KeywordTok{substr}\NormalTok{(string, }\DecValTok{1}\NormalTok{, }\KeywordTok{nchar}\NormalTok{(prefix)) ==}\StringTok{ }\NormalTok{prefix}
\NormalTok{\}}
\NormalTok{f2 <-}\StringTok{ }\NormalTok{function(x) \{}
  \NormalTok{if (}\KeywordTok{length}\NormalTok{(x) <=}\StringTok{ }\DecValTok{1}\NormalTok{) }\KeywordTok{return}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}
  \NormalTok{x[-}\KeywordTok{length}\NormalTok{(x)]}
\NormalTok{\}}
\NormalTok{f3 <-}\StringTok{ }\NormalTok{function(x, y) \{}
  \KeywordTok{rep}\NormalTok{(y, }\DataTypeTok{length.out =} \KeywordTok{length}\NormalTok{(x))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\item
  Take a function that you've written recently and spend 5 minutes
  brainstorming a better name for it and its arguments.
\item
  Compare and contrast \texttt{rnorm()} and \texttt{MASS::mvrnorm()}.
  How could you make them more consistent?
\item
  Make a case for why \texttt{norm\_r()}, \texttt{norm\_d()} etc would
  be better than \texttt{rnorm()}, \texttt{dnorm()}. Make a case for the
  opposite.
\end{enumerate}

\hypertarget{conditional-execution}{\section{Conditional
execution}\label{conditional-execution}}

An \texttt{if} statement allows you to conditionally execute code. It
looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (condition) \{}
  \CommentTok{# code executed when condition is TRUE}
\NormalTok{\} else \{}
  \CommentTok{# code executed when condition is FALSE}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

To get help on \texttt{if} you need to surround it in backticks:
\texttt{?`if`}. The help isn't particularly helpful if you're not
already an experienced programmer, but at least you know how to get to
it!

Here's a simple function that uses an if statement. The goal of this
function is to return a logical vector describing whether or not each
element of a vector is named.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{has_name <-}\StringTok{ }\NormalTok{function(x) \{}
  \NormalTok{nms <-}\StringTok{ }\KeywordTok{names}\NormalTok{(x)}
  \NormalTok{if (}\KeywordTok{is.null}\NormalTok{(nms)) \{}
    \KeywordTok{rep}\NormalTok{(}\OtherTok{FALSE}\NormalTok{, }\KeywordTok{length}\NormalTok{(x))}
  \NormalTok{\} else \{}
    \NormalTok{!}\KeywordTok{is.na}\NormalTok{(nms) &}\StringTok{ }\NormalTok{nms !=}\StringTok{ ""}
  \NormalTok{\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This function takes advantage of the standard return rule: a function
returns the last value that it computed. Here that is either one of the
two branches of the \texttt{if} statement.

\subsection{Conditions}\label{conditions}

The \texttt{condition} must evaluate to either \texttt{TRUE} or
\texttt{FALSE}. If it's a vector, you'll get a warning message; if it's
an \texttt{NA}, you'll get an error. Watch out for these messages in
your own code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)) \{\}}
\CommentTok{#> Warning in if (c(TRUE, FALSE)) \{: the condition has length > 1 and only the}
\CommentTok{#> first element will be used}
\CommentTok{#> NULL}

\NormalTok{if (}\OtherTok{NA}\NormalTok{) \{\}}
\CommentTok{#> Error in if (NA) \{: missing value where TRUE/FALSE needed}
\end{Highlighting}
\end{Shaded}

You can use \texttt{\textbar{}\textbar{}} (or) and \texttt{\&\&} (and)
to combine multiple logical expressions. These operators are
``short-circuiting'': as soon as \texttt{\textbar{}\textbar{}} sees the
first \texttt{TRUE} it returns \texttt{TRUE} without computing anything
else. As soon as \texttt{\&\&} sees the first \texttt{FALSE} it returns
\texttt{FALSE}. You should never use \texttt{\textbar{}} or \texttt{\&}
in an \texttt{if} statement: these are vectorised operations that apply
to multiple values (that's why you use them in \texttt{filter()}). If
you do have a logical vector, you can use \texttt{any()} or
\texttt{all()} to collapse it to a single value.

Be careful when testing for equality. \texttt{==} is vectorised, which
means that it's easy to get more than one output. Either check the
length is already 1, collapse with \texttt{all()} or \texttt{any()}, or
use the non-vectorised \texttt{identical()}. \texttt{identical()} is
very strict: it always returns either a single \texttt{TRUE} or a single
\texttt{FALSE}, and doesn't coerce types. This means that you need to be
careful when comparing integers and doubles:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{identical}\NormalTok{(0L, }\DecValTok{0}\NormalTok{)}
\CommentTok{#> [1] FALSE}
\end{Highlighting}
\end{Shaded}

You also need to be wary of floating point numbers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{) ^}\StringTok{ }\DecValTok{2}
\NormalTok{x}
\CommentTok{#> [1] 2}
\NormalTok{x ==}\StringTok{ }\DecValTok{2}
\CommentTok{#> [1] FALSE}
\NormalTok{x -}\StringTok{ }\DecValTok{2}
\CommentTok{#> [1] 4.44e-16}
\end{Highlighting}
\end{Shaded}

Instead use \texttt{dplyr::near()} for comparisons, as described in
\protect\hyperlink{comparisons}{comparisons}.

And remember, \texttt{x\ ==\ NA} doesn't do anything useful!

\subsection{Multiple conditions}\label{multiple-conditions}

You can chain multiple if statements together:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (this) \{}
  \CommentTok{# do that}
\NormalTok{\} else if (that) \{}
  \CommentTok{# do something else}
\NormalTok{\} else \{}
  \CommentTok{# }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

But if you end up with a very long series of chained \texttt{if}
statements, you should consider rewriting. One useful technique is the
\texttt{switch()} function. It allows you to evaluate selected code
based on position or name.

\begin{verbatim}
#> function(x, y, op) {
#>   switch(op,
#>     plus = x + y,
#>     minus = x - y,
#>     times = x * y,
#>     divide = x / y,
#>     stop("Unknown op!")
#>   )
#> }
\end{verbatim}

Another useful function that can often eliminate long chains of
\texttt{if} statements is \texttt{cut()}. It's used to discretise
continuous variables.

\subsection{Code style}\label{code-style}

Both \texttt{if} and \texttt{function} should (almost) always be
followed by squiggly brackets (\texttt{\{\}}), and the contents should
be indented by two spaces. This makes it easier to see the hierarchy in
your code by skimming the left-hand margin.

An opening curly brace should never go on its own line and should always
be followed by a new line. A closing curly brace should always go on its
own line, unless it's followed by \texttt{else}. Always indent the code
inside curly braces.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Good}
\NormalTok{if (y <}\StringTok{ }\DecValTok{0} \NormalTok{&&}\StringTok{ }\NormalTok{debug) \{}
  \KeywordTok{message}\NormalTok{(}\StringTok{"Y is negative"}\NormalTok{)}
\NormalTok{\}}

\NormalTok{if (y ==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
  \KeywordTok{log}\NormalTok{(x)}
\NormalTok{\} else \{}
  \NormalTok{y ^}\StringTok{ }\NormalTok{x}
\NormalTok{\}}

\CommentTok{# Bad}
\NormalTok{if (y <}\StringTok{ }\DecValTok{0} \NormalTok{&&}\StringTok{ }\NormalTok{debug)}
\KeywordTok{message}\NormalTok{(}\StringTok{"Y is negative"}\NormalTok{)}

\NormalTok{if (y ==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
  \KeywordTok{log}\NormalTok{(x)}
\NormalTok{\} }
\NormalTok{else \{}
  \NormalTok{y ^}\StringTok{ }\NormalTok{x}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

It's ok to drop the curly braces if you have a very short \texttt{if}
statement that can fit on one line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\DecValTok{10}
\NormalTok{x <-}\StringTok{ }\NormalTok{if (y <}\StringTok{ }\DecValTok{20}\NormalTok{) }\StringTok{"Too low"} \NormalTok{else }\StringTok{"Too high"}
\end{Highlighting}
\end{Shaded}

I recommend this only for very brief \texttt{if} statements. Otherwise,
the full form is easier to read:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (y <}\StringTok{ }\DecValTok{20}\NormalTok{) \{}
  \NormalTok{x <-}\StringTok{ "Too low"} 
\NormalTok{\} else \{}
  \NormalTok{x <-}\StringTok{ "Too high"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-49}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What's the difference between \texttt{if} and \texttt{ifelse()}?
  Carefully read the help and construct three examples that illustrate
  the key differences.
\item
  Write a greeting function that says ``good morning'', ``good
  afternoon'', or ``good evening'', depending on the time of day. (Hint:
  use a time argument that defaults to \texttt{lubridate::now()}. That
  will make it easier to test your function.)
\item
  Implement a \texttt{fizzbuzz} function. It takes a single number as
  input. If the number is divisible by three, it returns ``fizz''. If
  it's divisible by five it returns ``buzz''. If it's divisible by three
  and five, it returns ``fizzbuzz''. Otherwise, it returns the number.
  Make sure you first write working code before you create the function.
\item
  How could you use \texttt{cut()} to simplify this set of nested
  if-else statements?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (temp <=}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
  \StringTok{"freezing"}
\NormalTok{\} else if (temp <=}\StringTok{ }\DecValTok{10}\NormalTok{) \{}
  \StringTok{"cold"}
\NormalTok{\} else if (temp <=}\StringTok{ }\DecValTok{20}\NormalTok{) \{}
  \StringTok{"cool"}
\NormalTok{\} else if (temp <=}\StringTok{ }\DecValTok{30}\NormalTok{) \{}
  \StringTok{"warm"}
\NormalTok{\} else \{}
  \StringTok{"hot"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

  How would you change the call to \texttt{cut()} if I'd used
  \texttt{\textless{}} instead of \texttt{\textless{}=}? What is the
  other chief advantage of \texttt{cut()} for this problem? (Hint: what
  happens if you have many values in \texttt{temp}?)
\item
  What happens if you use \texttt{switch()} with numeric values?
\item
  What does this \texttt{switch()} call do? What happens if \texttt{x}
  is ``e''?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{switch(x, }
  \DataTypeTok{a =} \NormalTok{,}
  \DataTypeTok{b =} \StringTok{"ab"}\NormalTok{,}
  \DataTypeTok{c =} \NormalTok{,}
  \DataTypeTok{d =} \StringTok{"cd"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  Experiment, then carefully read the documentation.
\end{enumerate}

\section{Function arguments}\label{function-arguments}

The arguments to a function typically fall into two broad sets: one set
supplies the \textbf{data} to compute on, and the other supplies
arguments that control the \textbf{details} of the computation. For
example:

\begin{itemize}
\item
  In \texttt{log()}, the data is \texttt{x}, and the detail is the
  \texttt{base} of the logarithm.
\item
  In \texttt{mean()}, the data is \texttt{x}, and the details are how
  much data to trim from the ends (\texttt{trim}) and how to handle
  missing values (\texttt{na.rm}).
\item
  In \texttt{t.test()}, the data are \texttt{x} and \texttt{y}, and the
  details of the test are \texttt{alternative}, \texttt{mu},
  \texttt{paired}, \texttt{var.equal}, and \texttt{conf.level}.
\item
  In \texttt{str\_c()} you can supply any number of strings to
  \texttt{...}, and the details of the concatenation are controlled by
  \texttt{sep} and \texttt{collapse}.
\end{itemize}

Generally, data arguments should come first. Detail arguments should go
on the end, and usually should have default values. You specify a
default value in the same way you call a function with a named argument:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute confidence interval around mean using normal approximation}
\NormalTok{mean_ci <-}\StringTok{ }\NormalTok{function(x, }\DataTypeTok{conf =} \FloatTok{0.95}\NormalTok{) \{}
  \NormalTok{se <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(x) /}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{length}\NormalTok{(x))}
  \NormalTok{alpha <-}\StringTok{ }\DecValTok{1} \NormalTok{-}\StringTok{ }\NormalTok{conf}
  \KeywordTok{mean}\NormalTok{(x) +}\StringTok{ }\NormalTok{se *}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\KeywordTok{c}\NormalTok{(alpha /}\StringTok{ }\DecValTok{2}\NormalTok{, }\DecValTok{1} \NormalTok{-}\StringTok{ }\NormalTok{alpha /}\StringTok{ }\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\KeywordTok{mean_ci}\NormalTok{(x)}
\CommentTok{#> [1] 0.498 0.610}
\KeywordTok{mean_ci}\NormalTok{(x, }\DataTypeTok{conf =} \FloatTok{0.99}\NormalTok{)}
\CommentTok{#> [1] 0.480 0.628}
\end{Highlighting}
\end{Shaded}

The default value should almost always be the most common value. The few
exceptions to this rule are to do with safety. For example, it makes
sense for \texttt{na.rm} to default to \texttt{FALSE} because missing
values are important. Even though \texttt{na.rm\ =\ TRUE} is what you
usually put in your code, it's a bad idea to silently ignore missing
values by default.

When you call a function, you typically omit the names of the data
arguments, because they are used so commonly. If you override the
default value of a detail argument, you should use the full name:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Good}
\KeywordTok{mean}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Bad}
\KeywordTok{mean}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{, , }\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(, }\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{, }\OtherTok{NA}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

You can refer to an argument by its unique prefix (e.g.
\texttt{mean(x,\ n\ =\ TRUE)}), but this is generally best avoided given
the possibilities for confusion.

Notice that when you call a function, you should place a space around
\texttt{=} in function calls, and always put a space after a comma, not
before (just like in regular English). Using whitespace makes it easier
to skim the function for the important components.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Good}
\NormalTok{average <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(feet /}\StringTok{ }\DecValTok{12} \NormalTok{+}\StringTok{ }\NormalTok{inches, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Bad}
\NormalTok{average<-}\KeywordTok{mean}\NormalTok{(feet/}\DecValTok{12}\NormalTok{+inches,}\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Choosing names}\label{choosing-names}

The names of the arguments are also important. R doesn't care, but the
readers of your code (including future-you!) will. Generally you should
prefer longer, more descriptive names, but there are a handful of very
common, very short names. It's worth memorising these:

\begin{itemize}
\tightlist
\item
  \texttt{x}, \texttt{y}, \texttt{z}: vectors.
\item
  \texttt{w}: a vector of weights.
\item
  \texttt{df}: a data frame.
\item
  \texttt{i}, \texttt{j}: numeric indices (typically rows and columns).
\item
  \texttt{n}: length, or number of rows.
\item
  \texttt{p}: number of columns.
\end{itemize}

Otherwise, consider matching names of arguments in existing R functions.
For example, use \texttt{na.rm} to determine if missing values should be
removed.

\subsection{Checking values}\label{checking-values}

As you start to write more functions, you'll eventually get to the point
where you don't remember exactly how your function works. At this point
it's easy to call your function with invalid inputs. To avoid this
problem, it's often useful to make constraints explicit. For example,
imagine you've written some functions for computing weighted summary
statistics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wt_mean <-}\StringTok{ }\NormalTok{function(x, w) \{}
  \KeywordTok{sum}\NormalTok{(x *}\StringTok{ }\NormalTok{w) /}\StringTok{ }\KeywordTok{sum}\NormalTok{(x)}
\NormalTok{\}}
\NormalTok{wt_var <-}\StringTok{ }\NormalTok{function(x, w) \{}
  \NormalTok{mu <-}\StringTok{ }\KeywordTok{wt_mean}\NormalTok{(x, w)}
  \KeywordTok{sum}\NormalTok{(w *}\StringTok{ }\NormalTok{(x -}\StringTok{ }\NormalTok{mu) ^}\StringTok{ }\DecValTok{2}\NormalTok{) /}\StringTok{ }\KeywordTok{sum}\NormalTok{(w)}
\NormalTok{\}}
\NormalTok{wt_sd <-}\StringTok{ }\NormalTok{function(x, w) \{}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{wt_var}\NormalTok{(x, w))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

What happens if \texttt{x} and \texttt{w} are not the same length?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wt_mean}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{)}
\CommentTok{#> [1] 2.19}
\end{Highlighting}
\end{Shaded}

In this case, because of R's vector recycling rules, we don't get an
error.

It's good practice to check important preconditions, and throw an error
(with \texttt{stop()}), if they are not true:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wt_mean <-}\StringTok{ }\NormalTok{function(x, w) \{}
  \NormalTok{if (}\KeywordTok{length}\NormalTok{(x) !=}\StringTok{ }\KeywordTok{length}\NormalTok{(w)) \{}
    \KeywordTok{stop}\NormalTok{(}\StringTok{"`x` and `w` must be the same length"}\NormalTok{, }\DataTypeTok{call. =} \OtherTok{FALSE}\NormalTok{)}
  \NormalTok{\}}
  \KeywordTok{sum}\NormalTok{(w *}\StringTok{ }\NormalTok{x) /}\StringTok{ }\KeywordTok{sum}\NormalTok{(x)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Be careful not to take this too far. There's a tradeoff between how much
time you spend making your function robust, versus how long you spend
writing it. For example, if you also added a \texttt{na.rm} argument, I
probably wouldn't check it carefully:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wt_mean <-}\StringTok{ }\NormalTok{function(x, w, }\DataTypeTok{na.rm =} \OtherTok{FALSE}\NormalTok{) \{}
  \NormalTok{if (!}\KeywordTok{is.logical}\NormalTok{(na.rm)) \{}
    \KeywordTok{stop}\NormalTok{(}\StringTok{"`na.rm` must be logical"}\NormalTok{)}
  \NormalTok{\}}
  \NormalTok{if (}\KeywordTok{length}\NormalTok{(na.rm) !=}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
    \KeywordTok{stop}\NormalTok{(}\StringTok{"`na.rm` must be length 1"}\NormalTok{)}
  \NormalTok{\}}
  \NormalTok{if (}\KeywordTok{length}\NormalTok{(x) !=}\StringTok{ }\KeywordTok{length}\NormalTok{(w)) \{}
    \KeywordTok{stop}\NormalTok{(}\StringTok{"`x` and `w` must be the same length"}\NormalTok{, }\DataTypeTok{call. =} \OtherTok{FALSE}\NormalTok{)}
  \NormalTok{\}}
  
  \NormalTok{if (na.rm) \{}
    \NormalTok{miss <-}\StringTok{ }\KeywordTok{is.na}\NormalTok{(x) |}\StringTok{ }\KeywordTok{is.na}\NormalTok{(w)}
    \NormalTok{x <-}\StringTok{ }\NormalTok{x[!miss]}
    \NormalTok{w <-}\StringTok{ }\NormalTok{w[!miss]}
  \NormalTok{\}}
  \KeywordTok{sum}\NormalTok{(w *}\StringTok{ }\NormalTok{x) /}\StringTok{ }\KeywordTok{sum}\NormalTok{(x)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This is a lot of extra work for little additional gain. A useful
compromise is the built-in \texttt{stopifnot()}: it checks that each
argument is \texttt{TRUE}, and produces a generic error message if not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wt_mean <-}\StringTok{ }\NormalTok{function(x, w, }\DataTypeTok{na.rm =} \OtherTok{FALSE}\NormalTok{) \{}
  \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{is.logical}\NormalTok{(na.rm), }\KeywordTok{length}\NormalTok{(na.rm) ==}\StringTok{ }\DecValTok{1}\NormalTok{)}
  \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{length}\NormalTok{(x) ==}\StringTok{ }\KeywordTok{length}\NormalTok{(w))}
  
  \NormalTok{if (na.rm) \{}
    \NormalTok{miss <-}\StringTok{ }\KeywordTok{is.na}\NormalTok{(x) |}\StringTok{ }\KeywordTok{is.na}\NormalTok{(w)}
    \NormalTok{x <-}\StringTok{ }\NormalTok{x[!miss]}
    \NormalTok{w <-}\StringTok{ }\NormalTok{w[!miss]}
  \NormalTok{\}}
  \KeywordTok{sum}\NormalTok{(w *}\StringTok{ }\NormalTok{x) /}\StringTok{ }\KeywordTok{sum}\NormalTok{(x)}
\NormalTok{\}}
\KeywordTok{wt_mean}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{:}\DecValTok{1}\NormalTok{, }\DataTypeTok{na.rm =} \StringTok{"foo"}\NormalTok{)}
\CommentTok{#> Error: is.logical(na.rm) is not TRUE}
\end{Highlighting}
\end{Shaded}

Note that when using \texttt{stopifnot()} you assert what should be true
rather than checking for what might be wrong.

\subsection{Dot-dot-dot (\ldots{})}\label{dot-dot-dot}

Many functions in R take an arbitrary number of inputs:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\CommentTok{#> [1] 55}
\NormalTok{stringr::}\KeywordTok{str_c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"d"}\NormalTok{, }\StringTok{"e"}\NormalTok{, }\StringTok{"f"}\NormalTok{)}
\CommentTok{#> [1] "abcdef"}
\end{Highlighting}
\end{Shaded}

How do these functions work? They rely on a special argument:
\texttt{...} (pronounced dot-dot-dot). This special argument captures
any number of arguments that aren't otherwise matched.

It's useful because you can then send those \texttt{...} on to another
function. This is a useful catch-all if your function primarily wraps
another function. For example, I commonly create these helper functions
that wrap around \texttt{str\_c()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commas <-}\StringTok{ }\NormalTok{function(...) stringr::}\KeywordTok{str_c}\NormalTok{(..., }\DataTypeTok{collapse =} \StringTok{", "}\NormalTok{)}
\KeywordTok{commas}\NormalTok{(letters[}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{])}
\CommentTok{#> [1] "a, b, c, d, e, f, g, h, i, j"}

\NormalTok{rule <-}\StringTok{ }\NormalTok{function(..., }\DataTypeTok{pad =} \StringTok{"-"}\NormalTok{) \{}
  \NormalTok{title <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(...)}
  \NormalTok{width <-}\StringTok{ }\KeywordTok{getOption}\NormalTok{(}\StringTok{"width"}\NormalTok{) -}\StringTok{ }\KeywordTok{nchar}\NormalTok{(title) -}\StringTok{ }\DecValTok{5}
  \KeywordTok{cat}\NormalTok{(title, }\StringTok{" "}\NormalTok{, stringr::}\KeywordTok{str_dup}\NormalTok{(pad, width), }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\NormalTok{\}}
\KeywordTok{rule}\NormalTok{(}\StringTok{"Important output"}\NormalTok{)}
\CommentTok{#> Important output ------------------------------------------------------}
\end{Highlighting}
\end{Shaded}

Here \texttt{...} lets me forward on any arguments that I don't want to
deal with to \texttt{str\_c()}. It's a very convenient technique. But it
does come at a price: any misspelled arguments will not raise an error.
This makes it easy for typos to go unnoticed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\KeywordTok{sum}\NormalTok{(x, }\DataTypeTok{na.mr =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 4}
\end{Highlighting}
\end{Shaded}

If you just want to capture the values of the \texttt{...}, use
\texttt{list(...)}.

\subsection{Lazy evaluation}\label{lazy-evaluation}

Arguments in R are lazily evaluated: they're not computed until they're
needed. That means if they're never used, they're never called. This is
an important property of R as a programming language, but is generally
not important when you're writing your own functions for data analysis.
You can read more about lazy evaluation at
\url{http://adv-r.had.co.nz/Functions.html\#lazy-evaluation}.

\subsection{Exercises}\label{exercises-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What does \texttt{commas(letters,\ collapse\ =\ "-")} do? Why?
\item
  It'd be nice if you could supply multiple characters to the
  \texttt{pad} argument, e.g. \texttt{rule("Title",\ pad\ =\ "-+")}. Why
  doesn't this currently work? How could you fix it?
\item
  What does the \texttt{trim} argument to \texttt{mean()} do? When might
  you use it?
\item
  The default value for the \texttt{method} argument to \texttt{cor()}
  is \texttt{c("pearson",\ "kendall",\ "spearman")}. What does that
  mean? What value is used by default?
\end{enumerate}

\section{Return values}\label{return-values}

Figuring out what your function should return is usually
straightforward: it's why you created the function in the first place!
There are two things you should consider when returning a value:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Does returning early make your function easier to read?
\item
  Can you make your function pipeable?
\end{enumerate}

\subsection{Explicit return
statements}\label{explicit-return-statements}

The value returned by the function is usually the last statement it
evaluates, but you can choose to return early by using
\texttt{return()}. I think it's best to save the use of
\texttt{return()} to signal that you can return early with a simpler
solution. A common reason to do this is because the inputs are empty:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complicated_function <-}\StringTok{ }\NormalTok{function(x, y, z) \{}
  \NormalTok{if (}\KeywordTok{length}\NormalTok{(x) ==}\StringTok{ }\DecValTok{0} \NormalTok{||}\StringTok{ }\KeywordTok{length}\NormalTok{(y) ==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
    \KeywordTok{return}\NormalTok{(}\DecValTok{0}\NormalTok{)}
  \NormalTok{\}}
    
  \CommentTok{# Complicated code here}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Another reason is because you have a \texttt{if} statement with one
complex block and one simple block. For example, you might write an if
statement like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\NormalTok{function() \{}
  \NormalTok{if (x) \{}
    \CommentTok{# Do }
    \CommentTok{# something}
    \CommentTok{# that}
    \CommentTok{# takes}
    \CommentTok{# many}
    \CommentTok{# lines}
    \CommentTok{# to}
    \CommentTok{# express}
  \NormalTok{\} else \{}
    \CommentTok{# return something short}
  \NormalTok{\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

But if the first block is very long, by the time you get to the
\texttt{else}, you've forgotten the \texttt{condition}. One way to
rewrite it is to use an early return for the simple case:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{f <-}\StringTok{ }\NormalTok{function() \{}
  \NormalTok{if (!x) \{}
    \KeywordTok{return}\NormalTok{(something_short)}
  \NormalTok{\}}

  \CommentTok{# Do }
  \CommentTok{# something}
  \CommentTok{# that}
  \CommentTok{# takes}
  \CommentTok{# many}
  \CommentTok{# lines}
  \CommentTok{# to}
  \CommentTok{# express}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This tends to make the code easier to understand, because you don't need
quite so much context to understand it.

\subsection{Writing pipeable
functions}\label{writing-pipeable-functions}

If you want to write your own pipeable functions, thinking about the
return value is important. There are two main types of pipeable
functions: transformation and side-effect.

In \textbf{transformation} functions, there's a clear ``primary'' object
that is passed in as the first argument, and a modified version is
returned by the function. For example, the key objects for dplyr and
tidyr are data frames. If you can identify what the object type is for
your domain, you'll find that your functions just work with the pipe.

\textbf{Side-effect} functions are primarily called to perform an
action, like drawing a plot or saving a file, not transforming an
object. These functions should ``invisibly'' return the first argument,
so they're not printed by default, but can still be used in a pipeline.
For example, this simple function that prints out the number of missing
values in a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{show_missings <-}\StringTok{ }\NormalTok{function(df) \{}
  \NormalTok{n <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(df))}
  \KeywordTok{cat}\NormalTok{(}\StringTok{"Missing values: "}\NormalTok{, n, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
  
  \KeywordTok{invisible}\NormalTok{(df)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

If we call it interactively, the \texttt{invisible()} means that the
input \texttt{df} doesn't get printed out:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{show_missings}\NormalTok{(mtcars)}
\CommentTok{#> Missing values: 0}
\end{Highlighting}
\end{Shaded}

But it's still there, it's just not printed by default:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{show_missings}\NormalTok{(mtcars) }
\CommentTok{#> Missing values: 0}
\KeywordTok{class}\NormalTok{(x)}
\CommentTok{#> [1] "data.frame"}
\KeywordTok{dim}\NormalTok{(x)}
\CommentTok{#> [1] 32 11}
\end{Highlighting}
\end{Shaded}

And we can still use it in a pipe:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{show_missings}\NormalTok{() %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{mpg =} \KeywordTok{ifelse}\NormalTok{(mpg <}\StringTok{ }\DecValTok{20}\NormalTok{, }\OtherTok{NA}\NormalTok{, mpg)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{show_missings}\NormalTok{() }
\CommentTok{#> Missing values: 0}
\CommentTok{#> Missing values: 18}
\end{Highlighting}
\end{Shaded}

\section{Environment}\label{environment}

The last component of a function is its environment. This is not
something you need to understand deeply when you first start writing
functions. However, it's important to know a little bit about
environments because they are crucial to how functions work. The
environment of a function controls how R finds the value associated with
a name. For example, take this function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\NormalTok{function(x) \{}
  \NormalTok{x +}\StringTok{ }\NormalTok{y}
\NormalTok{\} }
\end{Highlighting}
\end{Shaded}

In many programming languages, this would be an error, because
\texttt{y} is not defined inside the function. In R, this is valid code
because R uses rules called \textbf{lexical scoping} to find the value
associated with a name. Since \texttt{y} is not defined inside the
function, R will look in the \textbf{environment} where the function was
defined:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\DecValTok{100}
\KeywordTok{f}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{#> [1] 110}

\NormalTok{y <-}\StringTok{ }\DecValTok{1000}
\KeywordTok{f}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{#> [1] 1010}
\end{Highlighting}
\end{Shaded}

This behaviour seems like a recipe for bugs, and indeed you should avoid
creating functions like this deliberately, but by and large it doesn't
cause too many problems (especially if you regularly restart R to get to
a clean slate).

The advantage of this behaviour is that from a language standpoint it
allows R to be very consistent. Every name is looked up using the same
set of rules. For \texttt{f()} that includes the behaviour of two things
that you might not expect: \texttt{\{} and \texttt{+}. This allows you
to do devious things like:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{`}\DataTypeTok{+}\StringTok{`} \NormalTok{<-}\StringTok{ }\NormalTok{function(x, y) \{}
  \NormalTok{if (}\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{) <}\StringTok{ }\FloatTok{0.1}\NormalTok{) \{}
    \KeywordTok{sum}\NormalTok{(x, y)}
  \NormalTok{\} else \{}
    \KeywordTok{sum}\NormalTok{(x, y) *}\StringTok{ }\FloatTok{1.1}
  \NormalTok{\}}
\NormalTok{\}}
\KeywordTok{table}\NormalTok{(}\KeywordTok{replicate}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1} \NormalTok{+}\StringTok{ }\DecValTok{2}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#>   3 3.3 }
\CommentTok{#> 100 900}
\KeywordTok{rm}\NormalTok{(}\StringTok{`}\DataTypeTok{+}\StringTok{`}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is a common phenomenon in R. R places few limits on your power. You
can do many things that you can't do in other programming languages. You
can do many things that 99\% of the time are extremely ill-advised (like
overriding how addition works!). But this power and flexibility is what
makes tools like ggplot2 and dplyr possible. Learning how to make best
use of this flexibility is beyond the scope of this book, but you can
read about in \href{http://adv-r.had.co.nz}{\emph{Advanced R}}.

\hypertarget{vectors}{\chapter{Vectors}\label{vectors}}

\section{Introduction}\label{introduction-13}

So far this book has focussed on tibbles and packages that work with
them. But as you start to write your own functions, and dig deeper into
R, you need to learn about vectors, the objects that underlie tibbles.
If you've learned R in a more traditional way, you're probably already
familiar with vectors, as most R resources start with vectors and work
their way up to tibbles. I think it's better to start with tibbles
because they're immediately useful, and then work your way down to the
underlying components.

Vectors are particularly important as most of the functions you will
write will work with vectors. It is possible to write functions that
work with tibbles (like ggplot2, dplyr, and tidyr), but the tools you
need write such functions are currently idiosyncratic and immature. I am
working on a better approach, \url{https://github.com/hadley/lazyeval},
but it will not be ready in time for the publication of the book. Even
when complete, you'll still need you understand vectors, it'll just make
it easier to write a user-friendly layer on top.

\subsection{Prerequisites}\label{prerequisites-13}

The focus of this chapter is on base R data structures, so it isn't
essential to load any packages. We will, however, use a handful of
functions from the \textbf{purrr} package to avoid some inconsistences
in base R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{Vector basics}\label{vector-basics}

There are two types of vectors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Atomic} vectors, of which there are six types:
  \textbf{logical}, \textbf{integer}, \textbf{double},
  \textbf{character}, \textbf{complex}, and \textbf{raw}. Integer and
  double vectors are collectively known as \textbf{numeric} vectors.
\item
  \textbf{Lists}, which are sometimes called recursive vectors because
  lists can contain other lists.
\end{enumerate}

The chief difference between atomic vectors and lists is that atomic
vectors are \textbf{homogeneous}, while lists can be
\textbf{heterogeneous}. There's one other related object: \texttt{NULL}.
\texttt{NULL} is often used to represent the absence of a vector (as
opposed to \texttt{NA} which is used to represent the absence of a value
in a vector). \texttt{NULL} typically behaves like a vector of length 0.
Figure \ref{fig:datatypes} summarises the interrelationships.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{diagrams/data-structures-overview} 

}

\caption{The hierarchy of R's vector types}\label{fig:datatypes}
\end{figure}

Every vector has two key properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Its \textbf{type}, which you can determine with \texttt{typeof()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(letters)}
\CommentTok{#> [1] "character"}
\KeywordTok{typeof}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{)}
\CommentTok{#> [1] "integer"}
\end{Highlighting}
\end{Shaded}
\item
  Its \textbf{length}, which you can determine with \texttt{length()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{)}
\KeywordTok{length}\NormalTok{(x)}
\CommentTok{#> [1] 3}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

Vectors can also contain arbitrary additional metadata in the form of
attributes. These attributes are used to create \textbf{augmented
vectors} which build on additional behaviour. There are four important
types of augmented vector:

\begin{itemize}
\tightlist
\item
  Factors are built on top of integer vectors.
\item
  Dates and date-times are built on of numeric vectors.
\item
  Data frames and tibbles are built on top of lists.
\end{itemize}

This chapter will introduce you to these important vectors from simplest
to most complicated. You'll start with atomic vectors, then build up to
lists, and finish off with augmented vectors.

\section{Important types of atomic
vector}\label{important-types-of-atomic-vector}

The four most important types of atomic vector are logical, integer,
double, and character. Raw and complex are rarely used during a data
analysis, so I won't discuss them here.

\subsection{Logical}\label{logical}

Logical vectors are the simplest type of atomic vector because they can
take only three possible values: \texttt{FALSE}, \texttt{TRUE}, and
\texttt{NA}. Logical vectors are usually constructed with comparison
operators, as described in \protect\hyperlink{comparisons}{comparisons}.
You can also create them by hand with \texttt{c()}:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\NormalTok{:}\DecValTok{10} \NormalTok{%%}\StringTok{ }\DecValTok{3} \NormalTok{==}\StringTok{ }\DecValTok{0}
\CommentTok{#>  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE}

\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\CommentTok{#> [1]  TRUE  TRUE FALSE    NA}
\end{Highlighting}
\end{Shaded}

\subsection{Numeric}\label{numeric}

Integer and double vectors are known collectively as numeric vectors. In
R, numbers are doubles by default. To make an integer, place an
\texttt{L} after the number:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "double"}
\KeywordTok{typeof}\NormalTok{(1L)}
\CommentTok{#> [1] "integer"}
\FloatTok{1.}\NormalTok{5L}
\CommentTok{#> [1] 1.5}
\end{Highlighting}
\end{Shaded}

The distinction between integers and doubles is not usually important,
but there are two important differences that you should be aware of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Doubles are approximations. Doubles represent floating point numbers
  that can not always be precisely represented with a fixed amount of
  memory. This means that you should consider all doubles to be
  approximations. For example, what is square of the square root of two?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{) ^}\StringTok{ }\DecValTok{2}
\NormalTok{x}
\CommentTok{#> [1] 2}
\NormalTok{x -}\StringTok{ }\DecValTok{2}
\CommentTok{#> [1] 4.44e-16}
\end{Highlighting}
\end{Shaded}

  This behaviour is common when working with floating point numbers:
  most calculations include some approximation error. Instead of
  comparing floating point numbers using \texttt{==}, you should use
  \texttt{dplyr::near()} which allows for some numerical tolerance.
\item
  Integers have one special value: \texttt{NA}, while doubles have four:
  \texttt{NA}, \texttt{NaN}, \texttt{Inf} and \texttt{-Inf}. All three
  special values \texttt{NaN}, \texttt{Inf} and \texttt{-Inf} can arise
  in during division:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(-}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) /}\StringTok{ }\DecValTok{0}
\CommentTok{#> [1] -Inf  NaN  Inf}
\end{Highlighting}
\end{Shaded}

  Avoid using \texttt{==} to check for these other special values.
  Instead use the helper functions \texttt{is.finite()},
  \texttt{is.infinite()}, and \texttt{is.nan()}:

  \begin{longtable}[]{@{}lllll@{}}
  \toprule
  & 0 & Inf & NA & NaN\tabularnewline
  \midrule
  \endhead
  \texttt{is.finite()} & x & & &\tabularnewline
  \texttt{is.infinite()} & & x & &\tabularnewline
  \texttt{is.na()} & & & x & x\tabularnewline
  \texttt{is.nan()} & & & & x\tabularnewline
  \bottomrule
  \end{longtable}
\end{enumerate}

\subsection{Character}\label{character}

Character vectors are the most complex type of atomic vector, because
each element of a character vector is a string, and a string can contain
an arbitrary amount of data.

You've already learned a lot about working with strings in
\protect\hyperlink{strings}{strings}. Here I wanted to mention one
important feature of the underlying string implementation: R uses a
global string pool. This means that each unique string is only stored in
memory once, and every use of the string points to that representation.
This reduces the amount of memory needed by duplicated strings. You can
see this behaviour in practice with \texttt{pryr::object\_size()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "This is a reasonably long string."}
\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(x)}
\CommentTok{#> 136 B}

\NormalTok{y <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(x, }\DecValTok{1000}\NormalTok{)}
\NormalTok{pryr::}\KeywordTok{object_size}\NormalTok{(y)}
\CommentTok{#> 8.13 kB}
\end{Highlighting}
\end{Shaded}

\texttt{y} doesn't take up 1,000x as much memory as \texttt{x}, because
each element of \texttt{y} is just a pointer to that same string. A
pointer is 8 bytes, so 1000 pointers to a 136 B string is 8 * 1000 + 136
= 8.13 kB.

\subsection{Missing values}\label{missing-values-4}

Note that each type of atomic vector has its own missing value:

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{NA}            \CommentTok{# logical}
\CommentTok{#> [1] NA}
\OtherTok{NA_integer_}   \CommentTok{# integer}
\CommentTok{#> [1] NA}
\OtherTok{NA_real_}      \CommentTok{# double}
\CommentTok{#> [1] NA}
\OtherTok{NA_character_} \CommentTok{# character}
\CommentTok{#> [1] NA}
\end{Highlighting}
\end{Shaded}

Normally you don't need to know about these different types because you
can always use \texttt{NA} and it will be converted to the correct type
using the implicit coercion rules described next. However, there are
some functions that are strict about their inputs, so it's useful to
have this knowledge sitting in your back pocket so you can be specific
when needed.

\subsection{Exercises}\label{exercises-51}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Describe the difference between \texttt{is.finite(x)} and
  \texttt{!is.infinite(x)}.
\item
  Read the source code for \texttt{dplyr::near()} (Hint: to see the
  source code, drop the \texttt{()}). How does it work?
\item
  A logical vector can take 3 possible values. How many possible values
  can an integer vector take? How many possible values can a double
  take? Use google to do some research.
\item
  Brainstorm at least four functions that allow you to convert a double
  to an integer. How do they differ? Be precise.
\item
  What functions from the readr package allow you to turn a string into
  logical, integer, and double vector?
\end{enumerate}

\section{Using atomic vectors}\label{using-atomic-vectors}

Now that you understand the different types of atomic vector, it's
useful to review some of the important tools for working with them.
These include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How to convert from one type to another, and when that happens
  automatically.
\item
  How to tell if an object is a specific type of vector.
\item
  What happens when you work with vectors of different lengths.
\item
  How to name the elements of a vector.
\item
  How to pull out elements of interest.
\end{enumerate}

\subsection{Coercion}\label{coercion}

There are two ways to convert, or coerce, one type of vector to another:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explicit coercion happens when you call a function like
  \texttt{as.logical()}, \texttt{as.integer()}, \texttt{as.double()}, or
  \texttt{as.character()}. Whenever you find yourself using explicit
  coercion, you should always check whether you can make the fix
  upstream, so that the vector never had the wrong type in the first
  place. For example, you may need to tweak your readr
  \texttt{col\_types} specification.
\item
  Implicit coercion happens when you use a vector in a specific context
  that expects a certain type of vector. For example, when you use a
  logical vector with a numeric summary function, or when you use a
  double vector where an integer vector is expected.
\end{enumerate}

Because explicit coercion is used relatively rarely, and is largely easy
to understand, I'll focus on implicit coercion here.

You've already seen the most important type of implicit coercion: using
a logical vector in a numeric context. In this case \texttt{TRUE} is
converted to \texttt{1} and \texttt{FALSE} converted to 0. That means
the sum of a logical vector is the number of trues, and the mean of a
logical vector is the proportion of trues:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\NormalTok{x >}\StringTok{ }\DecValTok{10}
\KeywordTok{sum}\NormalTok{(y)  }\CommentTok{# how many are greater than 10?}
\CommentTok{#> [1] 44}
\KeywordTok{mean}\NormalTok{(y) }\CommentTok{# what proportion are greater than 10?}
\CommentTok{#> [1] 0.44}
\end{Highlighting}
\end{Shaded}

You may see some code (typically older) that relies on implicit coercion
in the opposite direction, from integer to logical:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (}\KeywordTok{length}\NormalTok{(x)) \{}
  \CommentTok{# do something}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In this case, 0 is converted to \texttt{FALSE} and everything else is
converted to \texttt{TRUE}. I think this makes it harder to understand
your code, and I don't recommend it. Instead be explicit:
\texttt{length(x)\ \textgreater{}\ 0}.

It's also important to understand what happens when you try and create a
vector containing multiple types with \texttt{c()}: the most complex
type always wins.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, 1L))}
\CommentTok{#> [1] "integer"}
\KeywordTok{typeof}\NormalTok{(}\KeywordTok{c}\NormalTok{(1L, }\FloatTok{1.5}\NormalTok{))}
\CommentTok{#> [1] "double"}
\KeywordTok{typeof}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\StringTok{"a"}\NormalTok{))}
\CommentTok{#> [1] "character"}
\end{Highlighting}
\end{Shaded}

An atomic vector can not have a mix of different types because the type
is a property of the complete vector, not the individual elements. If
you need to mix multiple types in the same vector, you should use a
list, which you'll learn about shortly.

\subsection{Test functions}\label{test-functions}

Sometimes you want to do different things based on the type of vector.
One option is to use \texttt{typeof()}. Another is to use a test
function which returns a \texttt{TRUE} or \texttt{FALSE}. Base R
provides many functions like \texttt{is.vector()} and
\texttt{is.atomic()}, but they often returns surprising results.
Instead, it's safer to use the \texttt{is\_*} functions provided by
purrr, which are summarised in the table below.

\begin{longtable}[]{@{}llllll@{}}
\toprule
& lgl & int & dbl & chr & list\tabularnewline
\midrule
\endhead
\texttt{is\_logical()} & x & & & &\tabularnewline
\texttt{is\_integer()} & & x & & &\tabularnewline
\texttt{is\_double()} & & & x & &\tabularnewline
\texttt{is\_numeric()} & & x & x & &\tabularnewline
\texttt{is\_character()} & & & & x &\tabularnewline
\texttt{is\_atomic()} & x & x & x & x &\tabularnewline
\texttt{is\_list()} & & & & & x\tabularnewline
\texttt{is\_vector()} & x & x & x & x & x\tabularnewline
\bottomrule
\end{longtable}

Each predicate also comes with a ``scalar'' version, like
\texttt{is\_scalar\_atomic()}, which checks that the length is 1. This
is useful, for example, if you want to check that an argument to your
function is a single logical value.

\subsection{Scalars and recycling
rules}\label{scalars-and-recycling-rules}

As well as implicitly coercing the types of vectors to be compatible, R
will also implicitly coerce the length of vectors. This is called vector
\textbf{recycling}, because the shorter vector is repeated, or recycled,
to the same length as the longer vector.

This is generally most useful when you are mixing vectors and
``scalars''. I put scalars in quotes because R doesn't actually have
scalars: instead, a single number is a vector of length 1. Because there
are no scalars, most built-in functions are \textbf{vectorised}, meaning
that they will operate on a vector of numbers. That's why, for example,
this code works:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\DecValTok{10}\NormalTok{) +}\StringTok{ }\DecValTok{100}
\CommentTok{#>  [1] 109 108 104 102 103 110 106 107 105 101}
\KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{) >}\StringTok{ }\FloatTok{0.5}
\CommentTok{#>  [1]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE}
\end{Highlighting}
\end{Shaded}

In R, basic mathematical operations work with vectors. That means that
you should never need to perform explicit iteration when performing
simple mathematical computations.

It's intuitive what should happen if you add two vectors of the same
length, or a vector and a ``scalar'', but what happens if you add two
vectors of different lengths?

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\NormalTok{:}\DecValTok{10} \NormalTok{+}\StringTok{ }\DecValTok{1}\NormalTok{:}\DecValTok{2}
\CommentTok{#>  [1]  2  4  4  6  6  8  8 10 10 12}
\end{Highlighting}
\end{Shaded}

Here, R will expand the shortest vector to the same length as the
longest, so called recycling. This is silent except when the length of
the longer is not an integer multiple of the length of the shorter:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\NormalTok{:}\DecValTok{10} \NormalTok{+}\StringTok{ }\DecValTok{1}\NormalTok{:}\DecValTok{3}
\CommentTok{#> Warning in 1:10 + 1:3: longer object length is not a multiple of shorter}
\CommentTok{#> object length}
\CommentTok{#>  [1]  2  4  6  5  7  9  8 10 12 11}
\end{Highlighting}
\end{Shaded}

While vector recycling can be used to create very succinct, clever code,
it can also silently conceal problems. For this reason, the vectorised
functions in tidyverse will throw errors when you recycle anything other
than a scalar. If you do want to recycle, you'll need to do it yourself
with \texttt{rep()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{, }\DataTypeTok{y =} \DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{)}
\CommentTok{#> Error: Variables must be length 1 or 4.}
\CommentTok{#> Problem variables: 'y'}

\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{, }\DataTypeTok{y =} \KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{#> # A tibble: 4 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <int> <int>}
\CommentTok{#> 1     1     1}
\CommentTok{#> 2     2     2}
\CommentTok{#> 3     3     1}
\CommentTok{#> 4     4     2}

\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{, }\DataTypeTok{y =} \KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{each =} \DecValTok{2}\NormalTok{))}
\CommentTok{#> # A tibble: 4 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <int> <int>}
\CommentTok{#> 1     1     1}
\CommentTok{#> 2     2     1}
\CommentTok{#> 3     3     2}
\CommentTok{#> 4     4     2}
\end{Highlighting}
\end{Shaded}

\subsection{Naming vectors}\label{naming-vectors}

All types of vectors can be named. You can name them during creation
with \texttt{c()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{, }\DataTypeTok{y =} \DecValTok{2}\NormalTok{, }\DataTypeTok{z =} \DecValTok{4}\NormalTok{)}
\CommentTok{#> x y z }
\CommentTok{#> 1 2 4}
\end{Highlighting}
\end{Shaded}

Or after the fact with \texttt{purrr::set\_names()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set_names}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{))}
\CommentTok{#> a b c }
\CommentTok{#> 1 2 3}
\end{Highlighting}
\end{Shaded}

Named vectors are most useful for subsetting, described next.

\hypertarget{vector-subsetting}{\subsection{Subsetting}\label{vector-subsetting}}

So far we've used \texttt{dplyr::filter()} to filter the rows in a
tibble. \texttt{filter()} only works with tibble, so we'll need new tool
for vectors: \texttt{{[}}. \texttt{{[}} is the subsetting function, and
is called like \texttt{x{[}a{]}}. There are four types of things that
you can subset a vector with:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A numeric vector containing only integers. The integers must either be
  all positive, all negative, or zero.

  Subsetting with positive integers keeps the elements at those
  positions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"one"}\NormalTok{, }\StringTok{"two"}\NormalTok{, }\StringTok{"three"}\NormalTok{, }\StringTok{"four"}\NormalTok{, }\StringTok{"five"}\NormalTok{)}
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)]}
\CommentTok{#> [1] "three" "two"   "five"}
\end{Highlighting}
\end{Shaded}

  By repeating a position, you can actually make a longer output than
  input:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{)]}
\CommentTok{#> [1] "one"  "one"  "five" "five" "five" "two"}
\end{Highlighting}
\end{Shaded}

  Negative values drop the elements at the specified positions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{c}\NormalTok{(-}\DecValTok{1}\NormalTok{, -}\DecValTok{3}\NormalTok{, -}\DecValTok{5}\NormalTok{)]}
\CommentTok{#> [1] "two"  "four"}
\end{Highlighting}
\end{Shaded}

  It's an error to mix positive and negative values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, -}\DecValTok{1}\NormalTok{)]}
\CommentTok{#> Error in x[c(1, -1)]: only 0's may be mixed with negative subscripts}
\end{Highlighting}
\end{Shaded}

  The error message mentions subsetting with zero, which returns no
  values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{0}\NormalTok{]}
\CommentTok{#> character(0)}
\end{Highlighting}
\end{Shaded}

  This is not useful very often, but it can be helpful if you want to
  create unusual data structures to test your functions with.
\item
  Subsetting with a logical vector keeps all values corresponding to a
  \texttt{TRUE} value. This is most often useful in conjunction with the
  comparison functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\OtherTok{NA}\NormalTok{)}

\CommentTok{# All non-missing values of x}
\NormalTok{x[!}\KeywordTok{is.na}\NormalTok{(x)]}
\CommentTok{#> [1] 10  3  5  8  1}

\CommentTok{# All even (or missing!) values of x}
\NormalTok{x[x %%}\StringTok{ }\DecValTok{2} \NormalTok{==}\StringTok{ }\DecValTok{0}\NormalTok{]}
\CommentTok{#> [1] 10 NA  8 NA}
\end{Highlighting}
\end{Shaded}
\item
  If you have a named vector, you can subset it with a character vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{abc =} \DecValTok{1}\NormalTok{, }\DataTypeTok{def =} \DecValTok{2}\NormalTok{, }\DataTypeTok{xyz =} \DecValTok{5}\NormalTok{)}
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\StringTok{"xyz"}\NormalTok{, }\StringTok{"def"}\NormalTok{)]}
\CommentTok{#> xyz def }
\CommentTok{#>   5   2}
\end{Highlighting}
\end{Shaded}

  Like with positive integers, you can also use a character vector to
  duplicate individual entries.
\item
  The simplest type of subsetting is nothing, \texttt{x{[}{]}}, which
  returns the complete \texttt{x}. This is not useful for subsetting
  vectors, but it is useful when subsetting matrices (and other high
  dimensional structures) because it lets you select all the rows or all
  the columns, by leaving that index blank. For example, if \texttt{x}
  is 2d, \texttt{x{[}1,\ {]}} selects the first row and all the columns,
  and \texttt{x{[},\ -1{]}} selects all rows and all columns except the
  first.
\end{enumerate}

To learn more about the applications of subsetting, reading the
``Subsetting'' chapter of \emph{Advanced R}:
\url{http://adv-r.had.co.nz/Subsetting.html\#applications}.

There is an important variation of \texttt{{[}} called \texttt{{[}{[}}.
\texttt{{[}{[}} only ever extracts a single element, and always drops
names. It's a good idea to use it whenever you want to make it clear
that you're extracting a single item, as in a for loop. The distinction
between \texttt{{[}} and \texttt{{[}{[}} is most important for lists, as
we'll see shortly.

\subsection{Exercises}\label{exercises-52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What does \texttt{mean(is.na(x))} tell you about a vector \texttt{x}?
  What about \texttt{sum(!is.finite(x))}?
\item
  Carefully read the documentation of \texttt{is.vector()}. What does it
  actually test for? Why does \texttt{is.atomic()} not agree with the
  definition of atomic vectors above?
\item
  Compare and contrast \texttt{setNames()} with
  \texttt{purrr::set\_names()}.
\item
  Create functions that take a vector as input and returns:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    The last value. Should you use \texttt{{[}} or \texttt{{[}{[}}?
  \item
    The elements at even numbered positions.
  \item
    Every element except the last value.
  \item
    Only even numbers (and no missing values).
  \end{enumerate}
\item
  Why is \texttt{x{[}-which(x\ \textgreater{}\ 0){]}} not the same as
  \texttt{x{[}x\ \textless{}=\ 0{]}}?
\item
  What happens when you subset with a positive integer that's bigger
  than the length of the vector? What happens when you subset with a
  name that doesn't exist?
\end{enumerate}

\hypertarget{lists}{\section{Recursive vectors (lists)}\label{lists}}

Lists are a step up in complexity from atomic vectors, because lists can
contain other lists. This makes them suitable for representing
hierarchical or tree-like structures. You create a list with
\texttt{list()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{x}
\CommentTok{#> [[1]]}
\CommentTok{#> [1] 1}
\CommentTok{#> }
\CommentTok{#> [[2]]}
\CommentTok{#> [1] 2}
\CommentTok{#> }
\CommentTok{#> [[3]]}
\CommentTok{#> [1] 3}
\end{Highlighting}
\end{Shaded}

A very useful tool for working with lists is \texttt{str()} because it
focusses on the \textbf{str}ucture, not the contents.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(x)}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num 1}
\CommentTok{#>  $ : num 2}
\CommentTok{#>  $ : num 3}

\NormalTok{x_named <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{, }\DataTypeTok{b =} \DecValTok{2}\NormalTok{, }\DataTypeTok{c =} \DecValTok{3}\NormalTok{)}
\KeywordTok{str}\NormalTok{(x_named)}
\CommentTok{#> List of 3}
\CommentTok{#>  $ a: num 1}
\CommentTok{#>  $ b: num 2}
\CommentTok{#>  $ c: num 3}
\end{Highlighting}
\end{Shaded}

Unlike atomic vectors, \texttt{list()} can contain a mix of objects:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\StringTok{"a"}\NormalTok{, 1L, }\FloatTok{1.5}\NormalTok{, }\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{str}\NormalTok{(y)}
\CommentTok{#> List of 4}
\CommentTok{#>  $ : chr "a"}
\CommentTok{#>  $ : int 1}
\CommentTok{#>  $ : num 1.5}
\CommentTok{#>  $ : logi TRUE}
\end{Highlighting}
\end{Shaded}

Lists can even contain other lists!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{list}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\KeywordTok{str}\NormalTok{(z)}
\CommentTok{#> List of 2}
\CommentTok{#>  $ :List of 2}
\CommentTok{#>   ..$ : num 1}
\CommentTok{#>   ..$ : num 2}
\CommentTok{#>  $ :List of 2}
\CommentTok{#>   ..$ : num 3}
\CommentTok{#>   ..$ : num 4}
\end{Highlighting}
\end{Shaded}

\subsection{Visualising lists}\label{visualising-lists}

To explain more complicated list manipulation functions, it's helpful to
have a visual representation of lists. For example, take these three
lists:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{list}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{x3 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DecValTok{2}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DecValTok{3}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

I'll draw them as follows:

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/lists-structure} \end{center}

There are three principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Lists have rounded corners. Atomic vectors have square corners.
\item
  Children are drawn inside their parent, and have a slightly darker
  background to make it easier to see the hierarchy.
\item
  The orientation of the children (i.e.~rows or columns) isn't
  important, so I'll pick a row or column orientation to either save
  space or illustrate an important property in the example.
\end{enumerate}

\hypertarget{subsetting-1}{\subsection{Subsetting}\label{subsetting-1}}

There are three ways to subset a list, which I'll illustrate with a list
named \texttt{a}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{b =} \StringTok{"a string"}\NormalTok{, }\DataTypeTok{c =} \NormalTok{pi, }\DataTypeTok{d =} \KeywordTok{list}\NormalTok{(-}\DecValTok{1}\NormalTok{, -}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  \texttt{{[}} extracts a sub-list. The result will always be a list.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(a[}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{])}
\CommentTok{#> List of 2}
\CommentTok{#>  $ a: int [1:3] 1 2 3}
\CommentTok{#>  $ b: chr "a string"}
\KeywordTok{str}\NormalTok{(a[}\DecValTok{4}\NormalTok{])}
\CommentTok{#> List of 1}
\CommentTok{#>  $ d:List of 2}
\CommentTok{#>   ..$ : num -1}
\CommentTok{#>   ..$ : num -5}
\end{Highlighting}
\end{Shaded}

  Like with vectors, you can subset with a logical, integer, or
  character vector.
\item
  \texttt{{[}{[}} extracts a single component from a list. It removes a
  level of hierarchy from the list.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(a[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#>  int [1:3] 1 2 3}
\KeywordTok{str}\NormalTok{(a[[}\DecValTok{4}\NormalTok{]])}
\CommentTok{#> List of 2}
\CommentTok{#>  $ : num -1}
\CommentTok{#>  $ : num -5}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{\$} is a shorthand for extracting named elements of a list. It
  works similarly to \texttt{{[}{[}} except that you don't need to use
  quotes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a$a}
\CommentTok{#> [1] 1 2 3}
\NormalTok{a[[}\StringTok{"a"}\NormalTok{]]}
\CommentTok{#> [1] 1 2 3}
\end{Highlighting}
\end{Shaded}
\end{itemize}

The distinction between \texttt{{[}} and \texttt{{[}{[}} is really
important for lists, because \texttt{{[}{[}} drills down into the list
while \texttt{{[}} returns a new, smaller list. Compare the code and
output above with the visual representation in Figure
\ref{fig:lists-subsetting}.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{diagrams/lists-subsetting} 

}

\caption{Subsetting a list, visually.}\label{fig:lists-subsetting}
\end{figure}

\subsection{Lists of condiments}\label{lists-of-condiments}

The difference between \texttt{{[}} and \texttt{{[}{[}} is very
important, but it's easy to get confused. To help you remember, let me
show you an unusual pepper shaker.

\begin{center}\includegraphics[width=0.25\linewidth]{images/pepper} \end{center}

If this pepper shaker is your list \texttt{x}, then, \texttt{x{[}1{]}}
is a pepper shaker containing a single pepper packet:

\begin{center}\includegraphics[width=0.25\linewidth]{images/pepper-1} \end{center}

\texttt{x{[}2{]}} would look the same, but would contain the second
packet. \texttt{x{[}1:2{]}} would be a pepper shaker containing two
pepper packets.

\texttt{x{[}{[}1{]}{]}} is:

\begin{center}\includegraphics[width=0.25\linewidth]{images/pepper-2} \end{center}

If you wanted to get the content of the pepper package, you'd need
\texttt{x{[}{[}1{]}{]}{[}{[}1{]}{]}}:

\begin{center}\includegraphics[width=0.25\linewidth]{images/pepper-3} \end{center}

\subsection{Exercises}\label{exercises-53}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Draw the following lists as nested sets:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \texttt{list(a,\ b,\ list(c,\ d),\ list(e,\ f))}
  \item
    \texttt{list(list(list(list(list(list(a))))))}
  \end{enumerate}
\item
  What happens if you subset a tibble as if you're subsetting a list?
  What are the key differences between a list and a tibble?
\end{enumerate}

\section{Attributes}\label{attributes}

Any vector can contain arbitrary additional metadata through its
\textbf{attributes}. You can think of attributes as named list of
vectors that can be attached to any object. You can get and set
individual attribute values with \texttt{attr()} or see them all at once
with \texttt{attributes()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\NormalTok{:}\DecValTok{10}
\KeywordTok{attr}\NormalTok{(x, }\StringTok{"greeting"}\NormalTok{)}
\CommentTok{#> NULL}
\KeywordTok{attr}\NormalTok{(x, }\StringTok{"greeting"}\NormalTok{) <-}\StringTok{ "Hi!"}
\KeywordTok{attr}\NormalTok{(x, }\StringTok{"farewell"}\NormalTok{) <-}\StringTok{ "Bye!"}
\KeywordTok{attributes}\NormalTok{(x)}
\CommentTok{#> $greeting}
\CommentTok{#> [1] "Hi!"}
\CommentTok{#> }
\CommentTok{#> $farewell}
\CommentTok{#> [1] "Bye!"}
\end{Highlighting}
\end{Shaded}

There are three very important attributes that are used to implement
fundamental parts of R:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Names} are used to name the elements of a vector.
\item
  \textbf{Dimensions} (dims, for short) make a vector behave like a
  matrix or array.
\item
  \textbf{Class} is used to implement the S3 object oriented system.
\end{enumerate}

You've seen names above, and we won't cover dimensions because we don't
use matrices in this book. It remains to describe the class, which
controls how \textbf{generic functions} work. Generic functions are key
to object oriented programming in R, because they make functions behave
differently for different classes of input. A detailed discussion of
object oriented programming is beyond the scope of this book, but you
can read more about it in \emph{Advanced R} at
\url{http://adv-r.had.co.nz/OO-essentials.html\#s3}.

Here's what a typical generic function looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{as.Date}
\CommentTok{#> function (x, ...) }
\CommentTok{#> UseMethod("as.Date")}
\CommentTok{#> <bytecode: 0x7fb9b5e62040>}
\CommentTok{#> <environment: namespace:base>}
\end{Highlighting}
\end{Shaded}

The call to ``UseMethod'' means that this is a generic function, and it
will call a specific \textbf{method}, a function, based on the class of
the first argument. (All methods are functions; not all functions are
methods). You can list all the methods for a generic with
\texttt{methods()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{methods}\NormalTok{(}\StringTok{"as.Date"}\NormalTok{)}
\CommentTok{#> [1] as.Date.character as.Date.date      as.Date.dates     as.Date.default  }
\CommentTok{#> [5] as.Date.factor    as.Date.numeric   as.Date.POSIXct   as.Date.POSIXlt  }
\CommentTok{#> see '?methods' for accessing help and source code}
\end{Highlighting}
\end{Shaded}

For example, if \texttt{x} is a character vector, \texttt{as.Date()}
will call \texttt{as.Date.character()}; if it's a factor, it'll call
\texttt{as.Date.factor()}.

You can see the specific implementation of a method with
\texttt{getS3method()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getS3method}\NormalTok{(}\StringTok{"as.Date"}\NormalTok{, }\StringTok{"default"}\NormalTok{)}
\CommentTok{#> function (x, ...) }
\CommentTok{#> \{}
\CommentTok{#>     if (inherits(x, "Date")) }
\CommentTok{#>         return(x)}
\CommentTok{#>     if (is.logical(x) && all(is.na(x))) }
\CommentTok{#>         return(structure(as.numeric(x), class = "Date"))}
\CommentTok{#>     stop(gettextf("do not know how to convert '%s' to class %s", }
\CommentTok{#>         deparse(substitute(x)), dQuote("Date")), domain = NA)}
\CommentTok{#> \}}
\CommentTok{#> <bytecode: 0x7fb9b5b3e7b8>}
\CommentTok{#> <environment: namespace:base>}
\KeywordTok{getS3method}\NormalTok{(}\StringTok{"as.Date"}\NormalTok{, }\StringTok{"numeric"}\NormalTok{)}
\CommentTok{#> function (x, origin, ...) }
\CommentTok{#> \{}
\CommentTok{#>     if (missing(origin)) }
\CommentTok{#>         stop("'origin' must be supplied")}
\CommentTok{#>     as.Date(origin, ...) + x}
\CommentTok{#> \}}
\CommentTok{#> <bytecode: 0x7fb9b6898120>}
\CommentTok{#> <environment: namespace:base>}
\end{Highlighting}
\end{Shaded}

The most important S3 generic is \texttt{print()}: it controls how the
object is printed when you type its name at the console. Other important
generics are the subsetting functions \texttt{{[}}, \texttt{{[}{[}}, and
\texttt{\$}.

\section{Augmented vectors}\label{augmented-vectors}

Atomic vectors and lists are the building blocks for other important
vector types like factors and dates. I call these \textbf{augmented
vectors}, because they are vectors with additional \textbf{attributes},
including class. Because augmented vectors have a class, they behave
differently to the atomic vector on which they are built. In this book,
we make use of four important augmented vectors:

\begin{itemize}
\tightlist
\item
  Factors.
\item
  Date-times
\item
  Times.
\item
  Tibbles.
\end{itemize}

These are described below.

\hypertarget{factors-1}{\subsection{Factors}\label{factors-1}}

Factors are designed to represent categorical data that can take a fixed
set of possible values. Factors are built on top of integers, and have a
levels attribute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"ab"}\NormalTok{, }\StringTok{"cd"}\NormalTok{, }\StringTok{"ab"}\NormalTok{), }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"ab"}\NormalTok{, }\StringTok{"cd"}\NormalTok{, }\StringTok{"ef"}\NormalTok{))}
\KeywordTok{typeof}\NormalTok{(x)}
\CommentTok{#> [1] "integer"}
\KeywordTok{attributes}\NormalTok{(x)}
\CommentTok{#> $levels}
\CommentTok{#> [1] "ab" "cd" "ef"}
\CommentTok{#> }
\CommentTok{#> $class}
\CommentTok{#> [1] "factor"}
\end{Highlighting}
\end{Shaded}

\subsection{Dates and date-times}\label{dates-and-date-times}

Dates in R are numeric vectors that represent the number of days since 1
January 1970.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"1971-01-01"}\NormalTok{)}
\KeywordTok{unclass}\NormalTok{(x)}
\CommentTok{#> [1] 365}

\KeywordTok{typeof}\NormalTok{(x)}
\CommentTok{#> [1] "double"}
\KeywordTok{attributes}\NormalTok{(x)}
\CommentTok{#> $class}
\CommentTok{#> [1] "Date"}
\end{Highlighting}
\end{Shaded}

Date-times are numeric vectors with class \texttt{POSIXct} that
represent the number of seconds since 1 January 1970. (In case you were
wondering, ``POSIXct'' stands for ``Portable Operating System
Interface'', calendar time.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{lubridate::}\KeywordTok{ymd_hm}\NormalTok{(}\StringTok{"1970-01-01 01:00"}\NormalTok{)}
\KeywordTok{unclass}\NormalTok{(x)}
\CommentTok{#> [1] 3600}
\CommentTok{#> attr(,"tzone")}
\CommentTok{#> [1] "UTC"}

\KeywordTok{typeof}\NormalTok{(x)}
\CommentTok{#> [1] "double"}
\KeywordTok{attributes}\NormalTok{(x)}
\CommentTok{#> $tzone}
\CommentTok{#> [1] "UTC"}
\CommentTok{#> }
\CommentTok{#> $class}
\CommentTok{#> [1] "POSIXct" "POSIXt"}
\end{Highlighting}
\end{Shaded}

The \texttt{tzone} attribute is optional. It controls how the time is
printed, not what absolute time it refers to.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attr}\NormalTok{(x, }\StringTok{"tzone"}\NormalTok{) <-}\StringTok{ "US/Pacific"}
\NormalTok{x}
\CommentTok{#> [1] "1969-12-31 17:00:00 PST"}

\KeywordTok{attr}\NormalTok{(x, }\StringTok{"tzone"}\NormalTok{) <-}\StringTok{ "US/Eastern"}
\NormalTok{x}
\CommentTok{#> [1] "1969-12-31 20:00:00 EST"}
\end{Highlighting}
\end{Shaded}

There is another type of date-times called POSIXlt. These are built on
top of named lists:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.POSIXlt}\NormalTok{(x)}
\KeywordTok{typeof}\NormalTok{(y)}
\CommentTok{#> [1] "list"}
\KeywordTok{attributes}\NormalTok{(y)}
\CommentTok{#> $names}
\CommentTok{#>  [1] "sec"    "min"    "hour"   "mday"   "mon"    "year"   "wday"  }
\CommentTok{#>  [8] "yday"   "isdst"  "zone"   "gmtoff"}
\CommentTok{#> }
\CommentTok{#> $class}
\CommentTok{#> [1] "POSIXlt" "POSIXt" }
\CommentTok{#> }
\CommentTok{#> $tzone}
\CommentTok{#> [1] "US/Eastern" "EST"        "EDT"}
\end{Highlighting}
\end{Shaded}

POSIXlts are rare inside the tidyverse. They do crop up in base R,
because they are needed to extract specific components of a date, like
the year or month. Since lubridate provides helpers for you to do this
instead, you don't need them. POSIXct's are always easier to work with,
so if you find you have a POSIXlt, you should always convert it to a
regular data time \texttt{lubridate::as\_date\_time()}.

\hypertarget{tibbles-1}{\subsection{Tibbles}\label{tibbles-1}}

Tibbles are augmented lists: they have class ``tbl\_df'' + ``tbl'' +
``data.frame'', and \texttt{names} (column) and \texttt{row.names}
attributes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb <-}\StringTok{ }\NormalTok{tibble::}\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{, }\DataTypeTok{y =} \DecValTok{5}\NormalTok{:}\DecValTok{1}\NormalTok{)}
\KeywordTok{typeof}\NormalTok{(tb)}
\CommentTok{#> [1] "list"}
\KeywordTok{attributes}\NormalTok{(tb)}
\CommentTok{#> $names}
\CommentTok{#> [1] "x" "y"}
\CommentTok{#> }
\CommentTok{#> $class}
\CommentTok{#> [1] "tbl_df"     "tbl"        "data.frame"}
\CommentTok{#> }
\CommentTok{#> $row.names}
\CommentTok{#> [1] 1 2 3 4 5}
\end{Highlighting}
\end{Shaded}

The difference between a tibble and a list is that all the elements of a
data frame must be vectors with the same length. All functions that work
with tibbles enforce this constraint.

Traditional data.frames have a very similar structure:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{, }\DataTypeTok{y =} \DecValTok{5}\NormalTok{:}\DecValTok{1}\NormalTok{)}
\KeywordTok{typeof}\NormalTok{(df)}
\CommentTok{#> [1] "list"}
\KeywordTok{attributes}\NormalTok{(df)}
\CommentTok{#> $names}
\CommentTok{#> [1] "x" "y"}
\CommentTok{#> }
\CommentTok{#> $row.names}
\CommentTok{#> [1] 1 2 3 4 5}
\CommentTok{#> }
\CommentTok{#> $class}
\CommentTok{#> [1] "data.frame"}
\end{Highlighting}
\end{Shaded}

The main difference is the class. The class of tibble includes
``data.frame'' which means tibbles inherit the regular data frame
behaviour by default.

\subsection{Exercises}\label{exercises-54}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What does \texttt{hms::hms(3600)} return? How does it print? What
  primitive type is the augmented vector built on top of? What
  attributes does it use?
\item
  Try and make a tibble that has columns with different lengths. What
  happens?
\item
  Based on the definition above, is it ok to have a list as a column of
  a tibble?
\end{enumerate}

\hypertarget{iteration}{\chapter{Iteration}\label{iteration}}

\section{Introduction}\label{introduction-14}

In \protect\hyperlink{functions}{functions}, we talked about how
important it is to reduce duplication in your code by creating functions
instead of copying-and-pasting. Reducing code duplication has three main
benefits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It's easier to see the intent of your code, because your eyes are
  drawn to what's different, not what stays the same.
\item
  It's easier to respond to changes in requirements. As your needs
  change, you only need to make changes in one place, rather than
  remembering to change every place that you copied-and-pasted the code.
\item
  You're likely to have fewer bugs because each line of code is used in
  more places.
\end{enumerate}

One tool for reducing duplication is functions, which reduce duplication
by identifying repeated patterns of code and extract them out into
independent pieces that can be easily reused and updated. Another tool
for reducing duplication is \textbf{iteration}, which helps you when you
need to do the same thing to multiple inputs: repeating the same
operation on different columns, or on different datasets. In this
chapter you'll learn about two important iteration paradigms: imperative
programming and functional programming. On the imperative side you have
tools like for loops and while loops, which are a great place to start
because they make iteration very explicit, so it's obvious what's
happening. However, for loops are quite verbose, and require quite a bit
of bookkeeping code that is duplicated for every for loop. Functional
programming (FP) offers tools to extract out this duplicated code, so
each common for loop pattern gets its own function. Once you master the
vocabulary of FP, you can solve many common iteration problems with less
code, more ease, and fewer errors.

\subsection{Prerequisites}\label{prerequisites-14}

Once you've mastered the for loops provided by base R, you'll learn some
of the powerful programming tools provided by purrr, one of the
tidyverse core packages.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{For loops}\label{for-loops}

Imagine we have this simple tibble:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{a =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{b =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{c =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{d =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We want to compute the median of each column. You \emph{could} do with
copy-and-paste:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(df$a)}
\CommentTok{#> [1] -0.246}
\KeywordTok{median}\NormalTok{(df$b)}
\CommentTok{#> [1] -0.287}
\KeywordTok{median}\NormalTok{(df$c)}
\CommentTok{#> [1] -0.0567}
\KeywordTok{median}\NormalTok{(df$d)}
\CommentTok{#> [1] 0.144}
\end{Highlighting}
\end{Shaded}

But that breaks our rule of thumb: never copy and paste more than twice.
Instead, we could use a for loop:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{output <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"double"}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(df))  }\CommentTok{# 1. output}
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(df)) \{            }\CommentTok{# 2. sequence}
  \NormalTok{output[[i]] <-}\StringTok{ }\KeywordTok{median}\NormalTok{(df[[i]])      }\CommentTok{# 3. body}
\NormalTok{\}}
\NormalTok{output}
\CommentTok{#> [1] -0.2458 -0.2873 -0.0567  0.1443}
\end{Highlighting}
\end{Shaded}

Every for loop has three components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The \textbf{output}:
  \texttt{output\ \textless{}-\ vector("double",\ length(x))}. Before
  you start the loop, you must always allocate sufficient space for the
  output. This is very important for efficiency: if you grow the for
  loop at each iteration using \texttt{c()} (for example), your for loop
  will be very slow.

  A general way of creating an empty vector of given length is the
  \texttt{vector()} function. It has two arguments: the type of the
  vector (``logical'', ``integer'', ``double'', ``character'', etc) and
  the length of the vector.
\item
  The \textbf{sequence}: \texttt{i\ in\ seq\_along(df)}. This determines
  what to loop over: each run of the for loop will assign \texttt{i} to
  a different value from \texttt{seq\_along(df)}. It's useful to think
  of \texttt{i} as a pronoun, like ``it''.

  You might not have seen \texttt{seq\_along()} before. It's a safe
  version of the familiar \texttt{1:length(l)}, with an important
  difference: if you have a zero-length vector, \texttt{seq\_along()}
  does the right thing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"double"}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\KeywordTok{seq_along}\NormalTok{(y)}
\CommentTok{#> integer(0)}
\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(y)}
\CommentTok{#> [1] 1 0}
\end{Highlighting}
\end{Shaded}

  You probably won't create a zero-length vector deliberately, but it's
  easy to create them accidentally. If you use \texttt{1:length(x)}
  instead of \texttt{seq\_along(x)}, you're likely to get a confusing
  error message.
\item
  The \textbf{body}:
  \texttt{output{[}{[}i{]}{]}\ \textless{}-\ median(df{[}{[}i{]}{]})}.
  This is the code that does the work. It's run repeatedly, each time
  with a different value for \texttt{i}. The first iteration will run
  \texttt{output{[}{[}1{]}{]}\ \textless{}-\ median(df{[}{[}1{]}{]})},
  the second will run
  \texttt{output{[}{[}2{]}{]}\ \textless{}-\ median(df{[}{[}2{]}{]})},
  and so on.
\end{enumerate}

That's all there is to the for loop! Now is a good time to practice
creating some basic (and not so basic) for loops using the exercises
below. Then we'll move on some variations of the for loop that help you
solve other problems that will crop up in practice.

\subsection{Exercises}\label{exercises-55}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write for loops to:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Compute the mean of every column in \texttt{mtcars}.
  \item
    Determine the type of each column in \texttt{nycflights13::flights}.
  \item
    Compute the number of unique values in each column of \texttt{iris}.
  \item
    Generate 10 random normals for each of \(\mu = -10\), \(0\), \(10\),
    and \(100\).
  \end{enumerate}

  Think about the output, sequence, and body \textbf{before} you start
  writing the loop.
\item
  Eliminate the for loop in each of the following examples by taking
  advantage of an existing function that works with vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{ ""}
\NormalTok{for (x in letters) \{}
  \NormalTok{out <-}\StringTok{ }\NormalTok{stringr::}\KeywordTok{str_c}\NormalTok{(out, x)}
\NormalTok{\}}

\NormalTok{x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{sd <-}\StringTok{ }\DecValTok{0}
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(x)) \{}
  \NormalTok{sd <-}\StringTok{ }\NormalTok{sd +}\StringTok{ }\NormalTok{(x[i] -}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)) ^}\StringTok{ }\DecValTok{2}
\NormalTok{\}}
\NormalTok{sd <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(sd /}\StringTok{ }\NormalTok{(}\KeywordTok{length}\NormalTok{(x) -}\StringTok{ }\DecValTok{1}\NormalTok{))}

\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{out <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"numeric"}\NormalTok{, }\KeywordTok{length}\NormalTok{(x))}
\NormalTok{out[}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{x[}\DecValTok{1}\NormalTok{]}
\NormalTok{for (i in }\DecValTok{2}\NormalTok{:}\KeywordTok{length}\NormalTok{(x)) \{}
  \NormalTok{out[i] <-}\StringTok{ }\NormalTok{out[i -}\StringTok{ }\DecValTok{1}\NormalTok{] +}\StringTok{ }\NormalTok{x[i]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\item
  Combine your function writing and for loop skills:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Write a for loop that \texttt{prints()} the lyrics to the children's
    song ``Alice the camel''.
  \item
    Convert the nursery rhyme ``ten in the bed'' to a function.
    Generalise it to any number of people in any sleeping structure.
  \item
    Convert the song ``99 bottles of beer on the wall'' to a function.
    Generalise to any number of any vessel containing any liquid on any
    surface.
  \end{enumerate}
\item
  It's common to see for loops that don't preallocate the output and
  instead increase the length of a vector at each step:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{output <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"integer"}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(x)) \{}
  \NormalTok{output <-}\StringTok{ }\KeywordTok{c}\NormalTok{(output, }\KeywordTok{lengths}\NormalTok{(x[[i]]))}
\NormalTok{\}}
\NormalTok{output}
\end{Highlighting}
\end{Shaded}

  How does this affect performance? Design and execute an experiment.
\end{enumerate}

\section{For loop variations}\label{for-loop-variations}

Once you have the basic for loop under your belt, there are some
variations that you should be aware of. These variations are important
regardless of how you do iteration, so don't forget about them once
you've master the FP techniques you'll learn about in the next section.

There are four variations on the basic theme of the for loop:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Modifying an existing object, instead of creating a new object.
\item
  Looping over names or values, instead of indices.
\item
  Handling outputs of unknown length.
\item
  Handling sequences of unknown length.
\end{enumerate}

\subsection{Modifying an existing
object}\label{modifying-an-existing-object}

Sometimes you want to use a for loop to modify an existing object. For
example, remember our challenge from
\protect\hyperlink{functions}{functions}. We wanted to rescale every
column in a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{a =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{b =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{c =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{d =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{)}
\NormalTok{rescale01 <-}\StringTok{ }\NormalTok{function(x) \{}
  \NormalTok{rng <-}\StringTok{ }\KeywordTok{range}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
  \NormalTok{(x -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{]) /}\StringTok{ }\NormalTok{(rng[}\DecValTok{2}\NormalTok{] -}\StringTok{ }\NormalTok{rng[}\DecValTok{1}\NormalTok{])}
\NormalTok{\}}

\NormalTok{df$a <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$a)}
\NormalTok{df$b <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$b)}
\NormalTok{df$c <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$c)}
\NormalTok{df$d <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df$d)}
\end{Highlighting}
\end{Shaded}

To solve this with a for loop we again think about the three components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Output}: we already have the output --- it's the same as the
  input!
\item
  \textbf{Sequence}: we can think about a data frame as a list of
  columns, so we can iterate over each column with
  \texttt{seq\_along(df)}.
\item
  \textbf{Body}: apply \texttt{rescale01()}.
\end{enumerate}

This gives us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(df)) \{}
  \NormalTok{df[[i]] <-}\StringTok{ }\KeywordTok{rescale01}\NormalTok{(df[[i]])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Typically you'll be modifying a list or data frame with this sort of
loop, so remember to use \texttt{{[}{[}}, not \texttt{{[}}. You might
have spotted that I used \texttt{{[}{[}} in all my for loops: I think
it's better to use \texttt{{[}{[}} even for atomic vectors because it
makes it clear that I want to work with a single element.

\subsection{Looping patterns}\label{looping-patterns}

There are three basic ways to loop over a vector. So far I've shown you
the most general: looping over the numeric indices with
\texttt{for\ (i\ in\ seq\_along(xs))}, and extracting the value with
\texttt{x{[}{[}i{]}{]}}. There are two other forms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Loop over the elements: \texttt{for\ (x\ in\ xs)}. This is most useful
  if you only care about side-effects, like plotting or saving a file,
  because it's difficult to save the output efficiently.
\item
  Loop over the names: \texttt{for\ (nm\ in\ names(xs))}. This gives you
  name, which you can use to access the value with
  \texttt{x{[}{[}nm{]}{]}}. This is useful if you want to use the name
  in a plot title or a file name. If you're creating named output, make
  sure to name the results vector like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\KeywordTok{length}\NormalTok{(x))}
\KeywordTok{names}\NormalTok{(results) <-}\StringTok{ }\KeywordTok{names}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

Iteration over the numeric indices is the most general form, because
given the position you can extract both the name and the value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(x)) \{}
  \NormalTok{name <-}\StringTok{ }\KeywordTok{names}\NormalTok{(x)[[i]]}
  \NormalTok{value <-}\StringTok{ }\NormalTok{x[[i]]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Unknown output length}\label{unknown-output-length}

Sometimes you might not know how long the output will be. For example,
imagine you want to simulate some random vectors of random lengths. You
might be tempted to solve this problem by progressively growing the
vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{output <-}\StringTok{ }\KeywordTok{double}\NormalTok{()}
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(means)) \{}
  \NormalTok{n <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  \NormalTok{output <-}\StringTok{ }\KeywordTok{c}\NormalTok{(output, }\KeywordTok{rnorm}\NormalTok{(n, means[[i]]))}
\NormalTok{\}}
\KeywordTok{str}\NormalTok{(output)}
\CommentTok{#>  num [1:202] 0.912 0.205 2.584 -0.789 0.588 ...}
\end{Highlighting}
\end{Shaded}

But this is not very efficient because in each iteration, R has to copy
all the data from the previous iterations. In technical terms you get
``quadratic'' (\(O(n^2)\)) behaviour which means that a loop with three
times as many elements would take nine (\(3^2\)) times as long to run.

A better solution to save the results in a list, and then combine into a
single vector after the loop is done:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\KeywordTok{length}\NormalTok{(means))}
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(means)) \{}
  \NormalTok{n <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  \NormalTok{out[[i]] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, means[[i]])}
\NormalTok{\}}
\KeywordTok{str}\NormalTok{(out)}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num [1:83] 0.367 1.13 -0.941 0.218 1.415 ...}
\CommentTok{#>  $ : num [1:21] -0.485 -0.425 2.937 1.688 1.324 ...}
\CommentTok{#>  $ : num [1:40] 2.34 1.59 2.93 3.84 1.3 ...}
\KeywordTok{str}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(out))}
\CommentTok{#>  num [1:144] 0.367 1.13 -0.941 0.218 1.415 ...}
\end{Highlighting}
\end{Shaded}

Here I've used \texttt{unlist()} to flatten a list of vectors into a
single vector. A stricter option is to use
\texttt{purrr::flatten\_dbl()} --- it will throw an error if the input
isn't a list of doubles.

This pattern occurs in other places too:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You might be generating a long string. Instead of \texttt{paste()}ing
  together each iteration with the previous, save the output in a
  character vector and then combine that vector into a single string
  with \texttt{paste(output,\ collapse\ =\ "")}.
\item
  You might be generating a big data frame. Instead of sequentially
  \texttt{rbind()}ing in each iteration, save the output in a list, then
  use \texttt{dplyr::bind\_rows(output)} to combine the output into a
  single data frame.
\end{enumerate}

Watch out for this pattern. Whenever you see it, switch to a more
complex result object, and then combine in one step at the end.

\subsection{Unknown sequence length}\label{unknown-sequence-length}

Sometimes you don't even know how long the input sequence should run
for. This is common when doing simulations. For example, you might want
to loop until you get three heads in a row. You can't do that sort of
iteration with the for loop. Instead, you can use a while loop. A while
loop is simpler than for loop because it only has two components, a
condition and a body:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{while (condition) \{}
  \CommentTok{# body}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

A while loop is also more general than a for loop, because you can
rewrite any for loop as a while loop, but you can't rewrite every while
loop as a for loop:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(x)) \{}
  \CommentTok{# body}
\NormalTok{\}}

\CommentTok{# Equivalent to}
\NormalTok{i <-}\StringTok{ }\DecValTok{1}
\NormalTok{while (i <=}\StringTok{ }\KeywordTok{length}\NormalTok{(x)) \{}
  \CommentTok{# body}
  \NormalTok{i <-}\StringTok{ }\NormalTok{i +}\StringTok{ }\DecValTok{1} 
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Here's how we could use a while loop to find how many tries it takes to
get three heads in a row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flip <-}\StringTok{ }\NormalTok{function() }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"T"}\NormalTok{, }\StringTok{"H"}\NormalTok{), }\DecValTok{1}\NormalTok{)}

\NormalTok{flips <-}\StringTok{ }\DecValTok{0}
\NormalTok{nheads <-}\StringTok{ }\DecValTok{0}

\NormalTok{while (nheads <}\StringTok{ }\DecValTok{3}\NormalTok{) \{}
  \NormalTok{if (}\KeywordTok{flip}\NormalTok{() ==}\StringTok{ "H"}\NormalTok{) \{}
    \NormalTok{nheads <-}\StringTok{ }\NormalTok{nheads +}\StringTok{ }\DecValTok{1}
  \NormalTok{\} else \{}
    \NormalTok{nheads <-}\StringTok{ }\DecValTok{0}
  \NormalTok{\}}
  \NormalTok{flips <-}\StringTok{ }\NormalTok{flips +}\StringTok{ }\DecValTok{1}
\NormalTok{\}}
\NormalTok{flips}
\CommentTok{#> [1] 3}
\end{Highlighting}
\end{Shaded}

I mention while loops only briefly, because I hardly ever use them.
They're most often used for simulation, which is outside the scope of
this book. However, it is good to know they exist so that you're
prepared for problems where the number of iterations is not known in
advance.

\subsection{Exercises}\label{exercises-56}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Imagine you have a directory full of CSV files that you want to read
  in. You have their paths in a vector,
  \texttt{files\ \textless{}-\ dir("data/",\ pattern\ =\ "\textbackslash{}\textbackslash{}.csv\$",\ full.names\ =\ TRUE)},
  and now want to read each one with \texttt{read\_csv()}. Write the for
  loop that will load them into a single data frame.
\item
  What happens if you use \texttt{for\ (nm\ in\ names(x))} and
  \texttt{x} has no names? What if only some of the elements are named?
  What if the names are not unique?
\item
  Write a function that prints the mean of each numeric column in a data
  frame, along with its name. For example, \texttt{show\_mean(iris)}
  would print:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{show_mean}\NormalTok{(iris)}
\CommentTok{#> Sepal.Length: 5.84}
\CommentTok{#> Sepal.Width:  3.06}
\CommentTok{#> Petal.Length: 3.76}
\CommentTok{#> Petal.Width:  1.20}
\end{Highlighting}
\end{Shaded}

  (Extra challenge: what function did I use to make sure that the
  numbers lined up nicely, even though the variable names had different
  lengths?)
\item
  What does this code do? How does it work?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trans <-}\StringTok{ }\KeywordTok{list}\NormalTok{( }
  \DataTypeTok{disp =} \NormalTok{function(x) x *}\StringTok{ }\FloatTok{0.0163871}\NormalTok{,}
  \DataTypeTok{am =} \NormalTok{function(x) \{}
    \KeywordTok{factor}\NormalTok{(x, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"auto"}\NormalTok{, }\StringTok{"manual"}\NormalTok{))}
  \NormalTok{\}}
\NormalTok{)}
\NormalTok{for (var in }\KeywordTok{names}\NormalTok{(trans)) \{}
  \NormalTok{mtcars[[var]] <-}\StringTok{ }\NormalTok{trans[[var]](mtcars[[var]])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{For loops vs.~functionals}\label{for-loops-vs.functionals}

For loops are not as important in R as they are in other languages
because R is a functional programming language. This means that it's
possible to wrap up for loops in a function, and call that function
instead of using the for loop directly.

To see why this is important, consider (again) this simple data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{a =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{b =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{c =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{d =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Imagine you want to compute the mean of every column. You could do that
with a for loop:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{output <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"double"}\NormalTok{, }\KeywordTok{length}\NormalTok{(df))}
\NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(df)) \{}
  \NormalTok{output[[i]] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(df[[i]])}
\NormalTok{\}}
\NormalTok{output}
\CommentTok{#> [1]  0.2026 -0.2068  0.1275 -0.0917}
\end{Highlighting}
\end{Shaded}

You realise that you're going to want to compute the means of every
column pretty frequently, so you extract it out into a function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col_mean <-}\StringTok{ }\NormalTok{function(df) \{}
  \NormalTok{output <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"double"}\NormalTok{, }\KeywordTok{length}\NormalTok{(df))}
  \NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(df)) \{}
    \NormalTok{output[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(df[[i]])}
  \NormalTok{\}}
  \NormalTok{output}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

But then you think it'd also be helpful to be able to compute the
median, and the standard deviation, so you copy and paste your
\texttt{col\_mean()} function and replace the \texttt{mean()} with
\texttt{median()} and \texttt{sd()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col_median <-}\StringTok{ }\NormalTok{function(df) \{}
  \NormalTok{output <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"double"}\NormalTok{, }\KeywordTok{length}\NormalTok{(df))}
  \NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(df)) \{}
    \NormalTok{output[i] <-}\StringTok{ }\KeywordTok{median}\NormalTok{(df[[i]])}
  \NormalTok{\}}
  \NormalTok{output}
\NormalTok{\}}
\NormalTok{col_sd <-}\StringTok{ }\NormalTok{function(df) \{}
  \NormalTok{output <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"double"}\NormalTok{, }\KeywordTok{length}\NormalTok{(df))}
  \NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(df)) \{}
    \NormalTok{output[i] <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(df[[i]])}
  \NormalTok{\}}
  \NormalTok{output}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Uh oh! You've copied-and-pasted this code twice, so it's time to think
about how to generalise it. Notice that most of this code is for-loop
boilerplate and it's hard to see the one thing (\texttt{mean()},
\texttt{median()}, \texttt{sd()}) that is different between the
functions.

What would you do if you saw a set of functions like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f1 <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{abs}\NormalTok{(x -}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)) ^}\StringTok{ }\DecValTok{1}
\NormalTok{f2 <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{abs}\NormalTok{(x -}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)) ^}\StringTok{ }\DecValTok{2}
\NormalTok{f3 <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{abs}\NormalTok{(x -}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)) ^}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

Hopefully, you'd notice that there's a lot of duplication, and extract
it out into an additional argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\NormalTok{function(x, i) }\KeywordTok{abs}\NormalTok{(x -}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)) ^}\StringTok{ }\NormalTok{i}
\end{Highlighting}
\end{Shaded}

You've reduced the chance of bugs (because you now have 1/3 less code),
and made it easy to generalise to new situations.

We can do exactly the same thing with \texttt{col\_mean()},
\texttt{col\_median()} and \texttt{col\_sd()} by adding an argument that
supplies the function to apply to each column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col_summary <-}\StringTok{ }\NormalTok{function(df, fun) \{}
  \NormalTok{out <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"double"}\NormalTok{, }\KeywordTok{length}\NormalTok{(df))}
  \NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(df)) \{}
    \NormalTok{out[i] <-}\StringTok{ }\KeywordTok{fun}\NormalTok{(df[[i]])}
  \NormalTok{\}}
  \NormalTok{out}
\NormalTok{\}}
\KeywordTok{col_summary}\NormalTok{(df, median)}
\CommentTok{#> [1]  0.237 -0.218  0.254 -0.133}
\KeywordTok{col_summary}\NormalTok{(df, mean)}
\CommentTok{#> [1]  0.2026 -0.2068  0.1275 -0.0917}
\end{Highlighting}
\end{Shaded}

The idea of passing a function to another function is extremely powerful
idea, and it's one of the behaviours that makes R a functional
programming language. It might take you a while to wrap your head around
the idea, but it's worth the investment. In the rest of the chapter,
you'll learn about and use the \textbf{purrr} package, which provides
functions that eliminate the need for many common for loops. The apply
family of functions in base R (\texttt{apply()}, \texttt{lapply()},
\texttt{tapply()}, etc) solve a similar problem, but purrr is more
consistent and thus is easier to learn.

The goal of using purrr functions instead of for loops is to allow you
break common list manipulation challenges into independent pieces:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How can you solve the problem for a single element of the list? Once
  you've solved that problem, purrr takes care of generalising your
  solution to every element in the list.
\item
  If you're solving a complex problem, how can you break it down into
  bite-sized pieces that allow you to advance one small step towards a
  solution? With purrr, you get lots of small pieces that you can
  compose together with the pipe.
\end{enumerate}

This structure makes it easier to solve new problems. It also makes it
easier to understand your solutions to old problems when you re-read
your old code.

\subsection{Exercises}\label{exercises-57}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Read the documentation for \texttt{apply()}. In the 2d case, what two
  for loops does it generalise?
\item
  Adapt \texttt{col\_summary()} so that it only applies to numeric
  columns You might want to start with an \texttt{is\_numeric()}
  function that returns a logical vector that has a TRUE corresponding
  to each numeric column.
\end{enumerate}

\section{The map functions}\label{the-map-functions}

The pattern of looping over a vector, doing something to each element
and saving the results is so common that the purrr package provides a
family of functions to do it for you. There is one function for each
type of output:

\begin{itemize}
\tightlist
\item
  \texttt{map()} makes a list.
\item
  \texttt{map\_lgl()} makes a logical vector.
\item
  \texttt{map\_int()} makes an integer vector.
\item
  \texttt{map\_dbl()} makes a double vector.
\item
  \texttt{map\_chr()} makes a character vector.
\end{itemize}

Each function takes a vector as input, applies a function to each piece,
and then returns a new vector that's the same length (and has the same
names) as the input. The type of the vector is determined by the suffix
to the map function.

Once you master these functions, you'll find it takes much less time to
solve iteration problems. But you should never feel bad about using a
for loop instead of a map function. The map functions are a step up a
tower of abstraction, and it can take a long time to get your head
around how they work. The important thing is that you solve the problem
that you're working on, not write the most concise and elegant code
(although that's definitely something you want to strive towards!).

Some people will tell you to avoid for loops because they are slow.
They're wrong! (Well at least they're rather out of date, as for loops
haven't been slow for many years). The chief benefits of using functions
like \texttt{map()} is not speed, but clarity: they make your code
easier to write and to read.

We can use these functions to perform the same computations as the last
for loop. Those summary functions returned doubles, so we need to use
\texttt{map\_dbl()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{map_dbl}\NormalTok{(df, mean)}
\CommentTok{#>       a       b       c       d }
\CommentTok{#>  0.2026 -0.2068  0.1275 -0.0917}
\KeywordTok{map_dbl}\NormalTok{(df, median)}
\CommentTok{#>      a      b      c      d }
\CommentTok{#>  0.237 -0.218  0.254 -0.133}
\KeywordTok{map_dbl}\NormalTok{(df, sd)}
\CommentTok{#>     a     b     c     d }
\CommentTok{#> 0.796 0.759 1.164 1.062}
\end{Highlighting}
\end{Shaded}

Compared to using a for loop, focus is on the operation being performed
(i.e. \texttt{mean()}, \texttt{median()}, \texttt{sd()}), not the
bookkeeping required to loop over every element and store the output.
This is even more apparent if we use the pipe:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df %>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(mean)}
\CommentTok{#>       a       b       c       d }
\CommentTok{#>  0.2026 -0.2068  0.1275 -0.0917}
\NormalTok{df %>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(median)}
\CommentTok{#>      a      b      c      d }
\CommentTok{#>  0.237 -0.218  0.254 -0.133}
\NormalTok{df %>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(sd)}
\CommentTok{#>     a     b     c     d }
\CommentTok{#> 0.796 0.759 1.164 1.062}
\end{Highlighting}
\end{Shaded}

There are a few differences between \texttt{map\_*()} and
\texttt{col\_summary()}:

\begin{itemize}
\item
  All purrr functions are implemented in C. This makes them a little
  faster at the expense of readability.
\item
  The second argument, \texttt{.f}, the function to apply, can be a
  formula, a character vector, or an integer vector. You'll learn about
  those handy shortcuts in the next section.
\item
  \texttt{map\_*()} uses \ldots{} ({[}dot dot dot{]}) to pass along
  additional arguments to \texttt{.f} each time it's called:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{map_dbl}\NormalTok{(df, mean, }\DataTypeTok{trim =} \FloatTok{0.5}\NormalTok{)}
\CommentTok{#>      a      b      c      d }
\CommentTok{#>  0.237 -0.218  0.254 -0.133}
\end{Highlighting}
\end{Shaded}
\item
  The map functions also preserve names:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{y =} \DecValTok{4}\NormalTok{:}\DecValTok{5}\NormalTok{)}
\KeywordTok{map_int}\NormalTok{(z, length)}
\CommentTok{#> x y }
\CommentTok{#> 3 2}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsection{Shortcuts}\label{shortcuts}

There are a few shortcuts that you can use with \texttt{.f} in order to
save a little typing. Imagine you want to fit a linear model to each
group in a dataset. The following toy example splits the up the
\texttt{mtcars} dataset in to three pieces (one for each value of
cylinder) and fits the same linear model to each piece:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{split}\NormalTok{(.$cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map}\NormalTok{(function(df) }\KeywordTok{lm}\NormalTok{(mpg ~}\StringTok{ }\NormalTok{wt, }\DataTypeTok{data =} \NormalTok{df))}
\end{Highlighting}
\end{Shaded}

The syntax for creating an anonymous function in R is quite verbose so
purrr provides a convenient shortcut: a one-sided formula.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{split}\NormalTok{(.$cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map}\NormalTok{(~}\KeywordTok{lm}\NormalTok{(mpg ~}\StringTok{ }\NormalTok{wt, }\DataTypeTok{data =} \NormalTok{.))}
\end{Highlighting}
\end{Shaded}

Here I've used \texttt{.} as a pronoun: it refers to the current list
element (in the same way that \texttt{i} referred to the current index
in the for loop).

When you're looking at many models, you might want to extract a summary
statistic like the \(R^2\). To do that we need to first run
\texttt{summary()} and then extract the component called
\texttt{r.squared}. We could do that using the shorthand for anonymous
functions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map}\NormalTok{(summary) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map_dbl}\NormalTok{(~.$r.squared)}
\CommentTok{#>     4     6     8 }
\CommentTok{#> 0.509 0.465 0.423}
\end{Highlighting}
\end{Shaded}

But extracting named components is a common operation, so purrr provides
an even shorter shortcut: you can use a string.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map}\NormalTok{(summary) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map_dbl}\NormalTok{(}\StringTok{"r.squared"}\NormalTok{)}
\CommentTok{#>     4     6     8 }
\CommentTok{#> 0.509 0.465 0.423}
\end{Highlighting}
\end{Shaded}

You can also use an integer to select elements by position:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{), }\KeywordTok{list}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\KeywordTok{list}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{))}
\NormalTok{x %>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\CommentTok{#> [1] 2 5 8}
\end{Highlighting}
\end{Shaded}

\subsection{Base R}\label{base-r}

If you're familiar with the apply family of functions in base R, you
might have noticed some similarities with the purrr functions:

\begin{itemize}
\item
  \texttt{lapply()} is basically identical to \texttt{map()}, except
  that \texttt{map()} is consistent with all the other functions in
  purrr, and you can use the shortcuts for \texttt{.f}.
\item
  Base \texttt{sapply()} is a wrapper around \texttt{lapply()} that
  automatically simplifies the output. This is useful for interactive
  work but is problematic in a function because you never know what sort
  of output you'll get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{c}\NormalTok{(}\FloatTok{0.27}\NormalTok{, }\FloatTok{0.37}\NormalTok{, }\FloatTok{0.57}\NormalTok{, }\FloatTok{0.91}\NormalTok{, }\FloatTok{0.20}\NormalTok{),}
  \KeywordTok{c}\NormalTok{(}\FloatTok{0.90}\NormalTok{, }\FloatTok{0.94}\NormalTok{, }\FloatTok{0.66}\NormalTok{, }\FloatTok{0.63}\NormalTok{, }\FloatTok{0.06}\NormalTok{), }
  \KeywordTok{c}\NormalTok{(}\FloatTok{0.21}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.69}\NormalTok{, }\FloatTok{0.38}\NormalTok{, }\FloatTok{0.77}\NormalTok{)}
\NormalTok{)}
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{c}\NormalTok{(}\FloatTok{0.50}\NormalTok{, }\FloatTok{0.72}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\FloatTok{0.38}\NormalTok{, }\FloatTok{0.78}\NormalTok{), }
  \KeywordTok{c}\NormalTok{(}\FloatTok{0.93}\NormalTok{, }\FloatTok{0.21}\NormalTok{, }\FloatTok{0.65}\NormalTok{, }\FloatTok{0.13}\NormalTok{, }\FloatTok{0.27}\NormalTok{), }
  \KeywordTok{c}\NormalTok{(}\FloatTok{0.39}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.38}\NormalTok{, }\FloatTok{0.87}\NormalTok{, }\FloatTok{0.34}\NormalTok{)}
\NormalTok{)}

\NormalTok{threshold <-}\StringTok{ }\NormalTok{function(x, }\DataTypeTok{cutoff =} \FloatTok{0.8}\NormalTok{) x[x >}\StringTok{ }\NormalTok{cutoff]}
\NormalTok{x1 %>%}\StringTok{ }\KeywordTok{sapply}\NormalTok{(threshold) %>%}\StringTok{ }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num 0.91}
\CommentTok{#>  $ : num [1:2] 0.9 0.94}
\CommentTok{#>  $ : num(0)}
\NormalTok{x2 %>%}\StringTok{ }\KeywordTok{sapply}\NormalTok{(threshold) %>%}\StringTok{ }\KeywordTok{str}\NormalTok{()}
\CommentTok{#>  num [1:3] 0.99 0.93 0.87}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{vapply()} is a safe alternative to \texttt{sapply()} because
  you supply an additional argument that defines the type. The only
  problem with \texttt{vapply()} is that it's a lot of typing:
  \texttt{vapply(df,\ is.numeric,\ logical(1))} is equivalent to
  \texttt{map\_lgl(df,\ is.numeric)}. One of advantage of
  \texttt{vapply()} over purrr's map functions is that it can also
  produce matrices --- the map functions only ever produce vectors.
\end{itemize}

I focus on purrr functions here because they have more consistent names
and arguments, helpful shortcuts, and in the future will provide easy
parallelism and progress bars.

\subsection{Exercises}\label{exercises-58}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write code that uses one of the map functions to:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Compute the mean of every column in \texttt{mtcars}.
  \item
    Determine the type of each column in \texttt{nycflights13::flights}.
  \item
    Compute the number of unique values in each column of \texttt{iris}.
  \item
    Generate 10 random normals for each of \(\mu = -10\), \(0\), \(10\),
    and \(100\).
  \end{enumerate}
\item
  How can you create a single vector that for each column in a data
  frame indicates whether or not it's a factor?
\item
  What happens when you use the map functions on vectors that aren't
  lists? What does \texttt{map(1:5,\ runif)} do? Why?
\item
  What does \texttt{map(-2:2,\ rnorm,\ n\ =\ 5)} do? Why? What does
  \texttt{map\_dbl(-2:2,\ rnorm,\ n\ =\ 5)} do? Why?
\item
  Rewrite
  \texttt{map(x,\ function(df)\ lm(mpg\ \textasciitilde{}\ wt,\ data\ =\ df))}
  to eliminate the anonymous function.
\end{enumerate}

\section{Dealing with failure}\label{dealing-with-failure}

When you use the map functions to repeat many operations, the chances
are much higher that one of those operations will fail. When this
happens, you'll get an error message, and no output. This is annoying:
why does one failure prevent you from accessing all the other successes?
How do you ensure that one bad apple doesn't ruin the whole barrel?

In this section you'll learn how to deal this situation with a new
function: \texttt{safely()}. \texttt{safely()} is an adverb: it takes a
function (a verb) and returns a modified version. In this case, the
modified function will never throw an error. Instead, it always returns
a list with two elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{result} is the original result. If there was an error, this
  will be \texttt{NULL}.
\item
  \texttt{error} is an error object. If the operation was successful,
  this will be \texttt{NULL}.
\end{enumerate}

(You might be familiar with the \texttt{try()} function in base R. It's
similar, but because it sometimes returns the original result and it
sometimes returns an error object it's more difficult to work with.)

Let's illustrate this with a simple example: \texttt{log()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{safe_log <-}\StringTok{ }\KeywordTok{safely}\NormalTok{(log)}
\KeywordTok{str}\NormalTok{(}\KeywordTok{safe_log}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\CommentTok{#> List of 2}
\CommentTok{#>  $ result: num 2.3}
\CommentTok{#>  $ error : NULL}
\KeywordTok{str}\NormalTok{(}\KeywordTok{safe_log}\NormalTok{(}\StringTok{"a"}\NormalTok{))}
\CommentTok{#> List of 2}
\CommentTok{#>  $ result: NULL}
\CommentTok{#>  $ error :List of 2}
\CommentTok{#>   ..$ message: chr "non-numeric argument to mathematical function"}
\CommentTok{#>   ..$ call   : language .f(...)}
\CommentTok{#>   ..- attr(*, "class")= chr [1:3] "simpleError" "error" "condition"}
\end{Highlighting}
\end{Shaded}

When the function succeeds, the \texttt{result} element contains the
result and the \texttt{error} element is \texttt{NULL}. When the
function fails, the \texttt{result} element is \texttt{NULL} and the
\texttt{error} element contains an error object.

\texttt{safely()} is designed to work with map:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\StringTok{"a"}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\NormalTok{x %>%}\StringTok{ }\KeywordTok{map}\NormalTok{(}\KeywordTok{safely}\NormalTok{(log))}
\KeywordTok{str}\NormalTok{(y)}
\CommentTok{#> List of 3}
\CommentTok{#>  $ :List of 2}
\CommentTok{#>   ..$ result: num 0}
\CommentTok{#>   ..$ error : NULL}
\CommentTok{#>  $ :List of 2}
\CommentTok{#>   ..$ result: num 2.3}
\CommentTok{#>   ..$ error : NULL}
\CommentTok{#>  $ :List of 2}
\CommentTok{#>   ..$ result: NULL}
\CommentTok{#>   ..$ error :List of 2}
\CommentTok{#>   .. ..$ message: chr "non-numeric argument to mathematical function"}
\CommentTok{#>   .. ..$ call   : language .f(...)}
\CommentTok{#>   .. ..- attr(*, "class")= chr [1:3] "simpleError" "error" "condition"}
\end{Highlighting}
\end{Shaded}

This would be easier to work with if we had two lists: one of all the
errors and one of all the output. That's easy to get with
\texttt{purrr::transpose()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\NormalTok{y %>%}\StringTok{ }\KeywordTok{transpose}\NormalTok{()}
\KeywordTok{str}\NormalTok{(y)}
\CommentTok{#> List of 2}
\CommentTok{#>  $ result:List of 3}
\CommentTok{#>   ..$ : num 0}
\CommentTok{#>   ..$ : num 2.3}
\CommentTok{#>   ..$ : NULL}
\CommentTok{#>  $ error :List of 3}
\CommentTok{#>   ..$ : NULL}
\CommentTok{#>   ..$ : NULL}
\CommentTok{#>   ..$ :List of 2}
\CommentTok{#>   .. ..$ message: chr "non-numeric argument to mathematical function"}
\CommentTok{#>   .. ..$ call   : language .f(...)}
\CommentTok{#>   .. ..- attr(*, "class")= chr [1:3] "simpleError" "error" "condition"}
\end{Highlighting}
\end{Shaded}

It's up to you how to deal with the errors, but typically you'll either
look at the values of \texttt{x} where \texttt{y} is an error, or work
with the values of \texttt{y} that are ok:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{is_ok <-}\StringTok{ }\NormalTok{y$error %>%}\StringTok{ }\KeywordTok{map_lgl}\NormalTok{(is_null)}
\NormalTok{x[!is_ok]}
\CommentTok{#> [[1]]}
\CommentTok{#> [1] "a"}
\NormalTok{y$result[is_ok] %>%}\StringTok{ }\KeywordTok{flatten_dbl}\NormalTok{()}
\CommentTok{#> [1] 0.0 2.3}
\end{Highlighting}
\end{Shaded}

Purrr provides two other useful adverbs:

\begin{itemize}
\item
  Like \texttt{safely()}, \texttt{possibly()} always succeeds. It's
  simpler than \texttt{safely()}, because you give it a default value to
  return when there is an error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\StringTok{"a"}\NormalTok{)}
\NormalTok{x %>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(}\KeywordTok{possibly}\NormalTok{(log, }\OtherTok{NA_real_}\NormalTok{))}
\CommentTok{#> [1] 0.0 2.3  NA}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{quietly()} performs a similar role to \texttt{safely()}, but
  instead of capturing errors, it captures printed output, messages, and
  warnings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, -}\DecValTok{1}\NormalTok{)}
\NormalTok{x %>%}\StringTok{ }\KeywordTok{map}\NormalTok{(}\KeywordTok{quietly}\NormalTok{(log)) %>%}\StringTok{ }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> List of 2}
\CommentTok{#>  $ :List of 4}
\CommentTok{#>   ..$ result  : num 0}
\CommentTok{#>   ..$ output  : chr ""}
\CommentTok{#>   ..$ warnings: chr(0) }
\CommentTok{#>   ..$ messages: chr(0) }
\CommentTok{#>  $ :List of 4}
\CommentTok{#>   ..$ result  : num NaN}
\CommentTok{#>   ..$ output  : chr ""}
\CommentTok{#>   ..$ warnings: chr "NaNs produced"}
\CommentTok{#>   ..$ messages: chr(0)}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\section{Mapping over multiple
arguments}\label{mapping-over-multiple-arguments}

So far we've mapped along a single input. But often you have multiple
related inputs that you need iterate along in parallel. That's the job
of the \texttt{map2()} and \texttt{pmap()} functions. For example,
imagine you want to simulate some random normals with different means.
You know how to do that with \texttt{map()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, -}\DecValTok{3}\NormalTok{)}
\NormalTok{mu %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map}\NormalTok{(rnorm, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num [1:5] 5.45 5.5 5.78 6.51 3.18}
\CommentTok{#>  $ : num [1:5] 10.79 9.03 10.89 10.76 10.65}
\CommentTok{#>  $ : num [1:5] -3.54 -3.08 -5.01 -3.51 -2.9}
\end{Highlighting}
\end{Shaded}

What if you also want to vary the standard deviation? One way to do that
would be to iterate over the indices and index into vectors of means and
sds:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\KeywordTok{seq_along}\NormalTok{(mu) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map}\NormalTok{(~}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, mu[[.]], sigma[[.]])) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num [1:5] 4.94 2.57 4.37 4.12 5.29}
\CommentTok{#>  $ : num [1:5] 11.72 5.32 11.46 10.24 12.22}
\CommentTok{#>  $ : num [1:5] 3.68 -6.12 22.24 -7.2 10.37}
\end{Highlighting}
\end{Shaded}

But that obfuscates the intent of the code. Instead we could use
\texttt{map2()} which iterates over two vectors in parallel:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{map2}\NormalTok{(mu, sigma, rnorm, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{) %>%}\StringTok{ }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num [1:5] 4.78 5.59 4.93 4.3 4.47}
\CommentTok{#>  $ : num [1:5] 10.85 10.57 6.02 8.82 15.93}
\CommentTok{#>  $ : num [1:5] -1.12 7.39 -7.5 -10.09 -2.7}
\end{Highlighting}
\end{Shaded}

\texttt{map2()} generates this series of function calls:

\begin{center}\includegraphics[width=0.7\linewidth]{diagrams/lists-map2} \end{center}

Note that the arguments that vary for each call come \emph{before} the
function; arguments that are the same for every call come \emph{after}.

Like \texttt{map()}, \texttt{map2()} is just a wrapper around a for
loop:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{map2 <-}\StringTok{ }\NormalTok{function(x, y, f, ...) \{}
  \NormalTok{out <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\KeywordTok{length}\NormalTok{(x))}
  \NormalTok{for (i in }\KeywordTok{seq_along}\NormalTok{(x)) \{}
    \NormalTok{out[[i]] <-}\StringTok{ }\KeywordTok{f}\NormalTok{(x[[i]], y[[i]], ...)}
  \NormalTok{\}}
  \NormalTok{out}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You could also imagine \texttt{map3()}, \texttt{map4()},
\texttt{map5()}, \texttt{map6()} etc, but that would get tedious
quickly. Instead, purrr provides \texttt{pmap()} which takes a list of
arguments. You might use that if you wanted to vary the mean, standard
deviation, and number of samples:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{args1 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(n, mu, sigma)}
\NormalTok{args1 %>%}
\StringTok{  }\KeywordTok{pmap}\NormalTok{(rnorm) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num 4.55}
\CommentTok{#>  $ : num [1:3] 13.4 18.8 13.2}
\CommentTok{#>  $ : num [1:5] 0.685 10.801 -11.671 21.363 -2.562}
\end{Highlighting}
\end{Shaded}

That looks like:

\begin{center}\includegraphics[width=0.7\linewidth]{diagrams/lists-pmap-unnamed} \end{center}

If you don't name the elements of list, \texttt{pmap()} will use
positional matching when calling the function. That's a little fragile,
and makes the code harder to read, so it's better to name the arguments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{args2 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =} \NormalTok{mu, }\DataTypeTok{sd =} \NormalTok{sigma, }\DataTypeTok{n =} \NormalTok{n)}
\NormalTok{args2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{pmap}\NormalTok{(rnorm) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

That generates longer, but safer, calls:

\begin{center}\includegraphics[width=0.7\linewidth]{diagrams/lists-pmap-named} \end{center}

Since the arguments are all the same length, it makes sense to store
them in a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~mean, ~sd, ~n,}
    \DecValTok{5}\NormalTok{,     }\DecValTok{1}\NormalTok{,  }\DecValTok{1}\NormalTok{,}
   \DecValTok{10}\NormalTok{,     }\DecValTok{5}\NormalTok{,  }\DecValTok{3}\NormalTok{,}
   \NormalTok{-}\DecValTok{3}\NormalTok{,    }\DecValTok{10}\NormalTok{,  }\DecValTok{5}
\NormalTok{)}
\NormalTok{params %>%}\StringTok{ }
\StringTok{  }\KeywordTok{pmap}\NormalTok{(rnorm)}
\CommentTok{#> [[1]]}
\CommentTok{#> [1] 4.68}
\CommentTok{#> }
\CommentTok{#> [[2]]}
\CommentTok{#> [1] 23.44 12.85  7.28}
\CommentTok{#> }
\CommentTok{#> [[3]]}
\CommentTok{#> [1]  -5.34 -17.66   0.92   6.06   9.02}
\end{Highlighting}
\end{Shaded}

As soon as your code gets complicated, I think a data frame is a good
approach because it ensures that each column has a name and is the same
length as all the other columns.

\hypertarget{invoking-different-functions}{\subsection{Invoking
different functions}\label{invoking-different-functions}}

There's one more step up in complexity - as well as varying the
arguments to the function you might also vary the function itself:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"runif"}\NormalTok{, }\StringTok{"rnorm"}\NormalTok{, }\StringTok{"rpois"}\NormalTok{)}
\NormalTok{param <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{min =} \NormalTok{-}\DecValTok{1}\NormalTok{, }\DataTypeTok{max =} \DecValTok{1}\NormalTok{), }
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{sd =} \DecValTok{5}\NormalTok{), }
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{lambda =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To handle this case, you can use \texttt{invoke\_map()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{invoke_map}\NormalTok{(f, param, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{) %>%}\StringTok{ }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> List of 3}
\CommentTok{#>  $ : num [1:5] 0.762 0.36 -0.714 0.531 0.254}
\CommentTok{#>  $ : num [1:5] 3.07 -3.09 1.1 5.64 9.07}
\CommentTok{#>  $ : int [1:5] 9 14 8 9 7}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{diagrams/lists-invoke} \end{center}

The first argument is a list of functions or character vector of
function names. The second argument is a list of lists giving the
arguments that vary for each function. The subsequent arguments are
passed on to every function.

And again, you can use \texttt{tribble()} to make creating these
matching pairs a little easier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~f,      ~params,}
  \StringTok{"runif"}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{min =} \NormalTok{-}\DecValTok{1}\NormalTok{, }\DataTypeTok{max =} \DecValTok{1}\NormalTok{),}
  \StringTok{"rnorm"}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{sd =} \DecValTok{5}\NormalTok{),}
  \StringTok{"rpois"}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{lambda =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}
\NormalTok{sim %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sim =} \KeywordTok{invoke_map}\NormalTok{(f, params, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\section{Walk}\label{walk}

Walk is an alternative to map that you use when you want to call a
function for its side effects, rather than for its return value. You
typically do this because you want to render output to the screen or
save files to disk - the important thing is the action, not the return
value. Here's a very simple example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"a"}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{walk}\NormalTok{(print)}
\CommentTok{#> [1] 1}
\CommentTok{#> [1] "a"}
\CommentTok{#> [1] 3}
\end{Highlighting}
\end{Shaded}

\texttt{walk()} is generally not that useful compared to
\texttt{walk2()} or \texttt{pwalk()}. For example, if you had a list of
plots and a vector of file names, you could use \texttt{pwalk()} to save
each file to the corresponding location on disk:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{plots <-}\StringTok{ }\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{split}\NormalTok{(.$cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{map}\NormalTok{(~}\KeywordTok{ggplot}\NormalTok{(., }\KeywordTok{aes}\NormalTok{(mpg, wt)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{())}
\NormalTok{paths <-}\StringTok{ }\NormalTok{stringr::}\KeywordTok{str_c}\NormalTok{(}\KeywordTok{names}\NormalTok{(plots), }\StringTok{".pdf"}\NormalTok{)}

\KeywordTok{pwalk}\NormalTok{(}\KeywordTok{list}\NormalTok{(paths, plots), ggsave, }\DataTypeTok{path =} \KeywordTok{tempdir}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\texttt{walk()}, \texttt{walk2()} and \texttt{pwalk()} all invisibly
return \texttt{.x}, the first argument. This makes them suitable for use
in the middle of pipelines.

\section{Other patterns of for loops}\label{other-patterns-of-for-loops}

Purrr provides a number of other functions that abstract over other
types of for loops. You'll use them less frequently than the map
functions, but they're useful to know about. The goal here is to briefly
illustrate each function, so hopefully it will come to mind if you see a
similar problem in the future. Then you can go look up the documentation
for more details.

\subsection{Predicate functions}\label{predicate-functions}

A number of functions work with \textbf{predicate} functions that return
either a single \texttt{TRUE} or \texttt{FALSE}.

\texttt{keep()} and \texttt{discard()} keep elements of the input where
the predicate is \texttt{TRUE} or \texttt{FALSE} respectively:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris %>%}\StringTok{ }
\StringTok{  }\KeywordTok{keep}\NormalTok{(is.factor) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> 'data.frame':    150 obs. of  1 variable:}
\CommentTok{#>  $ Species: Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...}

\NormalTok{iris %>%}\StringTok{ }
\StringTok{  }\KeywordTok{discard}\NormalTok{(is.factor) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{str}\NormalTok{()}
\CommentTok{#> 'data.frame':    150 obs. of  4 variables:}
\CommentTok{#>  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...}
\CommentTok{#>  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...}
\CommentTok{#>  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...}
\CommentTok{#>  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...}
\end{Highlighting}
\end{Shaded}

\texttt{some()} and \texttt{every()} determine if the predicate is true
for any or for all of the elements.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{, letters, }\KeywordTok{list}\NormalTok{(}\DecValTok{10}\NormalTok{))}

\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{some}\NormalTok{(is_character)}
\CommentTok{#> [1] TRUE}

\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{every}\NormalTok{(is_vector)}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}

\texttt{detect()} finds the first element where the predicate is true;
\texttt{detect\_index()} returns its position.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{x}
\CommentTok{#>  [1]  8  7  5  6  9  2 10  1  3  4}

\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{detect}\NormalTok{(~}\StringTok{ }\NormalTok{. >}\StringTok{ }\DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] 8}

\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{detect_index}\NormalTok{(~}\StringTok{ }\NormalTok{. >}\StringTok{ }\DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

\texttt{head\_while()} and \texttt{tail\_while()} take elements from the
start or end of a vector while a predicate is true:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{head_while}\NormalTok{(~}\StringTok{ }\NormalTok{. >}\StringTok{ }\DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] 8 7}

\NormalTok{x %>%}\StringTok{ }
\StringTok{  }\KeywordTok{tail_while}\NormalTok{(~}\StringTok{ }\NormalTok{. >}\StringTok{ }\DecValTok{5}\NormalTok{)}
\CommentTok{#> integer(0)}
\end{Highlighting}
\end{Shaded}

\subsection{Reduce and accumulate}\label{reduce-and-accumulate}

Sometimes you have a complex list that you want to reduce to a simple
list by repeatedly applying a function that reduces a pair to a
singleton. This is useful if you want to apply a two-table dplyr verb to
multiple tables. For example, you might have a list of data frames, and
you want to reduce to a single data frame by joining the elements
together:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfs <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{age =} \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{name =} \StringTok{"John"}\NormalTok{, }\DataTypeTok{age =} \DecValTok{30}\NormalTok{),}
  \DataTypeTok{sex =} \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{name =} \KeywordTok{c}\NormalTok{(}\StringTok{"John"}\NormalTok{, }\StringTok{"Mary"}\NormalTok{), }\DataTypeTok{sex =} \KeywordTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{)),}
  \DataTypeTok{trt =} \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Mary"}\NormalTok{, }\DataTypeTok{treatment =} \StringTok{"A"}\NormalTok{)}
\NormalTok{)}

\NormalTok{dfs %>%}\StringTok{ }\KeywordTok{reduce}\NormalTok{(full_join)}
\CommentTok{#> Joining, by = "name"}
\CommentTok{#> Joining, by = "name"}
\CommentTok{#> # A tibble: 2 × 4}
\CommentTok{#>    name   age   sex treatment}
\CommentTok{#>   <chr> <dbl> <chr>     <chr>}
\CommentTok{#> 1  John    30     M      <NA>}
\CommentTok{#> 2  Mary    NA     F         A}
\end{Highlighting}
\end{Shaded}

Or maybe you have a list of vectors, and want to find the intersection:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vs <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{10}\NormalTok{),}
  \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{),}
  \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{)}

\NormalTok{vs %>%}\StringTok{ }\KeywordTok{reduce}\NormalTok{(intersect)}
\CommentTok{#> [1]  1  3 10}
\end{Highlighting}
\end{Shaded}

The reduce function takes a ``binary'' function (i.e.~a function with
two primary inputs), and applies it repeatedly to a list until there is
only a single element left.

Accumulate is similar but it keeps all the interim results. You could
use it to implement a cumulative sum:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{x}
\CommentTok{#>  [1]  6  9  8  5  2  4  7  1 10  3}
\NormalTok{x %>%}\StringTok{ }\KeywordTok{accumulate}\NormalTok{(}\StringTok{`}\DataTypeTok{+}\StringTok{`}\NormalTok{)}
\CommentTok{#>  [1]  6 15 23 28 30 34 41 42 52 55}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-59}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Implement your own version of \texttt{every()} using a for loop.
  Compare it with \texttt{purrr::every()}. What does purrr's version do
  that your version doesn't?
\item
  Create an enhanced \texttt{col\_sum()} that applies a summary function
  to every numeric column in a data frame.
\item
  A possible base R equivalent of \texttt{col\_sum()} is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col_sum3 <-}\StringTok{ }\NormalTok{function(df, f) \{}
  \NormalTok{is_num <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(df, is.numeric)}
  \NormalTok{df_num <-}\StringTok{ }\NormalTok{df[, is_num]}

  \KeywordTok{sapply}\NormalTok{(df_num, f)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

  But it has a number of bugs as illustrated with the following inputs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }
  \DataTypeTok{y =} \DecValTok{3}\NormalTok{:}\DecValTok{1}\NormalTok{,}
  \DataTypeTok{z =} \KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\NormalTok{)}
\CommentTok{# OK}
\KeywordTok{col_sum3}\NormalTok{(df, mean)}
\CommentTok{# Has problems: don't always return numeric vector}
\KeywordTok{col_sum3}\NormalTok{(df[}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{], mean)}
\KeywordTok{col_sum3}\NormalTok{(df[}\DecValTok{1}\NormalTok{], mean)}
\KeywordTok{col_sum3}\NormalTok{(df[}\DecValTok{0}\NormalTok{], mean)}
\end{Highlighting}
\end{Shaded}

  What causes the bugs?
\end{enumerate}

\part{Model}\label{part-model}


\hypertarget{model-intro}{\chapter{Introduction}\label{model-intro}}

Now that you are equipped with powerful programming tools we can finally
return to modelling. You'll use your new tools of data wrangling and
programming, to fit many models and understand how they work. The focus
of this book is on exploration, not confirmation or formal inference.
But you'll learn a few basic tools that help you understand the
variation within your models.

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/data-science-model} \end{center}

The goal of a model is to provide a simple low-dimensional summary of a
dataset. Ideally, the model will capture true ``signals'' (i.e.~patterns
generated by the phenomenon of interest), and ignore ``noise''
(i.e.~random variation that you're not interested in). Here we only
cover ``predictive'' models, which, as the name suggests, generate
predictions. There is another type of model that we're not going to
discuss: ``data discovery'' models. These models don't make predictions,
but instead help you discover interesting relationships within your
data. (These two categories of models are sometimes called supervised
and unsupervised, but I don't think that terminology is particularly
illuminating.)

This book is not going to give you a deep understanding of the
mathematical theory that underlies models. It will, however, build your
intuition about how statistical models work, and give you a family of
useful tools that allow you to use models to better understand your
data:

\begin{itemize}
\item
  In \protect\hyperlink{model-basics}{model basics}, you'll learn how
  models work mechanistically, focussing on the important family of
  linear models. You'll learn general tools for gaining insight into
  what a predictive model tells you about your data, focussing on simple
  simulated datasets.
\item
  In \protect\hyperlink{model-building}{model building}, you'll learn
  how to use models to pull out known patterns in real data. Once you
  have recognised an important pattern it's useful to make it explicit
  in a model, because then you can more easily see the subtler signals
  that remain.
\item
  In \protect\hyperlink{many-models}{many models}, you'll learn how to
  use many simple models to help understand complex datasets. This is a
  powerful technique, but to access it you'll need to combine modelling
  and programming tools.
\end{itemize}

These topics are notable because of what they don't include: any tools
for quantitatively assessing models. That is deliberate: precisely
quantifying a model requires a couple of big ideas that we just don't
have the space to cover here. For now, you'll rely on qualitative
assessment and your natural scepticism. In
\protect\hyperlink{learning-more-about-models}{Learning more about
models}, we'll point you to other resources where you can learn more.

\section{Hypothesis generation vs.~hypothesis
confirmation}\label{hypothesis-generation-vs.hypothesis-confirmation}

In this book, we are going to use models as a tool for exploration,
completing the trifecta of the tools for EDA that were introduced in
Part 1. This is not how models are usually taught, but as you will see,
models are an important tool for exploration. Traditionally, the focus
of modelling is on inference, or for confirming that an hypothesis is
true. Doing this correctly is not complicated, but it is hard. There is
a pair of ideas that you must understand in order to do inference
correctly:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each observation can either be used for exploration or confirmation,
  not both.
\item
  You can use an observation as many times as you like for exploration,
  but you can only use it once for confirmation. As soon as you use an
  observation twice, you've switched from confirmation to exploration.
\end{enumerate}

This is necessary because to confirm a hypothesis you must use data
independent of the data that you used to generate the hypothesis.
Otherwise you will be over optimistic. There is absolutely nothing wrong
with exploration, but you should never sell an exploratory analysis as a
confirmatory analysis because it is fundamentally misleading.

If you are serious about doing an confirmatory analysis, one approach is
to split your data into three pieces before you begin the analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  60\% of your data goes into a \textbf{training} (or exploration) set.
  You're allowed to do anything you like with this data: visualise it
  and fit tons of models to it.
\item
  20\% goes into a \textbf{query} set. You can use this data to compare
  models or visualisations by hand, but you're not allowed to use it as
  part of an automated process.
\item
  20\% is held back for a \textbf{test} set. You can only use this data
  ONCE, to test your final model.
\end{enumerate}

This partitioning allows you to explore the training data, occasionally
generating candidate hypotheses that you check with the query set. When
you are confident you have the right model, you can check it once with
the test data.

(Note that even when doing confirmatory modelling, you will still need
to do EDA. If you don't do any EDA you will remain blind to the quality
problems with your data.)

\hypertarget{model-basics}{\chapter{Model basics}\label{model-basics}}

\section{Introduction}\label{introduction-15}

The goal of a model is to provide a simple low-dimensional summary of a
dataset. In the context of this book we're going to use models to
partition data into patterns and residuals. Strong patterns will hide
subtler trends, so we'll use models to help peel back layers of
structure as we explore a dataset.

However, before we can start using models on interesting, real,
datasets, you need to understand the basics of how models work. For that
reason, this chapter of the book is unique because it uses only
simulated datasets. These datasets are very simple, and not at all
interesting, but they will help you understand the essence of modelling
before you apply the same techniques to real data in the next chapter.

There are three parts to a model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  First, you define a \textbf{family of models} that express a precise,
  but generic, pattern that you want to capture. For example, the
  pattern might be a straight line, or a quadatric curve. You will
  express the model family as an equation like
  \texttt{y\ =\ a\_1\ *\ x\ +\ a\_2} or
  \texttt{y\ =\ a\_1\ *\ x\ \^{}\ a\_2}. Here, \texttt{x} and \texttt{y}
  are known variables from your data, and \texttt{a\_1} and
  \texttt{a\_2} are parameters that can vary to capture different
  patterns.
\item
  Next, you generate a \textbf{fitted model} by finding the model from
  the family that is the closest to your data. This takes the generic
  model family and makes it specific, like \texttt{y\ =\ 3\ *\ x\ +\ 7}
  or \texttt{y\ =\ 9\ *\ x\ \^{}\ 2}.
\end{enumerate}

It's important to understand that a fitted model is just the closest
model from a family of models. That implies that you have the ``best''
model (according to some criteria); it doesn't imply that you have a
good model and it certainly doesn't imply that the model is ``true''.
George Box puts this well in his famous aphorism:

\begin{quote}
All models are wrong, but some are useful.
\end{quote}

It's worth reading the fuller context of the quote:

\begin{quote}
Now it would be very remarkable if any system existing in the real world
could be exactly represented by any simple model. However, cunningly
chosen parsimonious models often do provide remarkably useful
approximations. For example, the law PV = RT relating pressure P, volume
V and temperature T of an ``ideal'' gas via a constant R is not exactly
true for any real gas, but it frequently provides a useful approximation
and furthermore its structure is informative since it springs from a
physical view of the behavior of gas molecules.

For such a model there is no need to ask the question ``Is the model
true?''. If ``truth'' is to be the ``whole truth'' the answer must be
``No''. The only question of interest is ``Is the model illuminating and
useful?''.
\end{quote}

The goal of a model is not to uncover truth, but to discover a simple
approximation that is still useful.

\subsection{Prerequisites}\label{prerequisites-15}

In this chapter we'll use the modelr package which wraps around base R's
modelling functions to make them work naturally in a pipe.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{library}\NormalTok{(modelr)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{na.action =} \NormalTok{na.warn)}
\end{Highlighting}
\end{Shaded}

\section{A simple model}\label{a-simple-model}

Lets take a look at the simulated dataset \texttt{sim1}. It contains two
continuous variables, \texttt{x} and \texttt{y}. Let's plot them to see
how they're related:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(x, y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-2-1} \end{center}

You can see a strong pattern in the data. Let's use a model to capture
that pattern and make it explicit. It's our job to supply the basic form
of the model. In this case, the relationship looks linear, i.e.
\texttt{y\ =\ a\_0\ +\ a\_1\ *\ x}. Let's start by getting a feel for
what models from that family look like by randomly generating a few and
overlaying them on the data. For this simple case, we can use
\texttt{geom\_abline()} which takes a slope and intercept as parameters.
Later on we'll learn more general techniques that work with any model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{a1 =} \KeywordTok{runif}\NormalTok{(}\DecValTok{250}\NormalTok{, -}\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{),}
  \DataTypeTok{a2 =} \KeywordTok{runif}\NormalTok{(}\DecValTok{250}\NormalTok{, -}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(x, y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{intercept =} \NormalTok{a1, }\DataTypeTok{slope =} \NormalTok{a2), }\DataTypeTok{data =} \NormalTok{models, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{4}\NormalTok{) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-3-1} \end{center}

There are 250 models on this plot, but a lot are really bad! We need to
find the good models by making precise our intuition that a good model
is ``close'' to the data. We need a way to quantify the distance between
the data and a model. Then we can fit the model by finding the value of
\texttt{a\_0} and \texttt{a\_1} that generate the model with the
smallest distance from this data.

One easy place to start is to find the vertical distance between each
point and the model, as in the following diagram. (Note that I've
shifted the x values slightly so you can see the individual distances.)

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-4-1} \end{center}

This distance is just the difference between the y value given by the
model (the \textbf{prediction}), and the actual y value in the data (the
\textbf{response}).

To compute this distance, we first turn our model family into an R
function. This takes the model parameters and the data as inputs, and
gives values predicted by the model as output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 <-}\StringTok{ }\NormalTok{function(a, data) \{}
  \NormalTok{a[}\DecValTok{1}\NormalTok{] +}\StringTok{ }\NormalTok{data$x *}\StringTok{ }\NormalTok{a[}\DecValTok{2}\NormalTok{]}
\NormalTok{\}}
\KeywordTok{model1}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\FloatTok{1.5}\NormalTok{), sim1)}
\CommentTok{#>  [1]  8.5  8.5  8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5}
\CommentTok{#> [15] 14.5 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0}
\CommentTok{#> [29] 22.0 22.0}
\end{Highlighting}
\end{Shaded}

Next, we need some way to compute an overall distance between the
predicted and actual values. In other words, the plot above shows 30
distances: how do we collapse that into a single number?

One common way to do this in statistics to use the ``root-mean-squared
deviation''. We compute the difference between actual and predicted,
square them, average them, and the take the square root. This distance
has lots of appealing mathematical properties, which we're not going to
talk about here. You'll just have to take my word for it!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{measure_distance <-}\StringTok{ }\NormalTok{function(mod, data) \{}
  \NormalTok{diff <-}\StringTok{ }\NormalTok{data$y -}\StringTok{ }\KeywordTok{model1}\NormalTok{(mod, data)}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(diff ^}\StringTok{ }\DecValTok{2}\NormalTok{))}
\NormalTok{\}}
\KeywordTok{measure_distance}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\FloatTok{1.5}\NormalTok{), sim1)}
\CommentTok{#> [1] 2.67}
\end{Highlighting}
\end{Shaded}

Now we can use purrr to compute the distance for all the models defined
above. We need a helper function because our distance function expects
the model as a numeric vector of length 2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim1_dist <-}\StringTok{ }\NormalTok{function(a1, a2) \{}
  \KeywordTok{measure_distance}\NormalTok{(}\KeywordTok{c}\NormalTok{(a1, a2), sim1)}
\NormalTok{\}}

\NormalTok{models <-}\StringTok{ }\NormalTok{models %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{dist =} \NormalTok{purrr::}\KeywordTok{map2_dbl}\NormalTok{(a1, a2, sim1_dist))}
\NormalTok{models}
\CommentTok{#> # A tibble: 250 × 3}
\CommentTok{#>       a1      a2  dist}
\CommentTok{#>    <dbl>   <dbl> <dbl>}
\CommentTok{#> 1 -15.15  0.0889  30.8}
\CommentTok{#> 2  30.06 -0.8274  13.2}
\CommentTok{#> 3  16.05  2.2695  13.2}
\CommentTok{#> 4 -10.57  1.3769  18.7}
\CommentTok{#> 5 -19.56 -1.0359  41.8}
\CommentTok{#> 6   7.98  4.5948  19.3}
\CommentTok{#> # ... with 244 more rows}
\end{Highlighting}
\end{Shaded}

Next, let's overlay the 10 best models on to the data. I've coloured the
models by \texttt{-dist}: this is an easy way to make sure that the best
models (i.e.~the ones with the smallest distance) get the brighest
colours.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(x, y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"grey30"}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{intercept =} \NormalTok{a1, }\DataTypeTok{slope =} \NormalTok{a2, }\DataTypeTok{colour =} \NormalTok{-dist), }
    \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(models, }\KeywordTok{rank}\NormalTok{(dist) <=}\StringTok{ }\DecValTok{10}\NormalTok{)}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-8-1} \end{center}

We can also think about these models as observations, and visualising
with a scatterplot of \texttt{a1} vs \texttt{a2}, again coloured by
\texttt{-dist}. We can no longer directly see how the model compares to
the data, but we can see many models at once. Again, I've highlighted
the 10 best models, this time by drawing red circles underneath them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(models, }\KeywordTok{aes}\NormalTok{(a1, a2)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(models, }\KeywordTok{rank}\NormalTok{(dist) <=}\StringTok{ }\DecValTok{10}\NormalTok{), }\DataTypeTok{size =} \DecValTok{4}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{-dist))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-9-1} \end{center}

Instead of trying lots of random models, we could be more systematic and
generate an evenly spaced grid of points (this is called a grid search).
I picked the parameters of the grid roughly by looking at where the best
models were in the plot above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}
  \DataTypeTok{a1 =} \KeywordTok{seq}\NormalTok{(-}\DecValTok{5}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DataTypeTok{length =} \DecValTok{25}\NormalTok{),}
  \DataTypeTok{a2 =} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DataTypeTok{length =} \DecValTok{25}\NormalTok{)}
  \NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{dist =} \NormalTok{purrr::}\KeywordTok{map2_dbl}\NormalTok{(a1, a2, sim1_dist))}

\NormalTok{grid %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(a1, a2)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(grid, }\KeywordTok{rank}\NormalTok{(dist) <=}\StringTok{ }\DecValTok{10}\NormalTok{), }\DataTypeTok{size =} \DecValTok{4}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{-dist)) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-10-1} \end{center}

When you overlay the best 10 models back on the original data, they all
look pretty good:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(x, y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"grey30"}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{intercept =} \NormalTok{a1, }\DataTypeTok{slope =} \NormalTok{a2, }\DataTypeTok{colour =} \NormalTok{-dist), }
    \DataTypeTok{data =} \KeywordTok{filter}\NormalTok{(grid, }\KeywordTok{rank}\NormalTok{(dist) <=}\StringTok{ }\DecValTok{10}\NormalTok{)}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-11-1} \end{center}

You could imagine iteratively making the grid finer and finer until you
narrowed in on the best model. But there's a better way to tackle that
problem: a numerical minimisation tool called Newton-Raphson search. The
intuition of Newton-Raphson is pretty simple: you pick a starting point
and look around for the steepest slope. You then ski down that slope a
little way, and then repeat again and again, until you can't go any
lower. In R, we can do that with \texttt{optim()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best <-}\StringTok{ }\KeywordTok{optim}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), measure_distance, }\DataTypeTok{data =} \NormalTok{sim1)}
\NormalTok{best$par}
\CommentTok{#> [1] 4.22 2.05}

\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(x, y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"grey30"}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{intercept =} \NormalTok{best$par[}\DecValTok{1}\NormalTok{], }\DataTypeTok{slope =} \NormalTok{best$par[}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-12-1} \end{center}

Don't worry too much about the details of how \texttt{optim()} works.
It's the intuition that's important here. If you have a function that
defines the distance between a model and a dataset, an algorithm that
can minimise that distance by modifying the parameters of the model, you
can find the best model. The neat thing about this approach is that it
will work for any family of models that you can write an equation for.

There's one more approach that we can use for this model, because it's
is a special case of a broader family: linear models. A linear model has
the general form
\texttt{y\ =\ a\_1\ +\ a\_2\ *\ x\_1\ +\ a\_3\ *\ x\_2\ +\ ...\ +\ a\_n\ *\ x\_(n\ -\ 1)}.
So this simple model is equivalent to a general linear model where n is
2 and \texttt{x\_1} is \texttt{x}. R has a tool specifically designed
for fitting linear models called \texttt{lm()}. \texttt{lm()} has a
special way to specify the model family: formulas. Formulas look like
\texttt{y\ \textasciitilde{}\ x}, which \texttt{lm()} will translate to
a function like \texttt{y\ =\ a\_1\ +\ a\_2\ *\ x}. We can fit the model
and look at the output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim1_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =} \NormalTok{sim1)}
\KeywordTok{coef}\NormalTok{(sim1_mod)}
\CommentTok{#> (Intercept)           x }
\CommentTok{#>        4.22        2.05}
\end{Highlighting}
\end{Shaded}

These are exactly the same values we got with \texttt{optim()}! Behind
the scenes \texttt{lm()} doesn't use \texttt{optim()} but instead takes
advantage of the mathematical structure of linear models. Using some
connections between geometry, calculus, and linear algebra,
\texttt{lm()} actually finds the closest model in a single step, using a
sophisticated algorithm. This approach is both faster, and guarantees
that there is a global minimum.

\subsection{Exercises}\label{exercises-60}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  One downside of the linear model is that it is sensitive to unusual
  values because the distance incorporates a squared term. Fit a linear
  model to the simulated data below, and visualise the results. Rerun a
  few times to generate different simulated datasets. What do you notice
  about the model?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim1a <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{, }\DataTypeTok{each =} \DecValTok{3}\NormalTok{),}
  \DataTypeTok{y =} \NormalTok{x *}\StringTok{ }\FloatTok{1.5} \NormalTok{+}\StringTok{ }\DecValTok{6} \NormalTok{+}\StringTok{ }\KeywordTok{rt}\NormalTok{(}\KeywordTok{length}\NormalTok{(x), }\DataTypeTok{df =} \DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  One way to make linear models more robust is to use a different
  distance measure. For example, instead of root-mean-squared distance,
  you could use mean-absolute distance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{measure_distance <-}\StringTok{ }\NormalTok{function(mod, data) \{}
  \NormalTok{diff <-}\StringTok{ }\NormalTok{data$y -}\StringTok{ }\KeywordTok{make_prediction}\NormalTok{(mod, data)}
  \KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(diff))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

  Use \texttt{optim()} to fit this model to the simulated data above and
  compare it to the linear model.
\item
  One challenge with performing numerical optimisation is that it's only
  guaranteed to find one local optima. What's the problem with
  optimising a three parameter model like this?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 <-}\StringTok{ }\NormalTok{function(a, data) \{}
  \NormalTok{a[}\DecValTok{1}\NormalTok{] +}\StringTok{ }\NormalTok{data$x *}\StringTok{ }\NormalTok{a[}\DecValTok{2}\NormalTok{] +}\StringTok{ }\NormalTok{a[}\DecValTok{3}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{Visualising models}\label{visualising-models}

For simple models, like the one above, you can figure out what pattern
the model captures by carefully studying the model family and the fitted
coefficients. And if you ever take a statistics course on modelling,
you're likely to spend a lot of time doing just that. Here, however,
we're going to take a different tack. We're going to focus on
understanding a model by looking at its predictions. This has a big
advantage: every type of predictive model makes predictions (otherwise
what use would it be?) so we can use the same set of techniques to
understand any type of predictive model.

It's also useful to see what the model doesn't capture, the so-called
residuals which are left after subtracting the predictions from the
data. Residuals are powerful because they allow us to use models to
remove striking patterns so we can study the subtler trends that remain.

\subsection{Predictions}\label{predictions}

To visualise the predictions from a model, we start by generating an
evenly spaced grid of values that covers the region where our data lies.
The easiest way to do that is to use \texttt{modelr::data\_grid()}. Its
first argument is a data frame, and for each subsequent argument it
finds the unique variables and then generates all combinations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\NormalTok{sim1 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(x) }
\NormalTok{grid}
\CommentTok{#> # A tibble: 10 × 1}
\CommentTok{#>       x}
\CommentTok{#>   <int>}
\CommentTok{#> 1     1}
\CommentTok{#> 2     2}
\CommentTok{#> 3     3}
\CommentTok{#> 4     4}
\CommentTok{#> 5     5}
\CommentTok{#> 6     6}
\CommentTok{#> # ... with 4 more rows}
\end{Highlighting}
\end{Shaded}

(This will get more interesting when we start to add more variables to
our model.)

Next we add predictions. We'll use \texttt{modelr::add\_predictions()}
which takes a data frame and a model. It adds the predictions from the
model to a new column in the data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\NormalTok{grid %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(sim1_mod) }
\NormalTok{grid}
\CommentTok{#> # A tibble: 10 × 2}
\CommentTok{#>       x  pred}
\CommentTok{#>   <int> <dbl>}
\CommentTok{#> 1     1  6.27}
\CommentTok{#> 2     2  8.32}
\CommentTok{#> 3     3 10.38}
\CommentTok{#> 4     4 12.43}
\CommentTok{#> 5     5 14.48}
\CommentTok{#> 6     6 16.53}
\CommentTok{#> # ... with 4 more rows}
\end{Highlighting}
\end{Shaded}

(You can also use this function to add predictions to your original
dataset.)

Next, we plot the predictions. You might wonder about all this extra
work compared to just using \texttt{geom\_abline()}. But the advantage
of this approach is that it will work with \emph{any} model in R, from
the simplest to the most complex. You're only limited by your
visualisation skills. For more ideas about how to visualise more complex
model types, you might try
\url{http://vita.had.co.nz/papers/model-vis.html}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(x)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \NormalTok{y)) +}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \NormalTok{pred), }\DataTypeTok{data =} \NormalTok{grid, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-19-1} \end{center}

\subsection{Residuals}\label{residuals}

The flip-side of predictions are \textbf{residuals}. The predictions
tells you the pattern that the model has captured, and the residuals
tell you what the model has missed. The residuals are just the distances
between the observed and predicted values that we computed above.

We add residuals to the data with \texttt{add\_residuals()}, which works
much like \texttt{add\_predictions()}. Note, however, that we use the
original dataset, not a manufactured grid. This is because to compute
residuals we need actual y values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim1 <-}\StringTok{ }\NormalTok{sim1 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(sim1_mod)}
\NormalTok{sim1}
\CommentTok{#> # A tibble: 30 × 3}
\CommentTok{#>       x     y  resid}
\CommentTok{#>   <int> <dbl>  <dbl>}
\CommentTok{#> 1     1  4.20 -2.072}
\CommentTok{#> 2     1  7.51  1.238}
\CommentTok{#> 3     1  2.13 -4.147}
\CommentTok{#> 4     2  8.99  0.665}
\CommentTok{#> 5     2 10.24  1.919}
\CommentTok{#> 6     2 11.30  2.973}
\CommentTok{#> # ... with 24 more rows}
\end{Highlighting}
\end{Shaded}

There are a few different ways to understand what the residuals tell us
about the model. One way is to simply draw a frequency polygon to help
us understand the spread of the residuals:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(resid)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_freqpoly}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-21-1} \end{center}

This helps you calibrate the quality of the model: how far away are the
predictions from the observed values? Note that the average of the
residual will always be 0.

You'll often want to recreate plots using the residuals instead of the
original predictor. You'll see a lot of that in the next chapter.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim1, }\KeywordTok{aes}\NormalTok{(x, resid)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_ref_line}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-22-1} \end{center}

This looks like random noise, suggesting that our model has done a good
job of capturing the patterns in the dataset.

\subsection{Exercises}\label{exercises-61}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Instead of using \texttt{lm()} to fit a straight line, you can use
  \texttt{loess()} to fit a smooth curve. Repeat the process of model
  fitting, grid generation, predictions, and visualisation on
  \texttt{sim1} using \texttt{loess()} instead of \texttt{lm()}. How
  does the result compare to \texttt{geom\_smooth()}?
\item
  \texttt{add\_predictions()} is paired with
  \texttt{gather\_predictions()} and \texttt{spread\_predictions()}. How
  do these three functions differ?
\item
  What does \texttt{geom\_ref\_line()} do? What package does it come
  from? Why is displaying a reference line in plots showing residuals
  useful and important?
\item
  Why might you want to look at a frequency polygon of absolute
  residuals? What are the pros and cons compared to looking at the raw
  residuals?
\end{enumerate}

\section{Formulas and model families}\label{formulas-and-model-families}

You've seen formulas before when using \texttt{facet\_wrap()} and
\texttt{facet\_grid()}. In R, formulas provide a general way of getting
``special behaviour''. Rather than evaluating the values of the
variables right away, they capture them so they can be interpreted by
the function.

The majority of modelling functions in R use a standard conversion from
formulas to functions. You've seen one simple conversion already:
\texttt{y\ \textasciitilde{}\ x} is translated to
\texttt{y\ =\ a\_1\ +\ a\_2\ *\ x}. If you want to see what R actually
does, you can use the \texttt{model\_matrix()} function. It takes a data
frame and a formula and returns a tibble that defines the model
equation: each column in the output is associated with one coefficient
in the model, the function is always
\texttt{y\ =\ a\_1\ *\ out1\ +\ a\_2\ *\ out\_2}. For the simplest case
of \texttt{y\ \textasciitilde{}\ x1} this shows us something
interesting:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~y, ~x1, ~x2,}
  \DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{,}
  \DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{6}
\NormalTok{)}
\KeywordTok{model_matrix}\NormalTok{(df, y ~}\StringTok{ }\NormalTok{x1)}
\CommentTok{#> # A tibble: 2 × 2}
\CommentTok{#>   `(Intercept)`    x1}
\CommentTok{#>           <dbl> <dbl>}
\CommentTok{#> 1             1     2}
\CommentTok{#> 2             1     1}
\end{Highlighting}
\end{Shaded}

The way that R adds the intercept to the model is just by having a
column that is full of ones. By default, R will always add this column.
If you don't want, you need to explicitly drop it with \texttt{-1}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{model_matrix}\NormalTok{(df, y ~}\StringTok{ }\NormalTok{x1 -}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 1}
\CommentTok{#>      x1}
\CommentTok{#>   <dbl>}
\CommentTok{#> 1     2}
\CommentTok{#> 2     1}
\end{Highlighting}
\end{Shaded}

The model matrix grows in an unsurprising way when you add more
variables to the the model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{model_matrix}\NormalTok{(df, y ~}\StringTok{ }\NormalTok{x1 +}\StringTok{ }\NormalTok{x2)}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>   `(Intercept)`    x1    x2}
\CommentTok{#>           <dbl> <dbl> <dbl>}
\CommentTok{#> 1             1     2     5}
\CommentTok{#> 2             1     1     6}
\end{Highlighting}
\end{Shaded}

This formula notation is sometimes called ``Wilkinson-Rogers notation'',
and was initially described in \emph{Symbolic Description of Factorial
Models for Analysis of Variance}, by G. N. Wilkinson and C. E. Rogers
\url{https://www.jstor.org/stable/2346786}. It's worth digging up and
reading the original paper if you'd like to understand the full details
of the modelling algebra.

The following sections expand on how this formula notation works for
categorcal variables, interactions, and transformation.

\subsection{Categorical variables}\label{categorical-variables}

Generating a function from a formula is straight forward when the
predictor is continuous, but things get a bit more complicated when the
predictor is categorical. Imagine you have a formula like
\texttt{y\ \textasciitilde{}\ sex}, where sex could either be male or
female. It doesn't make sense to convert that to a formula like
\texttt{y\ =\ x\_0\ +\ x\_1\ *\ sex} because \texttt{sex} isn't a number
- you can't multiply it! Instead what R does is convert it to
\texttt{y\ =\ x\_0\ +\ x\_1\ *\ sex\_male} where \texttt{sex\_male} is
one if \texttt{sex} is male and zero otherwise:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~}\StringTok{ }\NormalTok{sex, ~}\StringTok{ }\NormalTok{response,}
  \StringTok{"male"}\NormalTok{, }\DecValTok{1}\NormalTok{,}
  \StringTok{"female"}\NormalTok{, }\DecValTok{2}\NormalTok{,}
  \StringTok{"male"}\NormalTok{, }\DecValTok{1}
\NormalTok{)}
\KeywordTok{model_matrix}\NormalTok{(df, response ~}\StringTok{ }\NormalTok{sex)}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>   `(Intercept)` sexmale}
\CommentTok{#>           <dbl>   <dbl>}
\CommentTok{#> 1             1       1}
\CommentTok{#> 2             1       0}
\CommentTok{#> 3             1       1}
\end{Highlighting}
\end{Shaded}

You might wonder why R also doesn't create a \texttt{sexfemale} column.
The problem is that would create a column that is perfectly predictable
based on the other columns (i.e. \texttt{sexfemale\ =\ 1\ -\ sexmale}).
Unfortunately the exact details of why this is a problem is beyond the
scope of this book, but basically it creates a model family that is too
flexible, and will have infinitely many models that are equally close to
the data.

Fortunately, however, if you focus on visualising predictions you don't
need to worry about the exact parameterisation. Let's look at some data
and models to make that concrete. Here's the \texttt{sim2} dataset from
modelr:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim2) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(x, y))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-27-1} \end{center}

We can fit a model to it, and generate predictions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =} \NormalTok{sim2)}

\NormalTok{grid <-}\StringTok{ }\NormalTok{sim2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(x) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod2)}
\NormalTok{grid}
\CommentTok{#> # A tibble: 4 × 2}
\CommentTok{#>       x  pred}
\CommentTok{#>   <chr> <dbl>}
\CommentTok{#> 1     a  1.15}
\CommentTok{#> 2     b  8.12}
\CommentTok{#> 3     c  6.13}
\CommentTok{#> 4     d  1.91}
\end{Highlighting}
\end{Shaded}

Effectively, a model with a categorical \texttt{x} will predict the mean
value for each category. (Why? Because the mean minimises the
root-mean-squared distance.) That's easy to see if we overlay the
predictions on top of the original data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim2, }\KeywordTok{aes}\NormalTok{(x)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \NormalTok{y)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \NormalTok{grid, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \NormalTok{pred), }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-29-1} \end{center}

You can't make predictions about levels that you didn't observe.
Sometimes you'll do this by accident so it's good to recognise this
error message:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \StringTok{"e"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod2)}
\CommentTok{#> Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor x has new level e}
\end{Highlighting}
\end{Shaded}

\subsection{Interactions (continuous and
categorical)}\label{interactions-continuous-and-categorical}

What happens when you combine a continuous and a categorical variable?
\texttt{sim3} contains a categorical predictor and a continuous
predictor. We can visualise it with a simple plot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim3, }\KeywordTok{aes}\NormalTok{(x1, y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{x2))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-31-1} \end{center}

There are two possible models you could fit to this data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x1 +}\StringTok{ }\NormalTok{x2, }\DataTypeTok{data =} \NormalTok{sim3)}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x1 *}\StringTok{ }\NormalTok{x2, }\DataTypeTok{data =} \NormalTok{sim3)}
\end{Highlighting}
\end{Shaded}

When you add variables with \texttt{+}, the model will estimate each
effect independent of all the others. It's possible to fit the so-called
interaction by using \texttt{*}. For example,
\texttt{y\ \textasciitilde{}\ x1\ *\ x2} is translated to
\texttt{y\ =\ a\_0\ +\ a\_1\ *\ a1\ +\ a\_2\ *\ a2\ +\ a\_12\ *\ a1\ *\ a2}.
Note that whenever you use \texttt{*}, both the interaction and the
individual components are included in the model.

To visualise these models we need two new tricks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We have two predictors, so we need to give \texttt{data\_grid()} both
  variables. It finds all the unique values of \texttt{x1} and
  \texttt{x2} and then generates all combinations.
\item
  To generate predictions from both models simultaneously, we can use
  \texttt{gather\_predictions()} which adds each prediction as a row.
  The complement of \texttt{gather\_predictions()} is
  \texttt{spread\_predictions()} which adds each prediction to a new
  column.
\end{enumerate}

Together this gives us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\NormalTok{sim3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(x1, x2) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather_predictions}\NormalTok{(mod1, mod2)}
\NormalTok{grid}
\CommentTok{#> # A tibble: 80 × 4}
\CommentTok{#>   model    x1     x2  pred}
\CommentTok{#>   <chr> <int> <fctr> <dbl>}
\CommentTok{#> 1  mod1     1      a  1.67}
\CommentTok{#> 2  mod1     1      b  4.56}
\CommentTok{#> 3  mod1     1      c  6.48}
\CommentTok{#> 4  mod1     1      d  4.03}
\CommentTok{#> 5  mod1     2      a  1.48}
\CommentTok{#> 6  mod1     2      b  4.37}
\CommentTok{#> # ... with 74 more rows}
\end{Highlighting}
\end{Shaded}

We can visualise the results for both models on one plot using
facetting:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(sim3, }\KeywordTok{aes}\NormalTok{(x1, y, }\DataTypeTok{colour =} \NormalTok{x2)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =} \NormalTok{grid, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \NormalTok{pred)) +}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-34-1} \end{center}

Note that the model that uses \texttt{+} has the same slope for each
line, but different intercepts. The model that uses \texttt{*} has a
different slope and intercept for each line.

Which model is better for this data? We can take look at the residuals.
Here I've facetted by both model and \texttt{x2} because it makes it
easier to see the pattern within each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim3 <-}\StringTok{ }\NormalTok{sim3 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather_residuals}\NormalTok{(mod1, mod2)}

\KeywordTok{ggplot}\NormalTok{(sim3, }\KeywordTok{aes}\NormalTok{(x1, resid, }\DataTypeTok{colour =} \NormalTok{x2)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(model ~}\StringTok{ }\NormalTok{x2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-35-1} \end{center}

There is little obvious pattern in the residuals for \texttt{mod2}. The
residuals for \texttt{mod1} show that the model has clearly missed some
pattern in \texttt{b}, and less so, but still present is pattern in
\texttt{c}, and \texttt{d}. You might wonder if there's a precise way to
tell which of \texttt{mod1} or \texttt{mod2} is better. There is, but it
requires a lot of mathematical background, and we don't really care.
Here, we're interested in a qualitative assessment of whether or not the
model has captured the pattern that we're interested in.

\subsection{Interactions (two
continuous)}\label{interactions-two-continuous}

Let's take a look at the equivalent model for two continuous variables.
Initially things proceed almost identically to the previous example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x1 +}\StringTok{ }\NormalTok{x2, }\DataTypeTok{data =} \NormalTok{sim4)}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x1 *}\StringTok{ }\NormalTok{x2, }\DataTypeTok{data =} \NormalTok{sim4)}

\NormalTok{grid <-}\StringTok{ }\NormalTok{sim4 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(}
    \DataTypeTok{x1 =} \KeywordTok{seq_range}\NormalTok{(x1, }\DecValTok{5}\NormalTok{), }
    \DataTypeTok{x2 =} \KeywordTok{seq_range}\NormalTok{(x2, }\DecValTok{5}\NormalTok{) }
  \NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather_predictions}\NormalTok{(mod1, mod2)}
\NormalTok{grid}
\CommentTok{#> # A tibble: 50 × 4}
\CommentTok{#>   model    x1    x2   pred}
\CommentTok{#>   <chr> <dbl> <dbl>  <dbl>}
\CommentTok{#> 1  mod1  -1.0  -1.0  0.996}
\CommentTok{#> 2  mod1  -1.0  -0.5 -0.395}
\CommentTok{#> 3  mod1  -1.0   0.0 -1.786}
\CommentTok{#> 4  mod1  -1.0   0.5 -3.177}
\CommentTok{#> 5  mod1  -1.0   1.0 -4.569}
\CommentTok{#> 6  mod1  -0.5  -1.0  1.907}
\CommentTok{#> # ... with 44 more rows}
\end{Highlighting}
\end{Shaded}

Note my use of \texttt{seq\_range()} inside \texttt{data\_grid()}.
Instead of using every unique value of \texttt{x}, I'm going to use a
regularly spaced grid of five values between the minimum and maximum
numbers. It's probably not super important here, but it's a useful
technique in general. There are two other useful arguments to
\texttt{seq\_range()}:

\begin{itemize}
\item
  \texttt{pretty\ =\ TRUE} will generate a ``pretty'' sequence,
  i.e.~something that looks nice to the human eye. This is useful if you
  want to produce tables of output:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq_range}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.0123}\NormalTok{, }\FloatTok{0.923423}\NormalTok{), }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] 0.0123 0.2401 0.4679 0.6956 0.9234}
\KeywordTok{seq_range}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.0123}\NormalTok{, }\FloatTok{0.923423}\NormalTok{), }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{pretty =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.0 0.2 0.4 0.6 0.8 1.0}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{trim\ =\ 0.1} will trim off 10\% of the tail values. This is
  useful if the variables have a long tailed distribution and you want
  to focus on generating values near the center:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{rcauchy}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\KeywordTok{seq_range}\NormalTok{(x1, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] -115.9  -83.5  -51.2  -18.8   13.5}
\KeywordTok{seq_range}\NormalTok{(x1, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{trim =} \FloatTok{0.10}\NormalTok{)}
\CommentTok{#> [1] -13.84  -8.71  -3.58   1.55   6.68}
\KeywordTok{seq_range}\NormalTok{(x1, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{trim =} \FloatTok{0.25}\NormalTok{)}
\CommentTok{#> [1] -2.1735 -1.0594  0.0547  1.1687  2.2828}
\KeywordTok{seq_range}\NormalTok{(x1, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{trim =} \FloatTok{0.50}\NormalTok{)}
\CommentTok{#> [1] -0.725 -0.268  0.189  0.647  1.104}
\end{Highlighting}
\end{Shaded}
\item
  \texttt{expand\ =\ 0.1} is in some sense the opposite of
  \texttt{trim()} it expands the range by 10\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{seq_range}\NormalTok{(x2, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{#> [1] 0.00 0.25 0.50 0.75 1.00}
\KeywordTok{seq_range}\NormalTok{(x2, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{expand =} \FloatTok{0.10}\NormalTok{)}
\CommentTok{#> [1] -0.050  0.225  0.500  0.775  1.050}
\KeywordTok{seq_range}\NormalTok{(x2, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{expand =} \FloatTok{0.25}\NormalTok{)}
\CommentTok{#> [1] -0.125  0.188  0.500  0.812  1.125}
\KeywordTok{seq_range}\NormalTok{(x2, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{expand =} \FloatTok{0.50}\NormalTok{)}
\CommentTok{#> [1] -0.250  0.125  0.500  0.875  1.250}
\end{Highlighting}
\end{Shaded}
\end{itemize}

Next let's try and visualise that model. We have two continuous
predictors, so you can imagine the model like a 3d surface. We could
display that using \texttt{geom\_tile()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(grid, }\KeywordTok{aes}\NormalTok{(x1, x2)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_tile}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =} \NormalTok{pred)) +}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-40-1} \end{center}

That doesn't suggest that the models are very different! But that's
partly an illusion: our eyes and brains are not very good at accurately
comparing shades of colour. Instead of looking at the surface from the
top, we could look at it from either side, showing multiple slices:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(grid, }\KeywordTok{aes}\NormalTok{(x1, pred, }\DataTypeTok{colour =} \NormalTok{x2, }\DataTypeTok{group =} \NormalTok{x2)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() +}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{model)}
\KeywordTok{ggplot}\NormalTok{(grid, }\KeywordTok{aes}\NormalTok{(x2, pred, }\DataTypeTok{colour =} \NormalTok{x1, }\DataTypeTok{group =} \NormalTok{x1)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() +}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-41-1} \includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-41-2} \end{center}

This shows you that interaction between two continuous variables works
basically the same way as for a categorical and continuous variable. An
interaction says that there's not a fixed offset: you need to consider
both values of \texttt{x1} and \texttt{x2} simultaneously in order to
predict \texttt{y}.

You can see that even with just two continuous variables, coming up with
good visualisations are hard. But that's reasonable: you shouldn't
expect it will be easy to understand how three or more variables
simultaneously interact! But again, we're saved a little because we're
using models for exploration, and you can gradually build up your model
over time. The model doesn't have to be perfect, it just has to help you
reveal a little more about your data.

I spent some time looking at the residuals to see if I could figure if
\texttt{mod2} did better than \texttt{mod1}. I think it does, but it's
pretty subtle. You'll have a chance to work on it in the exercises.

\subsection{Transformations}\label{transformations}

You can also perform transformations inside the model formula. For
example, \texttt{log(y)\ \textasciitilde{}\ sqrt(x1)\ +\ x2} is
transformed to
\texttt{y\ =\ a\_1\ +\ a\_2\ *\ x1\ *\ sqrt(x)\ +\ a\_3\ *\ x2}. If your
transformation involves \texttt{+}, \texttt{*}, \texttt{\^{}}, or
\texttt{-}, you'll need to wrap it in \texttt{I()} so R doesn't treat it
like part of the model specification. For example,
\texttt{y\ \textasciitilde{}\ x\ +\ I(x\ \^{}\ 2)} is translated to
\texttt{y\ =\ a\_1\ +\ a\_2\ *\ x\ +\ a\_3\ *\ x\^{}2}. If you forget
the \texttt{I()} and specify
\texttt{y\ \textasciitilde{}\ x\ \^{}\ 2\ +\ x}, R will compute
\texttt{y\ \textasciitilde{}\ x\ *\ x\ +\ x}. \texttt{x\ *\ x} means the
interaction of \texttt{x} with itself, which is the same as \texttt{x}.
R automatically drops redundant variables so \texttt{x\ +\ x} become
\texttt{x}, meaning that \texttt{y\ \textasciitilde{}\ x\ \^{}\ 2\ +\ x}
specifies the function \texttt{y\ =\ a\_1\ +\ a\_2\ *\ x}. That's
probably not what you intended!

Again, if you get confused about what your model is doing, you can
always use \texttt{model\_matrix()} to see exactly what equation
\texttt{lm()} is fitting:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~y, ~x,}
   \DecValTok{1}\NormalTok{,  }\DecValTok{1}\NormalTok{,}
   \DecValTok{2}\NormalTok{,  }\DecValTok{2}\NormalTok{, }
   \DecValTok{3}\NormalTok{,  }\DecValTok{3}
\NormalTok{)}
\KeywordTok{model_matrix}\NormalTok{(df, y ~}\StringTok{ }\NormalTok{x^}\DecValTok{2} \NormalTok{+}\StringTok{ }\NormalTok{x)}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>   `(Intercept)`     x}
\CommentTok{#>           <dbl> <dbl>}
\CommentTok{#> 1             1     1}
\CommentTok{#> 2             1     2}
\CommentTok{#> 3             1     3}
\KeywordTok{model_matrix}\NormalTok{(df, y ~}\StringTok{ }\KeywordTok{I}\NormalTok{(x^}\DecValTok{2}\NormalTok{) +}\StringTok{ }\NormalTok{x)}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>   `(Intercept)` `I(x^2)`     x}
\CommentTok{#>           <dbl>    <dbl> <dbl>}
\CommentTok{#> 1             1        1     1}
\CommentTok{#> 2             1        4     2}
\CommentTok{#> 3             1        9     3}
\end{Highlighting}
\end{Shaded}

Transformations are useful because you can use them to approximate
non-linear functions. If you've taken a calculus class, you may have
heard of Taylor's theorem which says you can approximate any smooth
function with an infinite sum of polynomials. That means you can use a
polynomial function to get arbitrarily close to a smooth function by
fitting an equation like
\texttt{y\ =\ a\_1\ +\ a\_2\ *\ x\ +\ a\_3\ *\ x\^{}2\ +\ a\_4\ *\ x\ \^{}\ 3}.
Typing that sequence by hand is tedious, so R provides a helper
function: \texttt{poly()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{model_matrix}\NormalTok{(df, y ~}\StringTok{ }\KeywordTok{poly}\NormalTok{(x, }\DecValTok{2}\NormalTok{))}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>   `(Intercept)` `poly(x, 2)1` `poly(x, 2)2`}
\CommentTok{#>           <dbl>         <dbl>         <dbl>}
\CommentTok{#> 1             1     -7.07e-01         0.408}
\CommentTok{#> 2             1     -7.85e-17        -0.816}
\CommentTok{#> 3             1      7.07e-01         0.408}
\end{Highlighting}
\end{Shaded}

However there's one major problem with using \texttt{poly()}: outside
the range of the data, polynomials rapidly shoot off to positive or
negative infinity. One safer alternative is to use the natural spline,
\texttt{splines::ns()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(splines)}
\KeywordTok{model_matrix}\NormalTok{(df, y ~}\StringTok{ }\KeywordTok{ns}\NormalTok{(x, }\DecValTok{2}\NormalTok{))}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>   `(Intercept)` `ns(x, 2)1` `ns(x, 2)2`}
\CommentTok{#>           <dbl>       <dbl>       <dbl>}
\CommentTok{#> 1             1       0.000       0.000}
\CommentTok{#> 2             1       0.566      -0.211}
\CommentTok{#> 3             1       0.344       0.771}
\end{Highlighting}
\end{Shaded}

Let's see what that looks like when we try and approximate a non-linear
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim5 <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{3.5} \NormalTok{*}\StringTok{ }\NormalTok{pi, }\DataTypeTok{length =} \DecValTok{50}\NormalTok{),}
  \DataTypeTok{y =} \DecValTok{4} \NormalTok{*}\StringTok{ }\KeywordTok{sin}\NormalTok{(x) +}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(x))}
\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(sim5, }\KeywordTok{aes}\NormalTok{(x, y)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-45-1} \end{center}

I'm going to fit five models to this data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\KeywordTok{ns}\NormalTok{(x, }\DecValTok{1}\NormalTok{), }\DataTypeTok{data =} \NormalTok{sim5)}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\KeywordTok{ns}\NormalTok{(x, }\DecValTok{2}\NormalTok{), }\DataTypeTok{data =} \NormalTok{sim5)}
\NormalTok{mod3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\KeywordTok{ns}\NormalTok{(x, }\DecValTok{3}\NormalTok{), }\DataTypeTok{data =} \NormalTok{sim5)}
\NormalTok{mod4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\KeywordTok{ns}\NormalTok{(x, }\DecValTok{4}\NormalTok{), }\DataTypeTok{data =} \NormalTok{sim5)}
\NormalTok{mod5 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\KeywordTok{ns}\NormalTok{(x, }\DecValTok{5}\NormalTok{), }\DataTypeTok{data =} \NormalTok{sim5)}

\NormalTok{grid <-}\StringTok{ }\NormalTok{sim5 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{seq_range}\NormalTok{(x, }\DataTypeTok{n =} \DecValTok{50}\NormalTok{, }\DataTypeTok{expand =} \FloatTok{0.1}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather_predictions}\NormalTok{(mod1, mod2, mod3, mod4, mod5, }\DataTypeTok{.pred =} \StringTok{"y"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(sim5, }\KeywordTok{aes}\NormalTok{(x, y)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =} \NormalTok{grid, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{) +}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-basics_files/figure-latex/unnamed-chunk-46-1} \end{center}

Notice that the extrapolation outside the range of the data is clearly
bad. This is the downside to approximating a function with a polynomial.
But this is a very real problem with every model: the model can never
tell you if the behaviour is true when you start extrapolating outside
the range of the data that you have seen. You must rely on theory and
science.

\subsection{Exercises}\label{exercises-62}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What happens if you repeat the analysis of \texttt{sim2} using a model
  without an intercept. What happens to the model equation? What happens
  to the predictions?
\item
  Use \texttt{model\_matrix()} to explore the equations generated for
  the models I fit to \texttt{sim3} and \texttt{sim4}. Why is \texttt{*}
  a good shorthand for interaction?
\item
  Using the basic principles, convert the formulas in the following two
  models into functions. (Hint: start by converting the categorical
  variable into 0-1 variables.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x1 +}\StringTok{ }\NormalTok{x2, }\DataTypeTok{data =} \NormalTok{sim3)}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x1 *}\StringTok{ }\NormalTok{x2, }\DataTypeTok{data =} \NormalTok{sim3)}
\end{Highlighting}
\end{Shaded}
\item
  For \texttt{sim4}, which of \texttt{mod1} and \texttt{mod2} is better?
  I think \texttt{mod2} does a slightly better job at removing patterns,
  but it's pretty subtle. Can you come up with a plot to support my
  claim?
\end{enumerate}

\section{Missing values}\label{missing-values-5}

Missing values obviously can not convey any information about the
relationship between the variables, so modelling functions will drop any
rows that contain missing values. R's default behaviour is to silently
drop them, but \texttt{options(na.action\ =\ na.warn)} (run in the
prerequisites), makes sure you get a warning.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x, ~y,}
  \DecValTok{1}\NormalTok{, }\FloatTok{2.2}\NormalTok{,}
  \DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{,}
  \DecValTok{3}\NormalTok{, }\FloatTok{3.5}\NormalTok{,}
  \DecValTok{4}\NormalTok{, }\FloatTok{8.3}\NormalTok{,}
  \OtherTok{NA}\NormalTok{, }\DecValTok{10}
\NormalTok{)}

\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =} \NormalTok{df)}
\CommentTok{#> Warning: Dropping 2 rows with missing values}
\end{Highlighting}
\end{Shaded}

To suppress the warning, set \texttt{na.action\ =\ na.exclude}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y ~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =} \NormalTok{df, }\DataTypeTok{na.action =} \NormalTok{na.exclude)}
\end{Highlighting}
\end{Shaded}

You can always see exactly how many observations were used with
\texttt{nobs()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nobs}\NormalTok{(mod)}
\CommentTok{#> [1] 3}
\end{Highlighting}
\end{Shaded}

\section{Other model families}\label{other-model-families}

This chapter has focussed exclusively on the class of linear models,
which assume a relationship of the form
\texttt{y\ =\ a\_1\ *\ x1\ +\ a\_2\ *\ x2\ +\ ...\ +\ a\_n\ *\ xn}.
Linear models additionally assume that the residuals have a normal
distribution, which we haven't talked about. There are a large set of
model classes that extend the linear model in various interesting ways.
Some of them are:

\begin{itemize}
\item
  \textbf{Generalised linear models}, e.g. \texttt{stats::glm()}. Linear
  models assume that the response is continuous and the error has a
  normal distribution. Generalised linear models extend linear models to
  include non-continuous responses (e.g.~binary data or counts). They
  work by defining a distance metric based on the statistical idea of
  likelihood.
\item
  \textbf{Generalised additive models}, e.g. \texttt{mgcv::gam()},
  extend generalised linear models to incorporate arbitrary smooth
  functions. That means you can write a formula like
  \texttt{y\ \textasciitilde{}\ s(x)} which becomes an equation like
  \texttt{y\ =\ f(x)} and let \texttt{gam()} estimate what that function
  is (subject to some smoothness constraints to make the problem
  tractable).
\item
  \textbf{Penalised linear models}, e.g. \texttt{glmnet::glmnet()}, add
  a penalty term to the distance that penalises complex models (as
  defined by the distance between the parameter vector and the origin).
  This tends to make models that generalise better to new datasets from
  the same population.
\item
  \textbf{Robust linear models}, e.g. \texttt{MASS:rlm()}, tweak the
  distance to downweight points that are very far away. This makes them
  less sensitive to the presence of outliers, at the cost of being not
  quite as good when there are no outliers.
\item
  \textbf{Trees}, e.g. \texttt{rpart::rpart()}, attack the problem in a
  completely different way than linear models. They fit a piece-wise
  constant model, splitting the data into progressively smaller and
  smaller pieces. Trees aren't terribly effective by themselves, but
  they are very powerful when used in aggregate by models like
  \textbf{random forests} (e.g. \texttt{randomForest::randomForest()})
  or \textbf{gradient boosting machines} (e.g.
  \texttt{xgboost::xgboost}.)
\end{itemize}

These models all work similarly from a programming perspective. Once
you've mastered linear models, you should find it easy to master the
mechanics of these other model classes. Being a skilled modeller is a
mixture of some good general principles and having a big toolbox of
techniques. Now that you've learned some general tools and one useful
class of models, you can go on and learn more classes from other
sources.

\hypertarget{model-building}{\chapter{Model
building}\label{model-building}}

\section{Introduction}\label{introduction-16}

In the previous chapter you learned how linear models worked, and
learned some basic tools for understanding what a model is telling you
about your data. The previous chapter focussed on simulated datasets to
help you learn about how models work. This chapter will focus on real
data, showing you how you can progressively build up a model to aid your
understanding of the data.

We will take advantage of the fact that you can think about a model
partitioning your data into pattern and residuals. We'll find patterns
with visualisation, then make them concrete and precise with a model.
We'll then repeat the process, but replace the old response variable
with the residuals from the model. The goal is to transition from
implicit knowledge in the data and your head to explicit knowledge in a
quantitative model. This makes it easier to apply to new domains, and
easier for others to use.

For very large and complex datasets this will be a lot of work. There
are certainly alternative approaches - a more machine learning approach
is simply to focus on the predictive ability of the model. These
approaches tend to produce black boxes: the model does a really good job
at generating predictions, but you don't know why. This is a totally
reasonable approach, but it does make it hard to apply your real world
knowledge to the model. That, in turn, makes it difficult to assess
whether or not the model will continue to work in the long-term, as
fundamentals change. For most real models, I'd expect you to use some
combination of this approach and a more classic automated approach.

It's a challenge to know when to stop. You need to figure out when your
model is good enough, and when additional investment is unlikely to pay
off. I particularly this quote from reddit user Broseidon241:

\begin{quote}
A long time ago in art class, my teacher told me ``An artist needs to
know when a piece is done. You can't tweak something into perfection -
wrap it up. If you don't like it, do it over again. Otherwise begin
something new''. Later in life, I heard ``A poor seamstress makes many
mistakes. A good seamstress works hard to correct those mistakes. A
great seamstress isn't afraid to throw out the garment and start over.''
\end{quote}

-- Broseidon241,
\url{https://www.reddit.com/r/datascience/comments/4irajq}

\subsection{Prerequisites}\label{prerequisites-16}

We'll use the same tools as in the previous chapter, but add in some
real datasets: \texttt{diamonds} from ggplot2, and \texttt{flights} from
nycflights13. We'll also need lubridate in order to work with the
date/times in \texttt{flights}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(modelr)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{na.action =} \NormalTok{na.warn)}

\KeywordTok{library}\NormalTok{(nycflights13)}
\KeywordTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

\section{Why are low quality diamonds more
expensive?}\label{diamond-prices}

In previous chapters we've seen a surprising relationship between the
quality of diamonds and their price: low quality diamonds (poor cuts,
bad colours, and inferior clarity) have higher prices.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(cut, price)) +}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{()}
\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(color, price)) +}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{()}
\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(clarity, price)) +}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-2-1} \includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-2-2} \includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-2-3} \end{center}

Note that the worst diamond color is J (slightly yellow), and the worst
clarity is I1 (inclusions visible to the naked eye).

\subsection{Price and carat}\label{price-and-carat}

It looks like lower quality diamonds have higher prices because there is
an important confounding variable: the weight (\texttt{carat}) of the
diamond. The weight of the diamond is the single most important factor
for determining the price of the diamond, and lower quality diamonds
tend to be larger.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(carat, price)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{)}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-3-1} \end{center}

We can make it easier to see how the other attributes of a diamond
affect its relative \texttt{price} by fitting a model to separate out
the effect of \texttt{carat}. But first, lets make a couple of tweaks to
the diamonds dataset to make it easier to work with:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Focus on diamonds smaller than 2.5 carats (99.7\% of the data)
\item
  Log-transform the carat and price variables.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(carat <=}\StringTok{ }\FloatTok{2.5}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{lprice =} \KeywordTok{log2}\NormalTok{(price), }\DataTypeTok{lcarat =} \KeywordTok{log2}\NormalTok{(carat))}
\end{Highlighting}
\end{Shaded}

Together, these changes make it easier to see the relationship between
\texttt{carat} and \texttt{price}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(lcarat, lprice)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{)}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-5-1} \end{center}

The log-transformation is particularly useful here because it makes the
pattern linear, and linear patterns are the easiest to work with. Let's
take the next step and remove that strong linear pattern. We first make
the pattern explicit by fitting a model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_diamond <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lprice ~}\StringTok{ }\NormalTok{lcarat, }\DataTypeTok{data =} \NormalTok{diamonds2)}
\end{Highlighting}
\end{Shaded}

Then we look at what the model tells us about the data. Note that I back
transform the predictions, undoing the log transformation, so I can
overlay the predictions on the raw data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\NormalTok{diamonds2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(}\DataTypeTok{carat =} \KeywordTok{seq_range}\NormalTok{(carat, }\DecValTok{20}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{lcarat =} \KeywordTok{log2}\NormalTok{(carat)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod_diamond, }\StringTok{"lprice"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{price =} \DecValTok{2} \NormalTok{^}\StringTok{ }\NormalTok{lprice)}

\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(carat, price)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =} \NormalTok{grid, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-7-1} \end{center}

That tells us something interesting about. If we believe our model, then
the large diamonds are much cheaper than expected. This is probably
because no diamond in this dataset costs more than \$19,000.

Now we can look at the residuals, which verifies that we've successfully
removed the strong linear pattern:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(mod_diamond, }\StringTok{"lresid"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(lcarat, lresid)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{)}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-8-1} \end{center}

Importantly, we can now re-do our motivating plots using those residuals
instead of \texttt{price}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(cut, lresid)) +}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{()}
\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(color, lresid)) +}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{()}
\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(clarity, lresid)) +}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-9-1} \includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-9-2} \includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-9-3} \end{center}

Now we see the relationship we expect: as the quality of the diamond
increases, so to does it's relative price. To interpret the \texttt{y}
axis, we need to think about what the residuals are telling us, and what
scale they are on. A residual of -1 indicates that \texttt{lprice} was 1
unit lower than a prediction based solely on its weight. \(2^{-1}\) is
1/2, points with a value of -1 are half the expected price, and
residuals with value 1 are twice the predicted price.

\subsection{A more complicated model}\label{a-more-complicated-model}

If we wanted to, we could continue to build up our model, moving the
effects we've observed into the model to make them explicit. For
example, we could include \texttt{color}, \texttt{cut}, and
\texttt{clarity} into the model so that we also make explicit the effect
of these three categorical variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_diamond2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lprice ~}\StringTok{ }\NormalTok{lcarat +}\StringTok{ }\NormalTok{color +}\StringTok{ }\NormalTok{cut +}\StringTok{ }\NormalTok{clarity, }\DataTypeTok{data =} \NormalTok{diamonds2)}
\end{Highlighting}
\end{Shaded}

This model now includes four predictors, so it's getting harder to
visualise. Fortunately, they're currently all independent which means
that we can plot them individually in four plots. To make the process a
little easier, we're going to use the \texttt{.model} argument to
\texttt{data\_grid}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\NormalTok{diamonds2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(cut, }\DataTypeTok{.model =} \NormalTok{mod_diamond2) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod_diamond2)}
\NormalTok{grid}
\CommentTok{#> # A tibble: 5 × 5}
\CommentTok{#>         cut lcarat color clarity  pred}
\CommentTok{#>       <ord>  <dbl> <chr>   <chr> <dbl>}
\CommentTok{#> 1      Fair -0.515     G     SI1  11.0}
\CommentTok{#> 2      Good -0.515     G     SI1  11.1}
\CommentTok{#> 3 Very Good -0.515     G     SI1  11.2}
\CommentTok{#> 4   Premium -0.515     G     SI1  11.2}
\CommentTok{#> 5     Ideal -0.515     G     SI1  11.2}

\KeywordTok{ggplot}\NormalTok{(grid, }\KeywordTok{aes}\NormalTok{(cut, pred)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-11-1} \end{center}

If the model needs variables that you haven't explicitly supplied,
\texttt{data\_grid()} will automatically fill them in with ``typical''
value. For continuous variables, it uses the median, and categorical
variables it uses the most common value (or values, if there's a tie).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(mod_diamond2, }\StringTok{"lresid2"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(lcarat, lresid2)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{)}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-12-1} \end{center}

This plot indicates that there are some diamonds with quite large
residuals - remember a residual of 2 indicates that the diamond is 4x
the price that we expected. It's often useful to look at unusual values
individually:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{abs}\NormalTok{(lresid2) >}\StringTok{ }\DecValTok{1}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod_diamond2) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pred =} \KeywordTok{round}\NormalTok{(}\DecValTok{2} \NormalTok{^}\StringTok{ }\NormalTok{pred)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(price, pred, carat:table, x:z) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(price)}
\CommentTok{#> # A tibble: 16 × 11}
\CommentTok{#>   price  pred carat     cut color clarity depth table     x     y     z}
\CommentTok{#>   <int> <dbl> <dbl>   <ord> <ord>   <ord> <dbl> <dbl> <dbl> <dbl> <dbl>}
\CommentTok{#> 1  1013   264  0.25    Fair     F     SI2  54.4    64  4.30  4.23  2.32}
\CommentTok{#> 2  1186   284  0.25 Premium     G     SI2  59.0    60  5.33  5.28  3.12}
\CommentTok{#> 3  1186   284  0.25 Premium     G     SI2  58.8    60  5.33  5.28  3.12}
\CommentTok{#> 4  1262  2644  1.03    Fair     E      I1  78.2    54  5.72  5.59  4.42}
\CommentTok{#> 5  1415   639  0.35    Fair     G     VS2  65.9    54  5.57  5.53  3.66}
\CommentTok{#> 6  1415   639  0.35    Fair     G     VS2  65.9    54  5.57  5.53  3.66}
\CommentTok{#> # ... with 10 more rows}
\end{Highlighting}
\end{Shaded}

Nothing really jumps out at me here, but it's probably worth spending
time considering if this indicates a problem with our model, or if there
are errors in the data. If there are mistakes in the data, this could be
an opportunity to buy diamonds that have been priced low incorrectly.

\subsection{Exercises}\label{exercises-63}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In the plot of \texttt{lcarat} vs. \texttt{lprice}, there are some
  bright vertical strips. What do they represent?
\item
  If \texttt{log(price)\ =\ a\_0\ +\ a\_1\ *\ log(carat)}, what does
  that say about the relationship between \texttt{price} and
  \texttt{carat}?
\item
  Extract the diamonds that have very high and very low residuals. Is
  there anything unusual about these diamonds? Are the particularly bad
  or good, or do you think these are pricing errors?
\item
  Does the final model, \texttt{mod\_diamonds2}, do a good job of
  predicting diamond prices? Would you trust it to tell you how much to
  spend if you were buying a diamond?
\end{enumerate}

\section{What affects the number of daily
flights?}\label{what-affects-the-number-of-daily-flights}

Let's work through a similar process for a dataset that seems even
simpler at first glance: the number of flights that leave NYC per day.
This is a really small dataset --- only 365 rows and 2 columns --- and
we're not going to end up with a fully realised model, but as you'll
see, the steps along the way will help us better understand the data.
Let's get started by counting the number of flights per day and
visualising it with ggplot2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily <-}\StringTok{ }\NormalTok{flights %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{date =} \KeywordTok{make_date}\NormalTok{(year, month, day)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(date) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\NormalTok{daily}
\CommentTok{#> # A tibble: 365 × 2}
\CommentTok{#>         date     n}
\CommentTok{#>       <date> <int>}
\CommentTok{#> 1 2013-01-01   842}
\CommentTok{#> 2 2013-01-02   943}
\CommentTok{#> 3 2013-01-03   914}
\CommentTok{#> 4 2013-01-04   915}
\CommentTok{#> 5 2013-01-05   720}
\CommentTok{#> 6 2013-01-06   832}
\CommentTok{#> # ... with 359 more rows}

\KeywordTok{ggplot}\NormalTok{(daily, }\KeywordTok{aes}\NormalTok{(date, n)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-14-1} \end{center}

\subsection{Day of week}\label{day-of-week}

Understanding the long-term trend is challenging because there's a very
strong day-of-week effect that dominates the subtler patterns. Let's
start by looking at the distribution of flight numbers by day-of-week:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily <-}\StringTok{ }\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{wday =} \KeywordTok{wday}\NormalTok{(date, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{))}
\KeywordTok{ggplot}\NormalTok{(daily, }\KeywordTok{aes}\NormalTok{(wday, n)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-15-1} \end{center}

There are fewer flights on weekends because most travel is for business.
The effect is particularly pronounced on Saturday: you might sometimes
leave on Sunday for a Monday morning meeting, but it's very rare that
you'd leave on Saturday as you'd much rather be at home with your
family.

One way to remove this strong pattern is to use a model. First, we fit
the model, and display its predictions overlaid on the original data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(n ~}\StringTok{ }\NormalTok{wday, }\DataTypeTok{data =} \NormalTok{daily)}

\NormalTok{grid <-}\StringTok{ }\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(wday) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod, }\StringTok{"n"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(daily, }\KeywordTok{aes}\NormalTok{(wday, n)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \NormalTok{grid, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-16-1} \end{center}

Next we compute and visualise the residuals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily <-}\StringTok{ }\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(mod)}
\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(date, resid)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_ref_line}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-17-1} \end{center}

Note the change in the y-axis: now we are seeing the deviation from the
expected number of flights, given the day of week. This plot is useful
because now that we've removed much of the large day-of-week effect, we
can see some of the subtler patterns that remain:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Our model seems to fail starting in June: you can still see a strong
  regular pattern that our model hasn't captured. Drawing a plot with
  one line for each day of the week makes the cause easier to see:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(daily, }\KeywordTok{aes}\NormalTok{(date, resid, }\DataTypeTok{colour =} \NormalTok{wday)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_ref_line}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-18-1} \end{center}

  Our model fails to accurately predict the number of flights on
  Saturday: during summer there are more flights than we expect, and
  during Fall there are fewer. We'll see how we can do better to capture
  this pattern in the next section.
\item
  There are some days with far fewer flights than expected:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(resid <}\StringTok{ }\NormalTok{-}\DecValTok{100}\NormalTok{)}
\CommentTok{#> # A tibble: 11 × 4}
\CommentTok{#>         date     n  wday resid}
\CommentTok{#>       <date> <int> <ord> <dbl>}
\CommentTok{#> 1 2013-01-01   842  Tues  -109}
\CommentTok{#> 2 2013-01-20   786   Sun  -105}
\CommentTok{#> 3 2013-05-26   729   Sun  -162}
\CommentTok{#> 4 2013-07-04   737 Thurs  -229}
\CommentTok{#> 5 2013-07-05   822   Fri  -145}
\CommentTok{#> 6 2013-09-01   718   Sun  -173}
\CommentTok{#> # ... with 5 more rows}
\end{Highlighting}
\end{Shaded}

  If you're familiar with American public holidays, you might spot New
  Year's day, July 4th, Thanksgiving and Christmas. There are some
  others that don't seem to correspond to public holidays. You'll work
  on those in one of the exercises.
\item
  There seems to be some smoother long term trend over the course of a
  year. We can highlight that trend with \texttt{geom\_smooth()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(date, resid)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_ref_line}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{colour =} \StringTok{"grey50"}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{span =} \FloatTok{0.20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-20-1} \end{center}

  There are fewer flights in January (and December), and more in summer
  (May-Sep). We can't do much with this pattern quantitatively, because
  we only have a single year of data. But we can use our domain
  knowledge to brainstorm potential explanations.
\end{enumerate}

\subsection{Seasonal Saturday effect}\label{seasonal-saturday-effect}

Let's first tackle our failure to accurately predict the number of
flights on Saturday. A good place to start is to go back to the raw
numbers, focussing on Saturdays:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(wday ==}\StringTok{ "Sat"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(date, n)) +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() +}
\StringTok{    }\KeywordTok{scale_x_date}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{date_breaks =} \StringTok{"1 month"}\NormalTok{, }\DataTypeTok{date_labels =} \StringTok{"%b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-21-1} \end{center}

(I've used both points and lines to make it more clear what is data and
what is interpolation.)

I suspect this pattern is caused by summer holidays: many people go on
holiday in the summer, and people don't mind travelling on Saturdays for
vacation. Looking at this plot, we might guess that summer holidays are
from early June to late August. That seems to line up fairly well with
the
\href{http://schools.nyc.gov/Calendar/2013-2014+School+Year+Calendars.htm}{state's
school terms}: summer break in 2013 was Jun 26--Sep 9.

Why are there more Saturday flights in the Spring than the Fall? I asked
some American friends and they suggested that it's less common to plan
family vacations during the Fall because of the big Thanksgiving and
Christmas holidays. We don't have the data to know for sure, but it
seems like a plausible working hypothesis.

Lets create a ``term'' variable that roughly captures the three school
terms, and check our work with a plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{term <-}\StringTok{ }\NormalTok{function(date) \{}
  \KeywordTok{cut}\NormalTok{(date, }
    \DataTypeTok{breaks =} \KeywordTok{ymd}\NormalTok{(}\DecValTok{20130101}\NormalTok{, }\DecValTok{20130605}\NormalTok{, }\DecValTok{20130825}\NormalTok{, }\DecValTok{20140101}\NormalTok{),}
    \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"spring"}\NormalTok{, }\StringTok{"summer"}\NormalTok{, }\StringTok{"fall"}\NormalTok{) }
  \NormalTok{)}
\NormalTok{\}}

\NormalTok{daily <-}\StringTok{ }\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{term =} \KeywordTok{term}\NormalTok{(date)) }

\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(wday ==}\StringTok{ "Sat"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(date, n, }\DataTypeTok{colour =} \NormalTok{term)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{3}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_x_date}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{date_breaks =} \StringTok{"1 month"}\NormalTok{, }\DataTypeTok{date_labels =} \StringTok{"%b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-22-1} \end{center}

(I manually tweaked the dates to get nice breaks in the plot. Using a
visualisation to help you understand what your function is doing is a
really powerful and general technique.)

It's useful to see how this new variable affects the other days of the
week:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(wday, n, }\DataTypeTok{colour =} \NormalTok{term)) +}
\StringTok{    }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-23-1} \end{center}

It looks like there is significant variation across the terms, so
fitting a separate day of week effect for each term is reasonable. This
improves our model, but not as much as we might hope:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(n ~}\StringTok{ }\NormalTok{wday, }\DataTypeTok{data =} \NormalTok{daily)}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(n ~}\StringTok{ }\NormalTok{wday *}\StringTok{ }\NormalTok{term, }\DataTypeTok{data =} \NormalTok{daily)}

\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather_residuals}\NormalTok{(}\DataTypeTok{without_term =} \NormalTok{mod1, }\DataTypeTok{with_term =} \NormalTok{mod2) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(date, resid, }\DataTypeTok{colour =} \NormalTok{model)) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-24-1} \end{center}

We can see the problem by overlaying the predictions from the model on
to the raw data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(wday, term) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod2, }\StringTok{"n"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(daily, }\KeywordTok{aes}\NormalTok{(wday, n)) +}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \NormalTok{grid, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~}\StringTok{ }\NormalTok{term)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-25-1} \end{center}

Our model is finding the \emph{mean} effect, but we have a lot of big
outliers, so mean tends to be far away from the typical value. We can
alleviate this problem by using a model that is robust to the effect of
outliers: \texttt{MASS::rlm()}. This greatly reduces the impact of the
outliers on our estimates, and gives a model that does a good job of
removing the day of week pattern:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod3 <-}\StringTok{ }\NormalTok{MASS::}\KeywordTok{rlm}\NormalTok{(n ~}\StringTok{ }\NormalTok{wday *}\StringTok{ }\NormalTok{term, }\DataTypeTok{data =} \NormalTok{daily)}

\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(mod3, }\StringTok{"resid"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(date, resid)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"white"}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-26-1} \end{center}

It's now much easier to see the long-term trend, and the positive and
negative outliers.

\subsection{Computed variables}\label{computed-variables}

If you're experimenting with many models and many visualisations, it's a
good idea to bundle the creation of variables up into a function so
there's no chance of accidentally applying a different transformation in
different places. For example, we could write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compute_vars <-}\StringTok{ }\NormalTok{function(data) \{}
  \NormalTok{data %>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}
      \DataTypeTok{term =} \KeywordTok{term}\NormalTok{(date), }
      \DataTypeTok{wday =} \KeywordTok{wday}\NormalTok{(date, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{)}
    \NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Another option is to put the transformations directly in the model
formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wday2 <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{wday}\NormalTok{(x, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{mod3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(n ~}\StringTok{ }\KeywordTok{wday2}\NormalTok{(date) *}\StringTok{ }\KeywordTok{term}\NormalTok{(date), }\DataTypeTok{data =} \NormalTok{daily)}
\end{Highlighting}
\end{Shaded}

Either approach is reasonable. Making the transformed variable explicit
is useful if you want to check your work, or use them in a
visualisation. But you can't easily use transformations (like splines)
that return multiple columns. Including the transformations in the model
function makes life a little easier when you're working with many
different datasets because the model is self contained.

\subsection{Time of year: an alternative
approach}\label{time-of-year-an-alternative-approach}

In the previous section we used our domain knowledge (how the US school
term affects travel) to improve the model. An alternative to using our
knowledge explicitly in the model is to give the data more room to
speak. We could use a more flexible model and allow that to capture the
pattern we're interested in. A simple linear trend isn't adequate, so we
could try using a natural spline to fit a smooth curve across the year:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(splines)}
\NormalTok{mod <-}\StringTok{ }\NormalTok{MASS::}\KeywordTok{rlm}\NormalTok{(n ~}\StringTok{ }\NormalTok{wday *}\StringTok{ }\KeywordTok{ns}\NormalTok{(date, }\DecValTok{5}\NormalTok{), }\DataTypeTok{data =} \NormalTok{daily)}

\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{data_grid}\NormalTok{(wday, }\DataTypeTok{date =} \KeywordTok{seq_range}\NormalTok{(date, }\DataTypeTok{n =} \DecValTok{13}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mod) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(date, pred, }\DataTypeTok{colour =} \NormalTok{wday)) +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() +}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-building_files/figure-latex/unnamed-chunk-29-1} \end{center}

We see a strong pattern in the numbers of Saturday flights. This is
reassuring, because we also saw that pattern in the raw data. It's a
good sign when you get the same signal from different approaches.

\subsection{Exercises}\label{exercises-64}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use your Google sleuthing skills to brainstorm why there were fewer
  than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all
  have the same explanation.) How would these days generalise to another
  year?
\item
  What do the three days with high positive residuals represent? How
  would these days generalise to another year?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{daily %>%}\StringTok{ }
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DecValTok{3}\NormalTok{, resid)}
\CommentTok{#> # A tibble: 3 × 5}
\CommentTok{#>         date     n  wday resid   term}
\CommentTok{#>       <date> <int> <ord> <dbl> <fctr>}
\CommentTok{#> 1 2013-11-30   857   Sat 112.4   fall}
\CommentTok{#> 2 2013-12-01   987   Sun  95.5   fall}
\CommentTok{#> 3 2013-12-28   814   Sat  69.4   fall}
\end{Highlighting}
\end{Shaded}
\item
  Create a new variable that splits the \texttt{wday} variable into
  terms, but only for Saturdays, i.e.~it should have \texttt{Thurs},
  \texttt{Fri}, but \texttt{Sat-summer}, \texttt{Sat-spring},
  \texttt{Sat-fall}. How does this model compare with the model with
  every combination of \texttt{wday} and \texttt{term}?
\item
  Create a new \texttt{wday} variable that combines the day of week,
  term (for Saturdays), and public holidays. What do the residuals of
  that model look like?
\item
  What happens if you fit a day of week effect that varies by month
  (i.e. \texttt{n\ \textasciitilde{}\ wday\ *\ month})? Why is this not
  very helpful?
\item
  What would you expect the model
  \texttt{n\ \textasciitilde{}\ wday\ +\ ns(date,\ 5)} to look like?
  Knowing what you know about the data, why would you expect it to be
  not particularly effective?
\item
  We hypothesised that people leaving on Sundays are more likely to be
  business travellers who need to be somewhere on Monday. Explore that
  hypothesis by seeing how it breaks down based on distance and time: if
  it's true, you'd expect to see more Sunday evening flights to places
  that are far away.
\item
  It's a little frustrating that Sunday and Saturday are on separate
  ends of the plot. Write a small function to set the levels of the
  factor so that the week starts on Monday.
\end{enumerate}

\hypertarget{learning-more-about-models}{\section{Learning more about
models}\label{learning-more-about-models}}

We have only scratched the absolute surface of modelling, but you have
hopefully gained some simple, but general-purpose tools that you can use
to improve your own data analyses. It's OK to start simple! As you've
seen, even very simple models can make a dramatic difference in your
ability to tease out interactions between variables.

These modelling chapters are even more opinionated than the rest of the
book. I approach modelling from a somewhat different perspective to most
others, and there is relatively little space devoted to it. Modelling
really deserves a book on its own, so I'd highly recommend that you read
at least one of these three books:

\begin{itemize}
\item
  \emph{Statistical Modeling: A Fresh Approach} by Danny Kaplan,
  \url{http://www.mosaic-web.org/go/StatisticalModeling/}. This book
  provides a gentle introduction to modelling, where you build your
  intuition, mathematical tools, and R skills in parallel. The book
  replaces a traditional ``introduction to statistics'' course,
  providing a curriculum that is up-to-date and relevant to data
  science.
\item
  \emph{An Introduction to Statistical Learning} by Gareth James,
  Daniela Witten, Trevor Hastie, and Robert Tibshirani,
  \url{http://www-bcf.usc.edu/~gareth/ISL/} (available online for free).
  This book presents a family of modern modelling techniques
  collectively known as statistical learning. For an even deeper
  understanding of the math behind the models, read the classic
  \emph{Elements of Statistical Learning} by Trevor Hastie, Robert
  Tibshirani, and Jerome Friedman,
  \url{http://statweb.stanford.edu/~tibs/ElemStatLearn/} (also available
  online for free).
\item
  \emph{Applied Predictive Modeling} by Max Kuhn and Kjell Johnson,
  \url{http://appliedpredictivemodeling.com}. This book is a companion
  to the \textbf{caret} package and provides practical tools for dealing
  with real-life predictive modelling challenges.
\end{itemize}

\hypertarget{many-models}{\chapter{Many models}\label{many-models}}

\section{Introduction}\label{introduction-17}

In this chapter you're going to learn three powerful ideas that help you
to work with large numbers of models with ease:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Using many simple models to better understand complex datasets.
\item
  Using list-columns to store arbitrary data structures in a data frame.
  For example, this will allow you to have a column that contains linear
  models.
\item
  Using the \textbf{broom} package, by David Robinson, to turn models
  into tidy data. This is a powerful technique for working with large
  numbers of models because once you have tidy data, you can apply all
  of the techniques that you've learned about earlier in the book.
\end{enumerate}

We'll start by diving into a motivating example using data about life
expectancy around the world. It's a small dataset but it illustrates how
important modelling can be for improving your visualisations. We'll use
a large number of simple models to partition out some of the strongest
signal so we can see the subtler signals that remain. We'll also see how
model summaries can help us pick out outliers and unusual trends.

The following sections will dive into more detail about the individual
techniques:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In \protect\hyperlink{list-columns-1}{list-columns}, you'll learn more
  about the list-column data structure, and why it's valid to put lists
  in data frames.
\item
  In \protect\hyperlink{creating-list-columns}{creating list-columns},
  you'll learn the three main ways in which you'll create list-columns.
\item
  In \protect\hyperlink{simplifying-list-columns}{simplifying
  list-columns} you'll learn how to convert list-columns back to regular
  atomic vectors (or sets of atomic vectors) so you can work with them
  more easily.
\item
  In \protect\hyperlink{making-tidy-data-with-broom}{making tidy data
  with broom}, you'll learn about the full set of tools provided by
  broom, and see how they can be applied to other types of data
  structure.
\end{enumerate}

This chapter is somewhat aspirational: if this book is your first
introduction to R, this chapter is likely to be a struggle. It requires
you have to deeply internalised ideas about modelling, data structures,
and iteration. So don't worry if you don't get it --- just put this
chapter aside for a few months, and come back when you want to stretch
your brain.

\subsection{Prerequisites}\label{prerequisites-17}

Working with many models requires many of the packages of the tidyverse
(for data exploration, wrangling, and programming) and modelr to
facilitate modelling.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(modelr)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{gapminder}\label{gapminder}

To motivate the power of many simple models, we're going to look into
the ``gapminder'' data. This data was popularised by Hans Rosling, a
Swedish doctor and statistician. If you've never heard of him, stop
reading this chapter right now and go watch one of his videos! He is a
fantastic data presenter and illustrates how you can use data to present
a compelling story. A good place to start is this short video filmed in
conjunction with the BBC:
\url{https://www.youtube.com/watch?v=jbkSRLYSojo}.

The gapminder data summarises the progression of countries over time,
looking at statistics like life expectancy and GDP. The data is easy to
access in R, thanks to Jenny Bryan who created the gapminder package:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gapminder)}
\NormalTok{gapminder}
\CommentTok{#> # A tibble: 1,704 × 6}
\CommentTok{#>       country continent  year lifeExp      pop gdpPercap}
\CommentTok{#>        <fctr>    <fctr> <int>   <dbl>    <int>     <dbl>}
\CommentTok{#> 1 Afghanistan      Asia  1952    28.8  8425333       779}
\CommentTok{#> 2 Afghanistan      Asia  1957    30.3  9240934       821}
\CommentTok{#> 3 Afghanistan      Asia  1962    32.0 10267083       853}
\CommentTok{#> 4 Afghanistan      Asia  1967    34.0 11537966       836}
\CommentTok{#> 5 Afghanistan      Asia  1972    36.1 13079460       740}
\CommentTok{#> 6 Afghanistan      Asia  1977    38.4 14880372       786}
\CommentTok{#> # ... with 1,698 more rows}
\end{Highlighting}
\end{Shaded}

In this case study, we're going to focus on just three variables to
answer the question ``How does life expectancy (\texttt{lifeExp}) change
over time (\texttt{year}) for each country (\texttt{country})?''. A good
place to start is with a plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(year, lifeExp, }\DataTypeTok{group =} \NormalTok{country)) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-3-1} \end{center}

This is a small dataset: it only has \textasciitilde{}1,700 observations
and 3 variables. But it's still hard to see what's going on! Overall, it
looks like life expectancy has been steadily improving. However, if you
look closely, you might notice some countries that don't follow this
pattern. How can we make those countries easier to see?

One way is to use the same approach as in the last chapter: there's a
strong signal (overall linear growth) that makes it hard to see subtler
trends. We'll tease these factors apart by fitting a model with a linear
trend. The model captures steady growth over time, and the residuals
will show what's left.

You already know how to do that if we had a single country:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nz <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(gapminder, country ==}\StringTok{ "New Zealand"}\NormalTok{)}
\NormalTok{nz %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(year, lifeExp)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Full data = "}\NormalTok{)}

\NormalTok{nz_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lifeExp ~}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =} \NormalTok{nz)}
\NormalTok{nz %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(nz_mod) %>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(year, pred)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Linear trend + "}\NormalTok{)}

\NormalTok{nz %>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(nz_mod) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(year, resid)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Remaining pattern"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.33\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-4-1}
\includegraphics[width=0.33\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-4-2}
\includegraphics[width=0.33\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-4-3}

How can we easily fit that model to every country?

\subsection{Nested data}\label{nested-data}

You could imagine copy and pasting that code multiple times; but you've
already learned a better way! Extract out the common code with a
function and repeat using a map function from purrr. This problem is
structured a little differently to what you've seen before. Instead of
repeating an action for each variable, we want to repeat an action for
each country, a subset of rows. To do that, we need a new data
structure: the \textbf{nested data frame}. To create a nested data frame
we start with a grouped data frame, and ``nest'' it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_country <-}\StringTok{ }\NormalTok{gapminder %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(country, continent) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{nest}\NormalTok{()}

\NormalTok{by_country}
\CommentTok{#> # A tibble: 142 × 3}
\CommentTok{#>       country continent              data}
\CommentTok{#>        <fctr>    <fctr>            <list>}
\CommentTok{#> 1 Afghanistan      Asia <tibble [12 × 4]>}
\CommentTok{#> 2     Albania    Europe <tibble [12 × 4]>}
\CommentTok{#> 3     Algeria    Africa <tibble [12 × 4]>}
\CommentTok{#> 4      Angola    Africa <tibble [12 × 4]>}
\CommentTok{#> 5   Argentina  Americas <tibble [12 × 4]>}
\CommentTok{#> 6   Australia   Oceania <tibble [12 × 4]>}
\CommentTok{#> # ... with 136 more rows}
\end{Highlighting}
\end{Shaded}

(I'm cheating a little by grouping on both \texttt{continent} and
\texttt{country}. Given \texttt{country}, \texttt{continent} is fixed,
so this doesn't add any more groups, but it's an easy way to carry an
extra variable along for the ride.)

This creates an data frame that has one row per group (per country), and
a rather unusual column: \texttt{data}. \texttt{data} is a list of data
frames (or tibbles, to be precise). This seems like a crazy idea: we
have a data frame with a column that is a list of other data frames!
I'll explain shortly why I think this is a good idea.

The \texttt{data} column is a little tricky to look at because it's a
moderately complicated list, and we're still working on good tools to
explore these objects. Unfortunately using \texttt{str()} is not
recommended as it will often produce very long output. But if you pluck
out a single element from the \texttt{data} column you'll see that it
contains all the data for that country (in this case, Afghanistan).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_country$data[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{#> # A tibble: 12 × 4}
\CommentTok{#>    year lifeExp      pop gdpPercap}
\CommentTok{#>   <int>   <dbl>    <int>     <dbl>}
\CommentTok{#> 1  1952    28.8  8425333       779}
\CommentTok{#> 2  1957    30.3  9240934       821}
\CommentTok{#> 3  1962    32.0 10267083       853}
\CommentTok{#> 4  1967    34.0 11537966       836}
\CommentTok{#> 5  1972    36.1 13079460       740}
\CommentTok{#> 6  1977    38.4 14880372       786}
\CommentTok{#> # ... with 6 more rows}
\end{Highlighting}
\end{Shaded}

Note the difference between a standard grouped data frame and a nested
data frame: in a grouped data frame, each row is an observation; in a
nested data frame, each row is a group. Another way to think about a
nested dataset is we now have a meta-observation: a row that represents
the complete time course for a country, rather than a single point in
time.

\subsection{List-columns}\label{list-columns}

Now that we have our nested data frame, we're in a good position to fit
some models. We have a model-fitting function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{country_model <-}\StringTok{ }\NormalTok{function(df) \{}
  \KeywordTok{lm}\NormalTok{(lifeExp ~}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =} \NormalTok{df)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And we want to apply it to every data frame. The data frames are in a
list, so we can use \texttt{purrr::map()} to apply
\texttt{country\_model} to each element:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\KeywordTok{map}\NormalTok{(by_country$data, country_model)}
\end{Highlighting}
\end{Shaded}

However, rather than leaving the list of models as a free-floating
object, I think it's better to store it as a column in the
\texttt{by\_country} data frame. Storing related objects in columns is a
key part of the value of data frames, and why I think list-columns are
such a good idea. In the course of working with these countries, we are
going to have lots of lists where we have one element per country. So
why not store them all together in one data frame?

In other words, instead of creating a new object in the global
environment, we're going to create a new variable in the
\texttt{by\_country} data frame. That's a job for
\texttt{dplyr::mutate()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_country <-}\StringTok{ }\NormalTok{by_country %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{map}\NormalTok{(data, country_model))}
\NormalTok{by_country}
\CommentTok{#> # A tibble: 142 × 4}
\CommentTok{#>       country continent              data    model}
\CommentTok{#>        <fctr>    <fctr>            <list>   <list>}
\CommentTok{#> 1 Afghanistan      Asia <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 2     Albania    Europe <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 3     Algeria    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 4      Angola    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 5   Argentina  Americas <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 6   Australia   Oceania <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> # ... with 136 more rows}
\end{Highlighting}
\end{Shaded}

This has a big advantage: because all the related objects are stored
together, you don't need to manually keep them in sync when you filter
or arrange. The semantics of the data frame takes care of that for you:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_country %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent ==}\StringTok{ "Europe"}\NormalTok{)}
\CommentTok{#> # A tibble: 30 × 4}
\CommentTok{#>                  country continent              data    model}
\CommentTok{#>                   <fctr>    <fctr>            <list>   <list>}
\CommentTok{#> 1                Albania    Europe <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 2                Austria    Europe <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 3                Belgium    Europe <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 4 Bosnia and Herzegovina    Europe <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 5               Bulgaria    Europe <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 6                Croatia    Europe <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> # ... with 24 more rows}
\NormalTok{by_country %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(continent, country)}
\CommentTok{#> # A tibble: 142 × 4}
\CommentTok{#>        country continent              data    model}
\CommentTok{#>         <fctr>    <fctr>            <list>   <list>}
\CommentTok{#> 1      Algeria    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 2       Angola    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 3        Benin    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 4     Botswana    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 5 Burkina Faso    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> 6      Burundi    Africa <tibble [12 × 4]> <S3: lm>}
\CommentTok{#> # ... with 136 more rows}
\end{Highlighting}
\end{Shaded}

If your list of data frames and list of models were separate objects,
you have to remember that whenever you re-order or subset one vector,
you need to re-order or subset all the others in order to keep them in
sync. If you forget, your code will continue to work, but it will give
the wrong answer!

\subsection{Unnesting}\label{unnesting}

Previously we computed the residuals of a single model with a single
dataset. Now we have 142 data frames and 142 models. To compute the
residuals, we need to call \texttt{add\_residuals()} with each
model-data pair:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_country <-}\StringTok{ }\NormalTok{by_country %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{resids =} \KeywordTok{map2}\NormalTok{(data, model, add_residuals)}
  \NormalTok{)}
\NormalTok{by_country}
\CommentTok{#> # A tibble: 142 × 5}
\CommentTok{#>       country continent              data    model            resids}
\CommentTok{#>        <fctr>    <fctr>            <list>   <list>            <list>}
\CommentTok{#> 1 Afghanistan      Asia <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 2     Albania    Europe <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 3     Algeria    Africa <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 4      Angola    Africa <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 5   Argentina  Americas <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 6   Australia   Oceania <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> # ... with 136 more rows}
\end{Highlighting}
\end{Shaded}

But how you can plot a list of data frames? Instead of struggling to
answer that question, let's turn the list of data frames back into a
regular data frame. Previously we used \texttt{nest()} to turn a regular
data frame into an nested data frame, and now we do the opposite with
\texttt{unnest()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resids <-}\StringTok{ }\KeywordTok{unnest}\NormalTok{(by_country, resids)}
\NormalTok{resids}
\CommentTok{#> # A tibble: 1,704 × 7}
\CommentTok{#>       country continent  year lifeExp      pop gdpPercap   resid}
\CommentTok{#>        <fctr>    <fctr> <int>   <dbl>    <int>     <dbl>   <dbl>}
\CommentTok{#> 1 Afghanistan      Asia  1952    28.8  8425333       779 -1.1063}
\CommentTok{#> 2 Afghanistan      Asia  1957    30.3  9240934       821 -0.9519}
\CommentTok{#> 3 Afghanistan      Asia  1962    32.0 10267083       853 -0.6636}
\CommentTok{#> 4 Afghanistan      Asia  1967    34.0 11537966       836 -0.0172}
\CommentTok{#> 5 Afghanistan      Asia  1972    36.1 13079460       740  0.6741}
\CommentTok{#> 6 Afghanistan      Asia  1977    38.4 14880372       786  1.6475}
\CommentTok{#> # ... with 1,698 more rows}
\end{Highlighting}
\end{Shaded}

Note that each regular column is repeated one for each row in the nested
column.

Now we have regular data frame, we can plot the residuals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resids %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(year, resid)) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =} \NormalTok{country), }\DataTypeTok{alpha =} \DecValTok{1} \NormalTok{/}\StringTok{ }\DecValTok{3}\NormalTok{) +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-13-1} \end{center}

Facetting by continent is particularly revealing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resids %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(year, resid, }\DataTypeTok{group =} \NormalTok{country)) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1} \NormalTok{/}\StringTok{ }\DecValTok{3}\NormalTok{) +}\StringTok{ }
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(~continent)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-14-1} \end{center}

It looks like we've missed some mild patterns. There's also something
interesting going on in Africa: we see some very large residuals which
suggests our model isn't fitting so well there. We'll explore that more
in the next section, attacking it from a slightly different angle.

\subsection{Model quality}\label{model-quality}

Instead of looking at the residuals from the model, we could look at
some general measurements of model quality. You learned how to compute
some specific measures in the previous chapter. Here we'll show a
different approach using the broom package. The broom package provides a
general set of functions to turn models into tidy data. Here we'll use
\texttt{broom::glance()} to extract some model quality metrics. If we
apply it to a model, we get a data frame with a single row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom::}\KeywordTok{glance}\NormalTok{(nz_mod)}
\CommentTok{#>   r.squared adj.r.squared sigma statistic  p.value df logLik  AIC  BIC}
\CommentTok{#> 1     0.954         0.949 0.804       205 5.41e-08  2  -13.3 32.6 34.1}
\CommentTok{#>   deviance df.residual}
\CommentTok{#> 1     6.47          10}
\end{Highlighting}
\end{Shaded}

We can use \texttt{mutate()} and \texttt{unnest()} to create a data
frame with a row for each country:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by_country %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{glance =} \KeywordTok{map}\NormalTok{(model, broom::glance)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{(glance)}
\CommentTok{#> # A tibble: 142 × 16}
\CommentTok{#>       country continent              data    model            resids}
\CommentTok{#>        <fctr>    <fctr>            <list>   <list>            <list>}
\CommentTok{#> 1 Afghanistan      Asia <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 2     Albania    Europe <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 3     Algeria    Africa <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 4      Angola    Africa <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 5   Argentina  Americas <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> 6   Australia   Oceania <tibble [12 × 4]> <S3: lm> <tibble [12 × 5]>}
\CommentTok{#> # ... with 136 more rows, and 11 more variables: r.squared <dbl>,}
\CommentTok{#> #   adj.r.squared <dbl>, sigma <dbl>, statistic <dbl>, p.value <dbl>,}
\CommentTok{#> #   df <int>, logLik <dbl>, AIC <dbl>, BIC <dbl>, deviance <dbl>,}
\CommentTok{#> #   df.residual <int>}
\end{Highlighting}
\end{Shaded}

This isn't quite the output we want, because it still includes all the
list columns. This is default behaviour when \texttt{unnest()} works on
single row data frames. To suppress these columns we use
\texttt{.drop\ =\ TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glance <-}\StringTok{ }\NormalTok{by_country %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{glance =} \KeywordTok{map}\NormalTok{(model, broom::glance)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{(glance, }\DataTypeTok{.drop =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{glance}
\CommentTok{#> # A tibble: 142 × 13}
\CommentTok{#>       country continent r.squared adj.r.squared sigma statistic  p.value}
\CommentTok{#>        <fctr>    <fctr>     <dbl>         <dbl> <dbl>     <dbl>    <dbl>}
\CommentTok{#> 1 Afghanistan      Asia     0.948         0.942 1.223     181.2 9.84e-08}
\CommentTok{#> 2     Albania    Europe     0.911         0.902 1.983     101.8 1.46e-06}
\CommentTok{#> 3     Algeria    Africa     0.985         0.984 1.323     661.9 1.81e-10}
\CommentTok{#> 4      Angola    Africa     0.888         0.877 1.407      79.1 4.59e-06}
\CommentTok{#> 5   Argentina  Americas     0.996         0.995 0.292    2246.4 4.22e-13}
\CommentTok{#> 6   Australia   Oceania     0.980         0.978 0.621     481.3 8.67e-10}
\CommentTok{#> # ... with 136 more rows, and 6 more variables: df <int>, logLik <dbl>,}
\CommentTok{#> #   AIC <dbl>, BIC <dbl>, deviance <dbl>, df.residual <int>}
\end{Highlighting}
\end{Shaded}

(Pay attention to the variables that aren't printed: there's a lot of
useful stuff there.)

With this data frame in hand, we can start to look for models that don't
fit well:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glance %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(r.squared)}
\CommentTok{#> # A tibble: 142 × 13}
\CommentTok{#>     country continent r.squared adj.r.squared sigma statistic p.value}
\CommentTok{#>      <fctr>    <fctr>     <dbl>         <dbl> <dbl>     <dbl>   <dbl>}
\CommentTok{#> 1    Rwanda    Africa    0.0172      -0.08112  6.56     0.175   0.685}
\CommentTok{#> 2  Botswana    Africa    0.0340      -0.06257  6.11     0.352   0.566}
\CommentTok{#> 3  Zimbabwe    Africa    0.0562      -0.03814  7.21     0.596   0.458}
\CommentTok{#> 4    Zambia    Africa    0.0598      -0.03418  4.53     0.636   0.444}
\CommentTok{#> 5 Swaziland    Africa    0.0682      -0.02497  6.64     0.732   0.412}
\CommentTok{#> 6   Lesotho    Africa    0.0849      -0.00666  5.93     0.927   0.358}
\CommentTok{#> # ... with 136 more rows, and 6 more variables: df <int>, logLik <dbl>,}
\CommentTok{#> #   AIC <dbl>, BIC <dbl>, deviance <dbl>, df.residual <int>}
\end{Highlighting}
\end{Shaded}

The worst models all appear to be in Africa. Let's double check that
with a plot. Here we have a relatively small number of observations and
a discrete variable, so \texttt{geom\_jitter()} is effective:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glance %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(continent, r.squared)) +}\StringTok{ }
\StringTok{    }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-19-1} \end{center}

We could pull out the countries with particularly bad \(R^2\) and plot
the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bad_fit <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(glance, r.squared <}\StringTok{ }\FloatTok{0.25}\NormalTok{)}

\NormalTok{gapminder %>%}\StringTok{ }
\StringTok{  }\KeywordTok{semi_join}\NormalTok{(bad_fit, }\DataTypeTok{by =} \StringTok{"country"}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(year, lifeExp, }\DataTypeTok{colour =} \NormalTok{country)) +}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/model-many_files/figure-latex/unnamed-chunk-20-1} \end{center}

We see two main effects here: the tragedies of the HIV/AIDS epidemic and
the Rwandan genocide.

\subsection{Exercises}\label{exercises-65}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A linear trend seems to be slightly too simple for the overall trend.
  Can you do better with a quadratic polynomial? How can you interpret
  the coefficients of the quadratic? (Hint you might want to transform
  \texttt{year} so that it has mean zero.)
\item
  Explore other methods for visualising the distribution of \(R^2\) per
  continent. You might want to try the ggbeeswarm package, which
  provides similar methods for avoiding overlaps as jitter, but uses
  deterministic methods.
\item
  To create the last plot (showing the data for the countries with the
  worst model fits), we needed two steps: we created a data frame with
  one row per country and then semi-joined it to the original dataset.
  It's possible avoid this join if we use \texttt{unnest()} instead of
  \texttt{unnest(.drop\ =\ TRUE)}. How?
\end{enumerate}

\hypertarget{list-columns-1}{\section{List-columns}\label{list-columns-1}}

Now that you've seen a basic workflow for managing many models, let's
dive back into some of the details. In this section, we'll explore the
list-column data structure in a little more detail. It's only recently
that I've really appreciated the idea of the list-column. List-columns
are implicit in the definition of the data frame: a data frame is a
named list of equal length vectors. A list is a vector, so it's always
been legitimate to put use a list as a column of a data frame. However,
base R doesn't make it easy to create list-columns, and
\texttt{data.frame()} treats a list as a list of columns:.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{:}\DecValTok{5}\NormalTok{))}
\CommentTok{#>   x.1.3 x.3.5}
\CommentTok{#> 1     1     3}
\CommentTok{#> 2     2     4}
\CommentTok{#> 3     3     5}
\end{Highlighting}
\end{Shaded}

You can prevent \texttt{data.frame()} from doing this with \texttt{I()},
but the result doesn't print particularly well:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{I}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{:}\DecValTok{5}\NormalTok{)), }
  \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\StringTok{"1, 2"}\NormalTok{, }\StringTok{"3, 4, 5"}\NormalTok{)}
\NormalTok{)}
\CommentTok{#>         x       y}
\CommentTok{#> 1 1, 2, 3    1, 2}
\CommentTok{#> 2 3, 4, 5 3, 4, 5}
\end{Highlighting}
\end{Shaded}

Tibble alleviates this problem by being lazier (\texttt{tibble()}
doesn't modify its inputs) and by providing a better print method:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{:}\DecValTok{5}\NormalTok{), }
  \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\StringTok{"1, 2"}\NormalTok{, }\StringTok{"3, 4, 5"}\NormalTok{)}
\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 2}
\CommentTok{#>           x       y}
\CommentTok{#>      <list>   <chr>}
\CommentTok{#> 1 <int [3]>    1, 2}
\CommentTok{#> 2 <int [3]> 3, 4, 5}
\end{Highlighting}
\end{Shaded}

It's even easier with \texttt{tribble()} as it can automatically work
out that you need a list:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tribble}\NormalTok{(}
   \NormalTok{~x, ~y,}
  \DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{"1, 2"}\NormalTok{,}
  \DecValTok{3}\NormalTok{:}\DecValTok{5}\NormalTok{, }\StringTok{"3, 4, 5"}
\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 2}
\CommentTok{#>           x       y}
\CommentTok{#>      <list>   <chr>}
\CommentTok{#> 1 <int [3]>    1, 2}
\CommentTok{#> 2 <int [3]> 3, 4, 5}
\end{Highlighting}
\end{Shaded}

List-columns are often most useful as intermediate data structure.
They're hard to work with directly, because most R functions work with
atomic vectors or data frames, but the advantage of keeping related
items together in a data frame is worth a little hassle.

Generally there are three parts of an effective list-column pipeline:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You create the list-column using one of \texttt{nest()},
  \texttt{summarise()} + \texttt{list()}, or \texttt{mutate()} + a map
  function, as described in
  \protect\hyperlink{creating-list-columns}{Creating list-columns}.
\item
  You create other intermediate list-columns by transforming existing
  list columns with \texttt{map()}, \texttt{map2()} or \texttt{pmap()}.
  For example, in the case study above, we created a list-column of
  models by transforming a list-column of data frames.
\item
  You simplify the list-column back down to a data frame or atomic
  vector, as described in
  \protect\hyperlink{simplifying-list-columns}{Simplifying
  list-columns}.
\end{enumerate}

\hypertarget{creating-list-columns}{\section{Creating
list-columns}\label{creating-list-columns}}

Typically, you won't create list-columns with \texttt{tibble()}.
Instead, you'll create them from regular columns, using one of three
methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  With \texttt{tidyr::nest()} to convert a grouped data frame into a
  nested data frame where you have list-column of data frames.
\item
  With \texttt{mutate()} and vectorised functions that return a list.
\item
  With \texttt{summarise()} and summary functions that return multiple
  results.
\end{enumerate}

Alternatively, you might create them from a named list, using
\texttt{tibble::enframe()}.

Generally, when creating list-columns, you should make sure they're
homogeneous: each element should contain the same type of thing. There
are no checks to make sure this is true, but if you use purrr and
remember what you've learned about type-stable functions, you should
find it happens naturally.

\subsection{With nesting}\label{with-nesting}

\texttt{nest()} creates a nested data frame, which is a data frame with
a list-column of data frames. In a nested data frame each row is a
meta-observation: the other columns give variables that define the
observation (like country and continent above), and the list-column of
data frames gives the individual observations that make up the
meta-observation.

There are two ways to use \texttt{nest()}. So far you've seen how to use
it with a grouped data frame. When applied to a grouped data frame,
\texttt{nest()} keeps the grouping columns as is, and bundles everything
else into the list-column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(country, continent) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{nest}\NormalTok{()}
\CommentTok{#> # A tibble: 142 × 3}
\CommentTok{#>       country continent              data}
\CommentTok{#>        <fctr>    <fctr>            <list>}
\CommentTok{#> 1 Afghanistan      Asia <tibble [12 × 4]>}
\CommentTok{#> 2     Albania    Europe <tibble [12 × 4]>}
\CommentTok{#> 3     Algeria    Africa <tibble [12 × 4]>}
\CommentTok{#> 4      Angola    Africa <tibble [12 × 4]>}
\CommentTok{#> 5   Argentina  Americas <tibble [12 × 4]>}
\CommentTok{#> 6   Australia   Oceania <tibble [12 × 4]>}
\CommentTok{#> # ... with 136 more rows}
\end{Highlighting}
\end{Shaded}

You can also use it on an ungrouped data frame, specifying which columns
you want to nest:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder %>%}\StringTok{ }
\StringTok{  }\KeywordTok{nest}\NormalTok{(year:gdpPercap)}
\CommentTok{#> # A tibble: 142 × 3}
\CommentTok{#>       country continent              data}
\CommentTok{#>        <fctr>    <fctr>            <list>}
\CommentTok{#> 1 Afghanistan      Asia <tibble [12 × 4]>}
\CommentTok{#> 2     Albania    Europe <tibble [12 × 4]>}
\CommentTok{#> 3     Algeria    Africa <tibble [12 × 4]>}
\CommentTok{#> 4      Angola    Africa <tibble [12 × 4]>}
\CommentTok{#> 5   Argentina  Americas <tibble [12 × 4]>}
\CommentTok{#> 6   Australia   Oceania <tibble [12 × 4]>}
\CommentTok{#> # ... with 136 more rows}
\end{Highlighting}
\end{Shaded}

\subsection{From vectorised functions}\label{from-vectorised-functions}

Some useful functions take an atomic vector and return a list. For
example, in \protect\hyperlink{strings}{strings} you learned about
\texttt{stringr::str\_split()} which takes a character vector and
returns a list of character vectors. If you use that inside mutate,
you'll get a list-column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x1,}
  \StringTok{"a,b,c"}\NormalTok{, }
  \StringTok{"d,e,f,g"}
\NormalTok{) }

\NormalTok{df %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x2 =} \NormalTok{stringr::}\KeywordTok{str_split}\NormalTok{(x1, }\StringTok{","}\NormalTok{))}
\CommentTok{#> # A tibble: 2 × 2}
\CommentTok{#>        x1        x2}
\CommentTok{#>     <chr>    <list>}
\CommentTok{#> 1   a,b,c <chr [3]>}
\CommentTok{#> 2 d,e,f,g <chr [4]>}
\end{Highlighting}
\end{Shaded}

\texttt{unnest()} knows how to handle these lists of vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x2 =} \NormalTok{stringr::}\KeywordTok{str_split}\NormalTok{(x1, }\StringTok{","}\NormalTok{)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{()}
\CommentTok{#> # A tibble: 7 × 2}
\CommentTok{#>        x1    x2}
\CommentTok{#>     <chr> <chr>}
\CommentTok{#> 1   a,b,c     a}
\CommentTok{#> 2   a,b,c     b}
\CommentTok{#> 3   a,b,c     c}
\CommentTok{#> 4 d,e,f,g     d}
\CommentTok{#> 5 d,e,f,g     e}
\CommentTok{#> 6 d,e,f,g     f}
\CommentTok{#> # ... with 1 more rows}
\end{Highlighting}
\end{Shaded}

(If you find yourself using this pattern a lot, make sure to check out
\texttt{tidyr:separate\_rows()} which is a wrapper around this common
pattern).

Another example of this pattern is using the \texttt{map()},
\texttt{map2()}, \texttt{pmap()} from purrr. For example, we could take
the final example from
\protect\hyperlink{invoking-different-functions}{Invoking different
functions} and rewrite it to use \texttt{mutate()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~f,      ~params,}
  \StringTok{"runif"}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{min =} \NormalTok{-}\DecValTok{1}\NormalTok{, }\DataTypeTok{max =} \NormalTok{-}\DecValTok{1}\NormalTok{),}
  \StringTok{"rnorm"}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{sd =} \DecValTok{5}\NormalTok{),}
  \StringTok{"rpois"}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{lambda =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}

\NormalTok{sim %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sims =} \KeywordTok{invoke_map}\NormalTok{(f, params, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{))}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>       f     params       sims}
\CommentTok{#>   <chr>     <list>     <list>}
\CommentTok{#> 1 runif <list [2]> <dbl [10]>}
\CommentTok{#> 2 rnorm <list [1]> <dbl [10]>}
\CommentTok{#> 3 rpois <list [1]> <int [10]>}
\end{Highlighting}
\end{Shaded}

Note that technically \texttt{sim} isn't homogeneous because it contains
both double and integer vectors. However, this is unlikely to cause many
problems since integers and doubles are both numeric vectors.

\subsection{From multivalued
summaries}\label{from-multivalued-summaries}

One restriction of \texttt{summarise()} is that it only works with
summary functions that return a single value. That means that you can't
use it with functions like \texttt{quantile()} that return a vector of
arbitrary length:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{q =} \KeywordTok{quantile}\NormalTok{(mpg))}
\CommentTok{#> Error in eval(expr, envir, enclos): expecting a single value}
\end{Highlighting}
\end{Shaded}

You can however, wrap the result in a list! This obeys the contract of
\texttt{summarise()}, because each summary is now a list (a vector) of
length 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{q =} \KeywordTok{list}\NormalTok{(}\KeywordTok{quantile}\NormalTok{(mpg)))}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>     cyl         q}
\CommentTok{#>   <dbl>    <list>}
\CommentTok{#> 1     4 <dbl [5]>}
\CommentTok{#> 2     6 <dbl [5]>}
\CommentTok{#> 3     8 <dbl [5]>}
\end{Highlighting}
\end{Shaded}

To make useful results with unnest, you'll also need to capture the
probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\FloatTok{0.99}\NormalTok{)}
\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{p =} \KeywordTok{list}\NormalTok{(probs), }\DataTypeTok{q =} \KeywordTok{list}\NormalTok{(}\KeywordTok{quantile}\NormalTok{(mpg, probs))) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{()}
\CommentTok{#> # A tibble: 15 × 3}
\CommentTok{#>     cyl     p     q}
\CommentTok{#>   <dbl> <dbl> <dbl>}
\CommentTok{#> 1     4  0.01  21.4}
\CommentTok{#> 2     4  0.25  22.8}
\CommentTok{#> 3     4  0.50  26.0}
\CommentTok{#> 4     4  0.75  30.4}
\CommentTok{#> 5     4  0.99  33.8}
\CommentTok{#> 6     6  0.01  17.8}
\CommentTok{#> # ... with 9 more rows}
\end{Highlighting}
\end{Shaded}

\subsection{From a named list}\label{from-a-named-list}

Data frames with list-columns provide a solution to a common problem:
what do you do if you want to iterate over both the contents of a list
and its elements? Instead of trying to jam everything into one object,
it's often easier to make a data frame: one column can contain the
elements, and one column can contain the list. An easy way to create
such a data frame from a list is \texttt{tibble::enframe()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{a =} \DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{,}
  \DataTypeTok{b =} \DecValTok{3}\NormalTok{:}\DecValTok{4}\NormalTok{, }
  \DataTypeTok{c =} \DecValTok{5}\NormalTok{:}\DecValTok{6}
\NormalTok{) }

\NormalTok{df <-}\StringTok{ }\KeywordTok{enframe}\NormalTok{(x)}
\NormalTok{df}
\CommentTok{#> # A tibble: 3 × 2}
\CommentTok{#>    name     value}
\CommentTok{#>   <chr>    <list>}
\CommentTok{#> 1     a <int [5]>}
\CommentTok{#> 2     b <int [2]>}
\CommentTok{#> 3     c <int [2]>}
\end{Highlighting}
\end{Shaded}

The advantage of this structure is that it generalises in a
straightforward way - names are useful if you have character vector of
metadata, but don't help if you have other types of data, or multiple
vectors.

Now if you want to iterate over names and values in parallel, you can
use \texttt{map2()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df %>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{smry =} \KeywordTok{map2_chr}\NormalTok{(name, value, ~}\StringTok{ }\NormalTok{stringr::}\KeywordTok{str_c}\NormalTok{(.x, }\StringTok{": "}\NormalTok{, .y[}\DecValTok{1}\NormalTok{]))}
  \NormalTok{)}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>    name     value  smry}
\CommentTok{#>   <chr>    <list> <chr>}
\CommentTok{#> 1     a <int [5]>  a: 1}
\CommentTok{#> 2     b <int [2]>  b: 3}
\CommentTok{#> 3     c <int [2]>  c: 5}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-66}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  List all the functions that you can think of that take a atomic vector
  and return a list.
\item
  Brainstorm useful summary functions that, like \texttt{quantile()},
  return multiple values.
\item
  What's missing in the following data frame? How does
  \texttt{quantile()} return that missing piece? Why isn't that helpful
  here?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{q =} \KeywordTok{list}\NormalTok{(}\KeywordTok{quantile}\NormalTok{(mpg))) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{()}
\CommentTok{#> # A tibble: 15 × 2}
\CommentTok{#>     cyl     q}
\CommentTok{#>   <dbl> <dbl>}
\CommentTok{#> 1     4  21.4}
\CommentTok{#> 2     4  22.8}
\CommentTok{#> 3     4  26.0}
\CommentTok{#> 4     4  30.4}
\CommentTok{#> 5     4  33.9}
\CommentTok{#> 6     6  17.8}
\CommentTok{#> # ... with 9 more rows}
\end{Highlighting}
\end{Shaded}
\item
  What does this code do? Why might might it be useful?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars %>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(cyl) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise_each}\NormalTok{(}\KeywordTok{funs}\NormalTok{(list))}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\hypertarget{simplifying-list-columns}{\section{Simplifying
list-columns}\label{simplifying-list-columns}}

To apply the techniques of data manipulation and visualisation you've
learned in this book, you'll need to simplify the list-column back to a
regular column (an atomic vector), or set of columns. The technique
you'll use to collapse back down to a simpler structure depends on
whether you want a single value per element, or multiple values:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If you want a single value, use \texttt{mutate()} with
  \texttt{map\_lgl()}, \texttt{map\_int()}, \texttt{map\_dbl()}, and
  \texttt{map\_chr()} to create an atomic vector.
\item
  If you want many values, use \texttt{unnest()} to convert list-columns
  back to regular columns, repeating the rows as many times as
  necessary.
\end{enumerate}

These are described in more detail below.

\subsection{List to vector}\label{list-to-vector}

If you can reduce your list column to an atomic vector then it will be a
regular column. For example, you can always summarise an object with
it's type and length, so this code will work regardless of what sort of
list-column you have:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x,}
  \NormalTok{letters[}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{],}
  \DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{,}
  \KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{)}
  
\NormalTok{df %>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}
  \DataTypeTok{type =} \KeywordTok{map_chr}\NormalTok{(x, typeof),}
  \DataTypeTok{length =} \KeywordTok{map_int}\NormalTok{(x, length)}
\NormalTok{)}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>           x      type length}
\CommentTok{#>      <list>     <chr>  <int>}
\CommentTok{#> 1 <chr [5]> character      5}
\CommentTok{#> 2 <int [3]>   integer      3}
\CommentTok{#> 3 <dbl [5]>    double      5}
\end{Highlighting}
\end{Shaded}

This is the same basic information that you get from the default tbl
print method, but now you can use it for filtering. This is a useful
technique if you have a heterogeneous list, and want to filter out the
parts aren't working for you.

Don't forget about the \texttt{map\_*()} shortcuts - you can use
\texttt{map\_chr(x,\ "apple")} to extract the string stored in
\texttt{apple} for each element of \texttt{x}. This is useful for
pulling apart nested lists into regular columns. Use the \texttt{.null}
argument to provide a value to use if the element is missing (instead of
returning \texttt{NULL}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x,}
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\NormalTok{, }\DataTypeTok{b =} \DecValTok{2}\NormalTok{),}
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{a =} \DecValTok{2}\NormalTok{, }\DataTypeTok{c =} \DecValTok{4}\NormalTok{)}
\NormalTok{)}
\NormalTok{df %>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}
  \DataTypeTok{a =} \KeywordTok{map_dbl}\NormalTok{(x, }\StringTok{"a"}\NormalTok{),}
  \DataTypeTok{b =} \KeywordTok{map_dbl}\NormalTok{(x, }\StringTok{"b"}\NormalTok{, }\DataTypeTok{.null =} \OtherTok{NA_real_}\NormalTok{)}
\NormalTok{)}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>            x     a     b}
\CommentTok{#>       <list> <dbl> <dbl>}
\CommentTok{#> 1 <list [2]>     1     2}
\CommentTok{#> 2 <list [2]>     2    NA}
\end{Highlighting}
\end{Shaded}

\subsection{Unnesting}\label{unnesting-1}

\texttt{unnest()} works by repeating the regular columns once for each
element of the list-column. For example, in the following very simple
example we repeat the first row 4 times (because there the first element
of \texttt{y} has length four), and the second row once:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{y =} \KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{)) %>%}\StringTok{ }\KeywordTok{unnest}\NormalTok{(y)}
\CommentTok{#> # A tibble: 5 × 2}
\CommentTok{#>       x     y}
\CommentTok{#>   <int> <dbl>}
\CommentTok{#> 1     1     1}
\CommentTok{#> 2     1     2}
\CommentTok{#> 3     1     3}
\CommentTok{#> 4     1     4}
\CommentTok{#> 5     2     1}
\end{Highlighting}
\end{Shaded}

This means that you can't simultaneously unnest two columns that contain
different number of elements:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ok, because y and z have the same number of elements in}
\CommentTok{# every row}
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x, ~y,           ~z,}
   \DecValTok{1}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{), }\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{,}
   \DecValTok{2}\NormalTok{, }\StringTok{"c"}\NormalTok{,           }\DecValTok{3}
\NormalTok{)}
\NormalTok{df1}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>       x         y         z}
\CommentTok{#>   <dbl>    <list>    <list>}
\CommentTok{#> 1     1 <chr [2]> <int [2]>}
\CommentTok{#> 2     2 <chr [1]> <dbl [1]>}
\NormalTok{df1 %>%}\StringTok{ }\KeywordTok{unnest}\NormalTok{(y, z)}
\CommentTok{#> # A tibble: 3 × 3}
\CommentTok{#>       x     y     z}
\CommentTok{#>   <dbl> <chr> <dbl>}
\CommentTok{#> 1     1     a     1}
\CommentTok{#> 2     1     b     2}
\CommentTok{#> 3     2     c     3}

\CommentTok{# Doesn't work because y and z have different number of elements}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \NormalTok{~x, ~y,           ~z,}
   \DecValTok{1}\NormalTok{, }\StringTok{"a"}\NormalTok{,         }\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{,  }
   \DecValTok{2}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{),   }\DecValTok{3}
\NormalTok{)}
\NormalTok{df2}
\CommentTok{#> # A tibble: 2 × 3}
\CommentTok{#>       x         y         z}
\CommentTok{#>   <dbl>    <list>    <list>}
\CommentTok{#> 1     1 <chr [1]> <int [2]>}
\CommentTok{#> 2     2 <chr [2]> <dbl [1]>}
\NormalTok{df2 %>%}\StringTok{ }\KeywordTok{unnest}\NormalTok{(y, z)}
\CommentTok{#> Error: All nested columns must have the same number of elements.}
\end{Highlighting}
\end{Shaded}

The same principle applies when unnesting list-columns of data frames.
You can unnest multiple list-cols as long as all the data frames in each
row have the same number of rows.

\subsection{Exercises}\label{exercises-67}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why might the \texttt{lengths()} function be useful for creating
  atomic vector columns from list-columns?
\item
  List the most common types of vector found in a data frame. What makes
  lists different?
\end{enumerate}

\hypertarget{making-tidy-data-with-broom}{\section{Making tidy data with
broom}\label{making-tidy-data-with-broom}}

The broom package provides three general tools for turning models into
tidy data frames:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{broom::glance(model)} returns a row for each model. Each
  column gives a model summary: either a measure of model quality, or
  complexity, or a combination of the two.
\item
  \texttt{broom::tidy(model)} returns a row for each coefficient in the
  model. Each column gives information about the estimate or its
  variability.
\item
  \texttt{broom::augment(model,\ data)} returns a row for each row in
  \texttt{data}, adding extra values like residuals, and influence
  statistics.
\end{enumerate}

\part{Communicate}\label{part-communicate}


\chapter{Introduction}\label{communicate-intro}

So far, you've learned the tools to get your data into R, tidy it into a
form convenient for analysis, and then understand your data through
transformation, visualisation and modelling. However, it doesn't matter
how great your analysis is unless you can explain it to others: you need
to \textbf{communicate} your results.

\begin{center}\includegraphics[width=0.75\linewidth]{diagrams/data-science-communicate} \end{center}

Communication is the theme of the following four chapters:

\begin{itemize}
\item
  In \protect\hyperlink{r-markdown}{R Markdown}, you will learn about R
  Markdown, a tool for integrating prose, code, and results. You can use
  R Markdown in notebook mode for analyst-to-analyst communication, and
  in report mode for analyst-to-decision-maker communication. Thanks to
  the power of R Markdown formats, you can even use the same document
  for both purposes.
\item
  In \protect\hyperlink{graphics-for-communication}{Graphics for
  communication}, you will learn how to take your exploratory graphics
  and turn them into expository graphics, graphics that help the
  newcomer to your analysis understand what's going on as quickly and
  easily as possible.
\item
  In \protect\hyperlink{r-markdown-formats}{R Markdown formats}, you'll
  learn a little about the many other varieties of outputs you can
  produce using R Markdown, including dashboards, websites, and books.
\item
  We'll finish up with \protect\hyperlink{r-markdown-workflow}{R
  Markdown workflow}, where you'll learn about the ``analysis notebook''
  and how to systematically record your successes and failures so that
  you can learn from them.
\end{itemize}

Unfortunately, these chapters focus mostly on the technical mechanics of
communication, not the really hard problems of communicating your
thoughts to other humans. However, there are lot of other great books
about communication, which we'll point you to at the end of each
chapter.

\hypertarget{r-markdown}{\chapter{R Markdown}\label{r-markdown}}

\section{Introduction}\label{introduction-18}

R Markdown provides an unified authoring framework for data science,
combining your code, its results, and your prose commentary. R Markdown
documents are fully reproducible and support dozens of output formats,
like PDFs, Word files, slideshows, and more.

R Markdown files are designed to be used in three ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For communicating to decision makers, who want to focus on the
  conclusions, not the code behind the analysis.
\item
  For collaborating with other data scientists (including future you!),
  who are interested in both your conclusions, and how you reached them
  (i.e.~the code).
\item
  As an environment in which to \emph{do} data science, as a modern day
  lab notebook where you can capture not only what you did, but also
  what you were thinking.
\end{enumerate}

R Markdown integrates a number of R packages and external tools. This
means that helps is, by-and-large, not available through \texttt{?}.
Instead, as you work through this chapter, and use R Markdown in the
future, keep these resources close to hand:

\begin{itemize}
\item
  R Markdown Cheat Sheet: \emph{Help \textgreater{} Cheatsheets
  \textgreater{} R Markdown Cheat Sheet},
\item
  R Markdown Reference Guide: \emph{Help \textgreater{} Cheatsheets
  \textgreater{} R Markdown Reference Guide}.
\end{itemize}

Both cheatsheets are also available at
\url{http://rstudio.com/cheatsheets}.

\subsection{Prerequisites}\label{prerequisites-18}

You need the \textbf{rmarkdown} package, but you don't need to
explicitly install it or load it, as RStudio automatically does both
when needed.

\section{R Markdown basics}\label{r-markdown-basics}

This is an R Markdown file, a plain text file that has the extension
\texttt{.Rmd}:

\begin{verbatim}
---
title: "Diamond sizes"
date: 2016-08-25
output: html_document
---

```{r setup, include = FALSE}
library(ggplot2)
library(dplyr)

smaller <- diamonds %>% 
  filter(carat <= 2.5)
```

We have data about `r nrow(diamonds)` diamonds. Only 
`r nrow(diamonds) - nrow(smaller)` are larger than
2.5 carats. The distribution of the remainder is shown
below:

```{r, echo = FALSE}
smaller %>% 
  ggplot(aes(carat)) + 
  geom_freqpoly(binwidth = 0.01)
```
\end{verbatim}

It contains three important types of content:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An (optional) \textbf{YAML header} surrounded by \texttt{-\/-\/-}s.
\item
  \textbf{Chunks} of R code surrounded by \texttt{```}.
\item
  Text mixed with simple text formatting like \texttt{\#\ heading} and
  \texttt{\_italics\_}.
\end{enumerate}

When you open an \texttt{.Rmd}, you get a notebook interface where code
and output are interleaved. You can run each code chunk by clicking the
Run icon (it looks like a play button at the top of the chunk), or by
pressing Cmd/Ctrl + Shift + Enter. RStudio executes the code and
displays the results inline with the code:

\begin{center}\includegraphics[width=0.75\linewidth]{rmarkdown/diamond-sizes-notebook} \end{center}

To produce a complete report containing all text, code, and results,
click ``Knit'' or press Cmd/Ctrl + Shift + K. You can also do this
programmatically with \texttt{rmarkdown::render("1-example.Rmd")}. This
will display the report in the viewer pane, and create a self-contained
HTML file that you can share with others.

\begin{center}\includegraphics[width=0.75\linewidth]{rmarkdown/diamond-sizes-report} \end{center}

When you \textbf{knit} the document R Markdown sends the .Rmd file to
\textbf{knitr}, \url{http://yihui.name/knitr/}, which executes all of
the code chunks and creates a new markdown (.md) document which includes
the code and its output. The markdown file generated by knitr is then
processed by \textbf{pandoc}, \url{http://pandoc.org/}, which is
responsible for creating the finished file. The advantage of this two
step workflow is that you can create a very wide range of output
formats, as you'll learn about in
\protect\hyperlink{r-markdown-formats}{R markdown formats}.

\begin{center}\includegraphics[width=0.75\linewidth]{images/RMarkdownFlow} \end{center}

To get started with your own \texttt{.Rmd} file, select \emph{File
\textgreater{} New File \textgreater{} R Markdown\ldots{}} in the
menubar. RStudio will launch a wizard that you can use to pre-populate
your file with useful content that reminds you how the key features of R
Markdown work.

The following sections dive into the three components of an R Markdown
document in more details: the markdown text, the code chunks, and the
YAML header.

\subsection{Exercises}\label{exercises-68}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create a new notebook using \emph{File \textgreater{} New File
  \textgreater{} R Notebook}. Read the instructions. Practice running
  the chunks. Verify that you can modify the code, re-run it, and see
  modified output.
\item
  Create a new R Markdown document with \emph{File \textgreater{} New
  File \textgreater{} R Markdown\ldots{}} Knit it by clicking the
  appropriate button. Knit it by using the appropriate keyboard short
  cut. Verify that you can modify the input and see the output update.
\item
  Compare and contrast the R notebook and R markdown files you created
  above. How are the outputs similar? How are they different? How are
  the inputs similar? How are they different? What happens if you copy
  the YAML header from one to the other?
\item
  Create one new R Markdown document for each of the three built-in
  formats: HTML, PDF and Word. Knit each of the three documents. How
  does the output differ? How does the input differ? (You may need to
  install LaTeX in order to build the PDF output --- RStudio will prompt
  you if this is necessary.)
\end{enumerate}

\section{Text formatting with
Markdown}\label{text-formatting-with-markdown}

Prose in \texttt{.Rmd} files is written in Markdown, a lightweight set
of conventions for formatting plain text files. Markdown is designed to
be easy to read and easy to write. It is also very easy to learn. The
guide below shows how to use Pandoc's Markdown, a slightly extended
version of Markdown that R Markdown understands.

\begin{verbatim}
Text formatting 
------------------------------------------------------------

*italic*  or _italic_
**bold**   __bold__
`code`
superscript^2^ and subscript~2~

Headings
------------------------------------------------------------

# 1st Level Header

## 2nd Level Header

### 3rd Level Header

Lists
------------------------------------------------------------

*   Bulleted list item 1

*   Item 2

    * Item 2a

    * Item 2b

1.  Numbered list item 1

1.  Item 2. The numbers are incremented automatically in the output.

Links and images
------------------------------------------------------------

<http://example.com>

[linked phrase](http://example.com)

![optional caption text](path/to/img.png)

Tables 
------------------------------------------------------------

First Header  | Second Header
------------- | -------------
Content Cell  | Content Cell
Content Cell  | Content Cell
\end{verbatim}

The best way to learn these is simply to try them out. It will take a
few days, but soon they will become second nature, and you won't need to
think about them. If you forget, you can get to a handy reference sheet
with \emph{Help \textgreater{} Markdown Quick Reference}.

\subsection{Exercises}\label{exercises-69}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Practice what you've learned by creating a brief CV. The title should
  be your name, and you should include headings for (at least) education
  or employment. Each of the sections should include a bulleted list of
  jobs/degrees. Highlight the year in bold.
\item
  Using the R Markdown quick reference, figure out how to:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Add a footnote.
  \item
    Add a horizontal rule.
  \item
    Add a block quote.
  \end{enumerate}
\item
  Copy and paste the contents of \texttt{diamond-sizes.Rmd} from
  \url{https://github.com/hadley/r4ds/tree/master/rmarkdown} in to a
  local R markdown document. Check that you can run it, then add text
  after the frequency polygon that describes its most striking features.
\end{enumerate}

\section{Code chunks}\label{code-chunks}

To run code inside an R Markdown document, you need to insert a chunk.
There are three ways to do so:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The keyboard shortcut Cmd/Ctrl + Alt + I
\item
  The ``Insert'' button icon in the editor toolbar.
\item
  By manually typing the chunk delimiters \texttt{```\{r\}} and
  \texttt{```}.
\end{enumerate}

Obviously, I'd recommend you learn the keyboard shortcut. It will save
you a lot of time in the long run!

You can continue to run the code using the keyboard shortcut that by now
(I hope!) you know and love: Cmd/Ctrl + Enter. However, chunks get a new
keyboard shortcut: Cmd/Ctrl + Shift + Enter, which runs all the code in
the chunk. Think of a chunk like a function. A chunk should be
relatively self-contained, and focussed around a single task.

The following sections describe the chunk header which consists of
\texttt{```\{r}, followed by an optional chunk name, followed by comma
separated options, followed by \texttt{\}}. Next comes your R code and
the chunk end is indicated by a final \texttt{```}.

\subsection{Chunk name}\label{chunk-name}

Chunks can be given an optional name: \texttt{```\{r\ by-name\}}. This
has three advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You can more easily navigate to specific chunks using the drop-down
  code navigator in the bottom-left of the script editor:

  \begin{center}\includegraphics[width=0.3\linewidth]{screenshots/rmarkdown-chunk-nav} \end{center}
\item
  Graphics produced by the chunks will have useful names that make them
  easier to use elsewhere. More on that in
  \protect\hyperlink{other-important-options}{other important options}.
\item
  You can set up networks of cached chunks to avoid re-performing
  expensive computations on every run. More on that below.
\end{enumerate}

There is one chunk name that imbues special behaviour: \texttt{setup}.
When you're in a notebook mode, the chunk named setup will be run
automatically once, before any other code is run.

\subsection{Chunk options}\label{chunk-options}

Chunk output can be customised with \textbf{options}, arguments supplied
to chunk header. Knitr provides almost 60 options that you can use to
customize your code chunks. Here we'll cover the most important chunk
options that you'll use frequently. You can see the full list at
\url{http://yihui.name/knitr/options/}.

The most important set of options controls if your code block is
executed and what results are inserted in the finished report:

\begin{itemize}
\item
  \texttt{eval\ =\ FALSE} prevents code from being evaluated. (And
  obviously if the code is not run, no results will be generated). This
  is useful for displaying example code, or for disabling a large block
  of code without commenting each line.
\item
  \texttt{include\ =\ FALSE} runs the code, but doesn't show the code or
  results in the final document. Use this for setup code that you don't
  want cluttering your report.
\item
  \texttt{echo\ =\ FALSE} prevents code, but not the results from
  appearing in the finished file. Use this when writing reports aimed at
  people who don't want to see the underlying R code.
\item
  \texttt{message\ =\ FALSE} or \texttt{warning\ =\ FALSE} prevents
  messages or warnings from appearing in the finished file.
\item
  \texttt{results\ =\ \textquotesingle{}hide\textquotesingle{}} hides
  printed output;
  \texttt{fig.show\ =\ \textquotesingle{}hide\textquotesingle{}} hides
  plots.
\item
  \texttt{error\ =\ TRUE} causes the render to continue even if code
  returns an error. This is rarely something you'll want to include in
  the final version of your report, but can be very useful if you need
  to debug exactly what is going on inside your \texttt{.Rmd}. It's also
  useful if you're teaching R and want to deliberately include an error.
  The default, \texttt{error\ =\ FALSE} causes knitting to fail if there
  is a single error in the document.
\end{itemize}

The following table summarises which types of output each option
supressess:

\begin{longtable}[]{@{}lllllll@{}}
\toprule
Option & Run code & Show code & Output & Plots & Messages &
Warnings\tabularnewline
\midrule
\endhead
\texttt{eval\ =\ FALSE} & - & & - & - & - & -\tabularnewline
\texttt{include\ =\ FALSE} & & - & - & - & - & -\tabularnewline
\texttt{echo\ =\ FALSE} & & - & & & &\tabularnewline
\texttt{results\ =\ "hide"} & & & - & & &\tabularnewline
\texttt{fig.show\ =\ "hide"} & & & & - & &\tabularnewline
\texttt{message\ =\ FALSE} & & & & & - &\tabularnewline
\texttt{warning\ =\ FALSE} & & & & & & -\tabularnewline
\bottomrule
\end{longtable}

\subsection{Table}\label{table}

By default, R Markdown prints data frames and matrices as you'd see them
in the console:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars[}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{, ]}
\CommentTok{#>                    mpg cyl disp  hp drat   wt qsec vs am gear carb}
\CommentTok{#> Mazda RX4         21.0   6  160 110 3.90 2.62 16.5  0  1    4    4}
\CommentTok{#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.88 17.0  0  1    4    4}
\CommentTok{#> Datsun 710        22.8   4  108  93 3.85 2.32 18.6  1  1    4    1}
\CommentTok{#> Hornet 4 Drive    21.4   6  258 110 3.08 3.21 19.4  1  0    3    1}
\CommentTok{#> Hornet Sportabout 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2}
\end{Highlighting}
\end{Shaded}

If you prefer that data be displayed with additional formatting you can
use the \texttt{knitr::kable} function. The code below generates Table
\ref{tab:kable}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr::}\KeywordTok{kable}\NormalTok{(}
  \NormalTok{mtcars[}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{, ], }
  \DataTypeTok{caption =} \StringTok{"A knitr kable."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:kable}A knitr kable.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r|r|r}
\hline
  & mpg & cyl & disp & hp & drat & wt & qsec & vs & am & gear & carb\\
\hline
Mazda RX4 & 21.0 & 6 & 160 & 110 & 3.90 & 2.62 & 16.5 & 0 & 1 & 4 & 4\\
\hline
Mazda RX4 Wag & 21.0 & 6 & 160 & 110 & 3.90 & 2.88 & 17.0 & 0 & 1 & 4 & 4\\
\hline
Datsun 710 & 22.8 & 4 & 108 & 93 & 3.85 & 2.32 & 18.6 & 1 & 1 & 4 & 1\\
\hline
Hornet 4 Drive & 21.4 & 6 & 258 & 110 & 3.08 & 3.21 & 19.4 & 1 & 0 & 3 & 1\\
\hline
Hornet Sportabout & 18.7 & 8 & 360 & 175 & 3.15 & 3.44 & 17.0 & 0 & 0 & 3 & 2\\
\hline
\end{tabular}
\end{table}

Read the documentation for \texttt{?knitr::kable} to see the other ways
in which you can customise the table. For even deeper customisation,
consider the \textbf{xtable}, \textbf{stargazer}, \textbf{pander},
\textbf{tables}, and \textbf{ascii} packages. Each provides a set of
tools for returning formatted tables from R code.

There is also a rich set of options for controlling how figures are
embedded. You'll learn about these in
\protect\hyperlink{saving-your-plots}{saving your plots}.

\subsection{Caching}\label{caching}

Normally, each knit of a document starts from a completely clean slate.
This is great for reproducibility, because it ensures that you've
captured every important computation in code. However, it can be painful
if you have some computations that take a long time. The solution is
\texttt{cache\ =\ TRUE}. When set, this will save the output of the
chunk to a specially named file on disk. On subsequent runs, knitr will
check to see if the code has changed, and if it hasn't, it will reuse
the cached results.

The caching system must be used with care, because by default it is
based on the code only, not its dependencies. For example, here the
\texttt{processed\_data} chunk depends on the \texttt{raw\_data} chunk:

\begin{verbatim}
```{r raw_data}
rawdata <- readr::read_csv("a_very_large_file.csv")
```

```{r processed_data, cache = TRUE}
processed_data <- rawdata %>% 
  filter(!is.na(import_var)) %>% 
  mutate(new_variable = complicated_transformation(x, y, z))
```
\end{verbatim}

Caching the \texttt{processed\_data} chunk means that it will get re-run
if the dplyr pipeline is changed, but it won't get rerun if the
\texttt{read\_csv()} call changes. You can avoid that problem with the
\texttt{dependson} chunk option:

\begin{verbatim}
```{r processed_data, cache = TRUE, dependson = "raw_data"}
processed_data <- rawdata %>% 
  filter(!is.na(import_var)) %>% 
  mutate(new_variable = complicated_transformation(x, y, z))
```
\end{verbatim}

\texttt{dependson} should contain a character vector of \emph{every}
chunk that the cached chunk depends on. Knitr will update the results
for the cached chunk whenever it detects that one of its dependencies
have changed.

Note that the chunks won't update if \texttt{a\_very\_large\_file.csv}
changes, because knitr caching only tracks changes within the
\texttt{.Rmd} file. If you want to also track changes to that file you
can use the \texttt{cache.extra} option. This is an arbitrary R
expression that will invalidate the cache whenever it changes. A good
function to use is \texttt{file.info()}: it returns a bunch of
information about the file including when it was last modified. Then you
can write:

\begin{verbatim}
```{r raw_data, cache.extra = file.info("a_very_large_file.csv")}
rawdata <- readr::read_csv("a_very_large_file.csv")
```
\end{verbatim}

As your caching strategies get progressively more complicated, it's a
good idea to regularly clear out all your caches with
\texttt{knitr::clean\_cache()}.

I've used the advice of
\href{https://twitter.com/drob/status/738786604731490304}{David
Robinson} to name these chunks: each chunk is named after the primary
object that it creates. This makes it easier to understand the
\texttt{dependson} specification.

\subsection{Global options}\label{global-options}

As you work more with knitr, you will discover that some of the default
chunk options don't fit your needs and you want to change them. You can
do this by calling \texttt{knitr::opts\_chunk\$set()} in a code chunk.
For example, when writing books and tutorials I set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr::opts_chunk$}\KeywordTok{set}\NormalTok{(}
  \DataTypeTok{comment =} \StringTok{"#>"}\NormalTok{,}
  \DataTypeTok{collapse =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This uses my preferred comment formatting, and ensures that the code and
output are kept closely entwined. On the other hand, if you were
preparing a report, you might set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr::opts_chunk$}\KeywordTok{set}\NormalTok{(}
  \DataTypeTok{echo =} \OtherTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That will hide the code by default, so only showing the chunks you
deliberately choose to show (with \texttt{echo\ =\ TRUE}). You might
consider setting \texttt{message\ =\ FALSE} and
\texttt{warning\ =\ FALSE}, but that would make it harder to debug
problems because you wouldn't see any messages in the final document.

\subsection{Inline code}\label{inline-code}

There is one other way to embed R code into an R Markdown document:
directly into the text, with: \texttt{`r\ `}. This can be very useful if
you mention properties of your data in the text. For example, in the
example document I used at the start of the chapter I had:

\begin{quote}
We have data about \texttt{`r\ nrow(diamonds)`} diamonds. Only
\texttt{`r\ nrow(diamonds)\ -\ nrow(smaller)`} are larger than 2.5
carats. The distribution of the remainder is shown below:
\end{quote}

When the report is knit, the results of these computations are inserted
into the text:

\begin{quote}
We have data about 53940 diamonds. Only 126 are larger than 2.5 carats.
The distribution of the remainder is shown below:
\end{quote}

When inserting numbers into text, \texttt{format()} is your friend. It
allows you to set the number of \texttt{digits} so you don't print to a
ridiculous degree of accuracy, and a \texttt{big.mark} to make numbers
easier to read. I'll often combine these into a helper function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{comma <-}\StringTok{ }\NormalTok{function(x) }\KeywordTok{format}\NormalTok{(x, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{big.mark =} \StringTok{","}\NormalTok{)}
\KeywordTok{comma}\NormalTok{(}\DecValTok{3452345}\NormalTok{)}
\CommentTok{#> [1] "3,452,345"}
\KeywordTok{comma}\NormalTok{(.}\DecValTok{12358124331}\NormalTok{)}
\CommentTok{#> [1] "0.12"}
\end{Highlighting}
\end{Shaded}

\subsection{Exercises}\label{exercises-70}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Add a section that explores how diamond sizes vary by cut, colour, and
  clarity. Assume you're writing a report for someone who doesn't know
  R, and instead of setting \texttt{echo\ =\ FALSE} on each chunk, set a
  global option.
\item
  Download \texttt{diamond-sizes.Rmd} from
  \url{https://github.com/hadley/r4ds/tree/master/rmarkdown}. Add a
  section that describes the largest 20 diamonds, including a table that
  displays their most important attributes.
\item
  Modify \texttt{diamonds-sizes.Rmd} to use \texttt{comma()} to produce
  nicely formatted output. Also include the percentage of diamonds that
  are larger than 2.5 carats.
\item
  Set up a network of chunks where \texttt{d} depends on \texttt{c} and
  \texttt{b}, and both \texttt{b} and \texttt{c} depend on \texttt{a}.
  Have each chunk print \texttt{lubridate::now()}, set
  \texttt{cache\ =\ TRUE}, then verify your understanding of caching.
\end{enumerate}

\section{Troubleshooting}\label{troubleshooting}

Troubleshooting R Markdown documents can be challenging because you are
no longer in an interactive R environment, and you will need to learn
some new tricks. The first thing you should always try is to recreate
the problem in an interactive session. Restart R, then ``Run all
chunks'' (either from Code menu, under Run region), or with the keyboard
shortcut Ctrl + Alt + R. If you're lucky, that will recreate the
problem, and you can figure out what's going on interactively.

If that doesn't help, there must be something different between your
interactive environment and the R markdown environment. You're going to
need to systematically explore the options. The most common difference
is the working directory: the working directory of an R Markdown is the
directory in which it lives. Check the working directory is what you
expect by including \texttt{getwd()} in a chunk.

Next, brainstorm all the things that might cause the bug. You'll need to
systematically check that they're the same in your R session and your R
markdown session. The easiest way to do that is to set
\texttt{error\ =\ TRUE} on the chunk causing the problem, then use
\texttt{print()} and \texttt{str()} to check that settings are as you
expect.

\section{YAML header}\label{yaml-header}

You can control many other ``whole document'' settings by tweaking the
parameters of the YAML header. You might wonder what YAML stands for:
it's ``yet another markup language'', which is designed for representing
hierarchical data in a way that's easy for humans to read and write. R
Markdown uses it to control many details of the output. Here we'll
discuss two: document parameters and bibliographies.

\subsection{Parameters}\label{parameters}

R Markdown documents can include one or more parameters whose values can
be set when you render the report. Parameters are useful when you want
to re-render the same report with distinct values for various key
inputs. For example, you might be producing sales reports per branch,
exam results by student, or demographic summaries by country. To declare
one or more parameters, use the \texttt{params} field.

This example use a \texttt{my\_class} parameter to determines which
class of cars to display:

\begin{verbatim}
---
output: html_document
params:
  my_class: "suv"
---

```{r setup, include = FALSE}
library(ggplot2)
library(dplyr)

class <- mpg %>% filter(class == params$my_class)
```

# Fuel economy for `r params$my_class`s

```{r, message = FALSE}
ggplot(class, aes(displ, hwy)) + 
  geom_point() + 
  geom_smooth(se = FALSE)
```
\end{verbatim}

As you can see, parameters are available within the code chunks as a
read-only list named \texttt{params}.

You can write atomic vectors directly into the YAML header. You can also
run arbitrary R expressions by prefacing the parameter value with
\texttt{!r}. This is a good way to specify date/time parameters.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{params:}
  \FunctionTok{start:} \NormalTok{!r lubridate::ymd("2015-01-01")}
  \FunctionTok{snapshot:} \NormalTok{!r lubridate::ymd_hms("2015-01-01 12:30:00")}
\end{Highlighting}
\end{Shaded}

In RStudio, you can click the ``Knit with Parameters'' option in the
Knit dropdown menu to set parameters, render, and preview the report in
a single user friendly step. You can customise the dialog by setting
other options in the header. See
\url{http://rmarkdown.rstudio.com/developer_parameterized_reports.html\#parameter_user_interfaces}
for more details.

Alternatively, if you need to produce many such paramterised reports,
you can call \texttt{rmarkdown::render()} with a list of
\texttt{params}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmarkdown::}\KeywordTok{render}\NormalTok{(}\StringTok{"fuel-economy.Rmd"}\NormalTok{, }\DataTypeTok{params =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{my_class =} \StringTok{"suv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This is particularly powerful in conjunction with
\texttt{purrr:pwalk()}. The following example creates a report for each
value of \texttt{class} found in \texttt{mpg}. First we create a data
frame that has one row for each class, giving the \texttt{filename} of
the report and the \texttt{params}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reports <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{class =} \KeywordTok{unique}\NormalTok{(mpg$class),}
  \DataTypeTok{filename =} \NormalTok{stringr::}\KeywordTok{str_c}\NormalTok{(}\StringTok{"fuel-economy-"}\NormalTok{, class, }\StringTok{".html"}\NormalTok{),}
  \DataTypeTok{params =} \NormalTok{purrr::}\KeywordTok{map}\NormalTok{(class, ~}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{my_class =} \NormalTok{.))}
\NormalTok{)}
\NormalTok{reports}
\CommentTok{#> # A tibble: 7 × 3}
\CommentTok{#>     class                  filename     params}
\CommentTok{#>     <chr>                     <chr>     <list>}
\CommentTok{#> 1 compact fuel-economy-compact.html <list [1]>}
\CommentTok{#> 2 midsize fuel-economy-midsize.html <list [1]>}
\CommentTok{#> 3     suv     fuel-economy-suv.html <list [1]>}
\CommentTok{#> 4 2seater fuel-economy-2seater.html <list [1]>}
\CommentTok{#> 5 minivan fuel-economy-minivan.html <list [1]>}
\CommentTok{#> 6  pickup  fuel-economy-pickup.html <list [1]>}
\CommentTok{#> # ... with 1 more rows}
\end{Highlighting}
\end{Shaded}

Then we match the column names to the argument names of
\texttt{render()}, and use purrr's \textbf{parallel} walk to call
\texttt{render()} once for each row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reports %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\DataTypeTok{output_file =} \NormalTok{filename, params) %>%}\StringTok{ }
\StringTok{  }\NormalTok{purrr::}\KeywordTok{pwalk}\NormalTok{(rmarkdown::render, }\DataTypeTok{input =} \StringTok{"fuel-economy.Rmd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Bibliographies and
Citations}\label{bibliographies-and-citations}

Pandoc can automatically generate citations and a bibliography in a
number of styles. To use this feature, specify a bibliography file using
the \texttt{bibliography} field in your file's header. The field should
contain a path from the directory that contains your .Rmd file to the
file that contains the bibliography file:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bibliography:} \NormalTok{rmarkdown.bib}
\end{Highlighting}
\end{Shaded}

You can use many common bibliography formats including BibLaTeX, BibTeX,
endnote, medline.

To create a citation within your .Rmd file, use a key composed of `@' +
the citation identifier from the bibliography file. Then place the
citation in square brackets. Here are some examples:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Separate multiple citations with a }\BaseNTok{`;`}\NormalTok{: Blah blah [@smith04; @doe99].}

\NormalTok{You can add arbitrary comments inside the square brackets: }
\NormalTok{Blah blah [see @doe99, pp. 33-35; also @smith04, ch. 1].}

\NormalTok{Remove the square brackets to create an in-text citation: @smith04 }
\NormalTok{says blah, or @smith04 [p. 33] says blah.}

\NormalTok{Add a }\BaseNTok{`-`} \NormalTok{before the citation to suppress the author's name: }
\NormalTok{Smith says blah [-@smith04].}
\end{Highlighting}
\end{Shaded}

When R Markdown renders your file, it will build and append a
bibliography to the end of your document. The bibliography will contain
each of the cited references from your bibliography file, but it will
not contain a section heading. As a result it is common practice to end
your file with a section header for the bibliography, such as
\texttt{\#\ References} or \texttt{\#\ Bibliography}.

You can change the style of your citations and bibliography by
referencing a CSL (citation style language) file in the \texttt{csl}
field:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bibliography:} \NormalTok{rmarkdown.bib}
\FunctionTok{csl:} \NormalTok{apa.csl}
\end{Highlighting}
\end{Shaded}

As with the bibliography field, your csl file should contain a path to
the file. Here I assume that the csl file is in the same directory as
the .Rmd file. A good place to find CSL style files for common
bibliography styles is
\url{http://github.com/citation-style-language/styles}.

\section{Learning more}\label{learning-more-2}

R Markdown is still relatively young, and is still growing rapidly. The
best place to stay on top of innovations is the official R Markdown
website: \url{http://rmarkdown.rstudio.com}.

There are two important topics that we haven't covered here:
collaboration, and the details of accurately communicating your ideas to
other humans. Collaboration is a vital part of modern data science, and
you can make your life much easier by using version control tools, like
Git and GitHub. We recommend two free resources that will teach you
about Git:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  ``Happy Git with R'': a user friendly introduction to Git and GitHub
  from R users, by Jenny Bryan. The book is freely available online:
  \url{http://happygitwithr.com}
\item
  The ``Git and GitHub'' chapter of \emph{R Packages}, by Hadley. You
  can also read it for free online:
  \url{http://r-pkgs.had.co.nz/git.html}.
\end{enumerate}

I have also not touched on what you should actually write in order to
clearly communicate the results of your analysis. To improve your
writing, I highly recommend reading either
\href{https://amzn.com/0134080416}{\emph{Style: Lessons in Clarity and
Grace}} by Joseph M. Williams \& Joseph Bizup, or
\href{https://amzn.com/0205296327}{\emph{The Sense of Structure: Writing
from the Reader's Perspective}} by George Gopen. Both books will help
you understand the structure of sentences and paragraphs, and give you
the tools to make your writing more clear. (These books are rather
expensive if purchased new, but they're used by many English classes so
there are plenty of cheap second-hand copies). George Gopen also has a
number of short articles on writing at
\url{http://georgegopen.com/articles/litigation/}. They are aimed at
lawyers, but almost everything applies to data scientists too.

\hypertarget{graphics-for-communication}{\chapter{Graphics for
communication}\label{graphics-for-communication}}

\section{Introduction}\label{introduction-19}

In \protect\hyperlink{exploratory-data-analysis}{exploratory data
analysis}, you learned how to use plots as tools for \emph{exploration}.
When you make exploratory plots, you know---even before looking---which
variables the plot will display. You made each plot for a purpose, could
quickly look at it, and then move on to the next plot. In the course of
most analyses, you'll produce tens or hundreds of plots, most of which
are immediately thrown away.

Now that you understand your data, you need to \emph{communicate} your
understanding to others. Your audience will likely not share your
background knowledge and will not be deeply invested in the data. To
help others quickly build up a good mental model of the data, you will
need to invest considerable effort in making your plots as
self-explanatory as possible. In this chapter, you'll learn some of the
tools that ggplot2 provides to do so.

This chapter focuses on the tools you need to create good graphics. I
assume that you know what you want, and just need to know how to do it.
For that reason, I highly recommend pairing this chapter with a good
general visualisation book. I particularly like
\href{https://amzn.com/0321934075}{\emph{The Truthful Art}}, by Albert
Cairo. It doesn't teach the mechanics of creating visualisations, but
instead focuses on what you need to think about in order to create
effective graphics.

\subsection{Prerequisites}\label{prerequisites-19}

In this chapter, we'll focus once again on ggplot2. We'll also use a
little dplyr for data manipulation, and a few ggplot2 extension
packages, including \textbf{ggrepel} and \textbf{viridis}. Rather than
loading those extensions here, we'll refer to their functions
explicitly, using the \texttt{::} notation. This will help make it clear
which functions are built into ggplot2, and which come from other
packages. Don't forget you'll need to install those packages with
\texttt{install.packages()} if you don't already have them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{Label}\label{label}

The easiest place to start when turning an exploratory graphic into an
expository graphic is with good labels. You add labels with the
\texttt{labs()} function. This example adds a plot title:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) +}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Fuel efficiency generally decreases with engine size"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-3-1} \end{center}

The purpose of a plot title is to summarise the main finding. Avoid
titles that just describe what the plot is, e.g. ``A scatterplot of
engine displacement vs.~fuel economy''.

If you need to add more text, there are two other useful labels that you
can use in ggplot2 2.2.0 and above (which should be available by the
time you're reading this book):

\begin{itemize}
\item
  \texttt{subtitle} adds additional detail in a smaller font beneath the
  title.
\item
  \texttt{caption} adds text at the bottom right of the plot, often used
  to describe the source of the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) +}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{title =} \StringTok{"Fuel efficiency generally decreases with engine size"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Two seaters (sports cars) are an exception because of their light weight"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"Data from fueleconomy.gov"}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-4-1} \end{center}

You can also use \texttt{labs()} to replace the axis and legend titles.
It's usually a good idea to replace short variable names with more
detailed descriptions, and to include the units.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) +}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Engine displacement (L)"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Highway fuel economy (mpg)"}\NormalTok{,}
    \DataTypeTok{colour =} \StringTok{"Car type"}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-5-1} \end{center}

It's possible to use mathematical equations instead of text strings.
Just switch \texttt{""} out for \texttt{quote()} and read about the
available options in \texttt{?plotmath}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{y =} \KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(df, }\KeywordTok{aes}\NormalTok{(x, y)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \KeywordTok{quote}\NormalTok{(}\KeywordTok{sum}\NormalTok{(x[i] ^}\StringTok{ }\DecValTok{2}\NormalTok{, i ==}\StringTok{ }\DecValTok{1}\NormalTok{, n)),}
    \DataTypeTok{y =} \KeywordTok{quote}\NormalTok{(alpha +}\StringTok{ }\NormalTok{beta +}\StringTok{ }\KeywordTok{frac}\NormalTok{(delta, theta))}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-6-1} \end{center}

\subsection{Exercises}\label{exercises-71}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create one plot on the fuel economy data with customised
  \texttt{title}, \texttt{subtitle}, \texttt{caption}, \texttt{x},
  \texttt{y}, and \texttt{colour} labels.
\item
  The \texttt{geom\_smooth()} is somewhat misleading because the
  \texttt{hwy} for large engines is skewed upwards due to the inclusion
  of lightweight sports cars with big engines. Use your modelling tools
  to fit and display a better model.
\item
  Take an exploratory graphic that you've created in the last month, and
  add informative titles to make it easier for others to understand.
\end{enumerate}

\section{Annotations}\label{annotations}

In addition to labelling major components of your plot, it's often
useful to label individual observations or groups of observations. The
first tool you have at your disposal is \texttt{geom\_text()}.
\texttt{geom\_text()} is similar to \texttt{geom\_point()}, but it has
an additional aesthetic: \texttt{label}. This makes it possible to add
textual labels to your plots.

There are two possible sources of labels. First, you might have a tibble
that provides labels. The plot below isn't terribly useful, but it
illustrates a useful approach: pull out the most efficient car in each
class with dplyr, and then label it on the plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best_in_class <-}\StringTok{ }\NormalTok{mpg %>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(class) %>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{row_number}\NormalTok{(}\KeywordTok{desc}\NormalTok{(hwy)) ==}\StringTok{ }\DecValTok{1}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =} \NormalTok{model), }\DataTypeTok{data =} \NormalTok{best_in_class)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-7-1} \end{center}

This is hard to read because the labels overlap with each other, and
with the points. We can make things a little better by switching to
\texttt{geom\_label()} which draws a rectangle behind the text. We also
use the \texttt{nudge\_y} parameter to move the labels slightly above
the corresponding points:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_label}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =} \NormalTok{model), }\DataTypeTok{data =} \NormalTok{best_in_class, }\DataTypeTok{nudge_y =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-8-1} \end{center}

That helps a bit, but if you look closely in the top-left hand corner,
you'll notice that there are two labels practically on top of each
other. This happens because the highway mileage and displacement for the
best cars in the compact and subcompact categories are exactly the same.
There's no way that we can fix these by applying the same transformation
for every label. Instead, we can use the \textbf{ggrepel} package by
Kamil Slowikowski. This useful package will automatically adjust labels
so that they don't overlap:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{3}\NormalTok{, }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{, }\DataTypeTok{data =} \NormalTok{best_in_class) +}
\StringTok{  }\NormalTok{ggrepel::}\KeywordTok{geom_label_repel}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =} \NormalTok{model), }\DataTypeTok{data =} \NormalTok{best_in_class)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-9-1} \end{center}

Note another handy technique used here: I added a second layer of large,
hollow points to highlight the points that I've labelled.

You can sometimes use the same idea to replace the legend with labels
placed directly on the plot. It's not wonderful for this plot, but it
isn't too bad. (\texttt{theme(legend.position\ =\ "none"}) turns the
legend off --- we'll talk about it more shortly.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{class_avg <-}\StringTok{ }\NormalTok{mpg %>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(class) %>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{displ =} \KeywordTok{median}\NormalTok{(displ),}
    \DataTypeTok{hwy =} \KeywordTok{median}\NormalTok{(hwy)}
  \NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy, }\DataTypeTok{colour =} \NormalTok{class)) +}
\StringTok{  }\NormalTok{ggrepel::}\KeywordTok{geom_label_repel}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =} \NormalTok{class),}
    \DataTypeTok{data =} \NormalTok{class_avg,}
    \DataTypeTok{size =} \DecValTok{6}\NormalTok{,}
    \DataTypeTok{label.size =} \DecValTok{0}\NormalTok{,}
    \DataTypeTok{segment.color =} \OtherTok{NA}
  \NormalTok{) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-10-1} \end{center}

Alternatively, you might just want to add a single label to the plot,
but you'll still need to create a data frame. Often, you want the label
in the corner of the plot, so it's convenient to create a new data frame
using \texttt{summarise()} to compute the maximum values of x and y.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{label <-}\StringTok{ }\NormalTok{mpg %>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{displ =} \KeywordTok{max}\NormalTok{(displ),}
    \DataTypeTok{hwy =} \KeywordTok{max}\NormalTok{(hwy),}
    \DataTypeTok{label =} \StringTok{"Increasing engine size is }\CharTok{\textbackslash{}n}\StringTok{related to decreasing fuel economy."}
  \NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =} \NormalTok{label), }\DataTypeTok{data =} \NormalTok{label, }\DataTypeTok{vjust =} \StringTok{"top"}\NormalTok{, }\DataTypeTok{hjust =} \StringTok{"right"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-11-1} \end{center}

If you want to place the text exactly on the borders of the plot, you
can use \texttt{+Inf} and \texttt{-Inf}. Since we're no longer computing
the positions from \texttt{mpg}, we can use \texttt{tibble()} to create
the data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{label <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{displ =} \OtherTok{Inf}\NormalTok{,}
  \DataTypeTok{hwy =} \OtherTok{Inf}\NormalTok{,}
  \DataTypeTok{label =} \StringTok{"Increasing engine size is }\CharTok{\textbackslash{}n}\StringTok{related to decreasing fuel economy."}
\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =} \NormalTok{label), }\DataTypeTok{data =} \NormalTok{label, }\DataTypeTok{vjust =} \StringTok{"top"}\NormalTok{, }\DataTypeTok{hjust =} \StringTok{"right"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-12-1} \end{center}

In these examples, I manually broke the label up into lines using
\texttt{"\textbackslash{}n"}. Another approach is to use
\texttt{stringr::str\_wrap()} to automatically add line breaks, given
the number of characters you want per line:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"Increasing engine size is related to decreasing fuel economy."} \NormalTok{%>%}
\StringTok{  }\NormalTok{stringr::}\KeywordTok{str_wrap}\NormalTok{(}\DataTypeTok{width =} \DecValTok{40}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{writeLines}\NormalTok{()}
\CommentTok{#> Increasing engine size is related to}
\CommentTok{#> decreasing fuel economy.}
\end{Highlighting}
\end{Shaded}

Note the use of \texttt{hjust} and \texttt{vjust} to control the
alignment of the label. Figure \ref{fig:just} shows all nine possible
combinations.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/just-1} 

}

\caption{All nine combinations of `hjust` and `vjust`.}\label{fig:just}
\end{figure}

Remember, in addition to \texttt{geom\_text()}, you have many other
geoms in ggplot2 available to help annotate your plot. A few ideas:

\begin{itemize}
\item
  Use \texttt{geom\_hline()} and \texttt{geom\_vline()} to add reference
  lines. I often make them thick (\texttt{size\ =\ 2}) and white
  (\texttt{colour\ =\ white}), and draw them underneath the primary data
  layer. That makes them easy to see, without drawing attention away
  from the data.
\item
  Use \texttt{geom\_rect()} to draw a rectangle around points of
  interest. The boundaries of the rectangle are defined by aesthetics
  \texttt{xmin}, \texttt{xmax}, \texttt{ymin}, \texttt{ymax}.
\item
  Use \texttt{geom\_segment()} with the \texttt{arrow} argument to draw
  attention to a point with an arrow. Use aesthetics \texttt{x} and
  \texttt{y} to define the starting location, and \texttt{xend} and
  \texttt{yend} to define the end location.
\end{itemize}

The only limit is your imagination (and your patience with positioning
annotations to be aesthetically pleasing)!

\subsection{Exercises}\label{exercises-72}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use \texttt{geom\_text()} with infinite positions to place text at the
  four corners of the plot.
\item
  Read the documentation for \texttt{annotate()}. How can you use it to
  add a text label to a plot without having to create a tibble?
\item
  How do labels with \texttt{geom\_text()} interact with faceting? How
  can you add a label to a single facet? How can you put a different
  label in each facet? (Hint: think about the underlying data.)
\item
  What arguments to \texttt{geom\_label()} control the appearance of the
  background box?
\item
  What are the four arguments to \texttt{arrow()}? How do they work?
  Create a series of plots that demonstrate the most important options.
\end{enumerate}

\section{Scales}\label{scales}

The third way you can make your plot better for communication is to
adjust the scales. Scales control the mapping from data values to things
that you can perceive. Normally, ggplot2 automatically adds scales for
you. For example, when you type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class))}
\end{Highlighting}
\end{Shaded}

ggplot2 automatically adds default scales behind the scenes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_colour_discrete}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Note the naming scheme for scales: \texttt{scale\_} followed by the name
of the aesthetic, then \texttt{\_}, then the name of the scale. The
default scales are named according to the type of variable they align
with: continuous, discrete, datetime, or date. There are lots of
non-default scales which you'll learn about below.

The default scales have been carefully chosen to do a good job for a
wide range of inputs. Nevertheless, you might want to override the
defaults for two reasons:

\begin{itemize}
\item
  You might want to tweak some of the parameters of the default scale.
  This allows you to do things like change the breaks on the axes, or
  the key labels on the legend.
\item
  You might want to replace the scale altogether, and use a completely
  different algorithm. Often you can do better than the default because
  you know more about the data.
\end{itemize}

\subsection{Axis ticks and legend
keys}\label{axis-ticks-and-legend-keys}

There are two primary arguments that affect the appearance of the ticks
on the axes and the keys on the legend: \texttt{breaks} and
\texttt{labels}. Breaks controls the position of the ticks, or the
values associated with the keys. Labels controls the text label
associated with each tick/key. The most common use of \texttt{breaks} is
to override the default choice:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DataTypeTok{by =} \DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-15-1} \end{center}

You can use \texttt{labels} in the same way (a character vector the same
length as \texttt{breaks}), but you can also set it to \texttt{NULL} to
suppress the labels altogether. This is useful for maps, or for
publishing plots where you can't share the absolute numbers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{labels =} \OtherTok{NULL}\NormalTok{) +}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =} \OtherTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-16-1} \end{center}

You can also use \texttt{breaks} and \texttt{labels} to control the
appearance of legends. Collectively axes and legends are called
\textbf{guides}. Axes are used for x and y aesthetics; legends are used
for everything else.

Another use of \texttt{breaks} is when you have relatively few data
points and want to highlight exactly where the observations occur. For
example, take this plot that shows when each US president started and
ended their term.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{presidential %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{id =} \DecValTok{33} \NormalTok{+}\StringTok{ }\KeywordTok{row_number}\NormalTok{()) %>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(start, id)) +}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{    }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =} \NormalTok{end, }\DataTypeTok{yend =} \NormalTok{id)) +}
\StringTok{    }\KeywordTok{scale_x_date}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \NormalTok{presidential$start, }\DataTypeTok{date_labels =} \StringTok{"'%y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-17-1} \end{center}

Note that the specification of breaks and labels for date and datetime
scales is a little different:

\begin{itemize}
\item
  \texttt{date\_labels} takes a format specification, in the same form
  as \texttt{parse\_datetime()}.
\item
  \texttt{date\_breaks} (not shown here), takes a string like ``2 days''
  or ``1 month''.
\end{itemize}

\subsection{Legend layout}\label{legend-layout}

You will most often use \texttt{breaks} and \texttt{labels} to tweak the
axes. While they both also work for legends, there are a few other
techniques you are more likely to use.

To control the overall position of the legend, you need to use a
\texttt{theme()} setting. We'll come back to themes at the end of the
chapter, but in brief, they control the non-data parts of the plot. The
theme setting \texttt{legend.position} controls where the legend is
drawn:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class))}

\NormalTok{base +}\StringTok{ }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"left"}\NormalTok{)}
\NormalTok{base +}\StringTok{ }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{)}
\NormalTok{base +}\StringTok{ }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\NormalTok{base +}\StringTok{ }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"right"}\NormalTok{) }\CommentTok{# the default}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-18-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-18-2}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-18-3}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-18-4}

You can also use \texttt{legend.position\ =\ "none"} to suppress the
display of the legend altogether.

To control the display of individual legends, use \texttt{guides()}
along with \texttt{guide\_legend()} or \texttt{guide\_colourbar()}. The
following example shows two important settings: controlling the number
of rows the legend uses with \texttt{nrow}, and overriding one of the
aesthetics to make the points bigger. This is particularly useful if you
have used a low \texttt{alpha} to display many points on a plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) +}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) +}
\StringTok{  }\KeywordTok{guides}\NormalTok{(}\DataTypeTok{colour =} \KeywordTok{guide_legend}\NormalTok{(}\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{, }\DataTypeTok{override.aes =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{size =} \DecValTok{4}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-19-1} \end{center}

\subsection{Replacing a scale}\label{replacing-a-scale}

Instead of just tweaking the details a little, you can instead replace
the scale altogether. There are two types of scales you're mostly likely
to want to switch out: continuous position scales and colour scales.
Fortunately, the same principles apply to all the other aesthetics, so
once you've mastered position and colour, you'll be able to quickly pick
up other scale replacements.

It's very useful to plot transformations of your variable. For example,
as we've seen in \href{diamond-prices}{diamond prices} it's easier to
see the precise relationship between \texttt{carat} and \texttt{price}
if we log transform them:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(carat, price)) +}
\StringTok{  }\KeywordTok{geom_bin2d}\NormalTok{()}

\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(}\KeywordTok{log10}\NormalTok{(carat), }\KeywordTok{log10}\NormalTok{(price))) +}
\StringTok{  }\KeywordTok{geom_bin2d}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-20-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-20-2}

However, the disadvantage of this transformation is that the axes are
now labelled with the transformed values, making it hard to interpret
the plot. Instead of doing the transformation in the aesthetic mapping,
we can instead do it with the scale. This is visually identical, except
the axes are labelled on the original data scale.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(carat, price)) +}
\StringTok{  }\KeywordTok{geom_bin2d}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_log10}\NormalTok{() +}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-21-1} \end{center}

Another scale that is frequently customised is colour.The default
categorical scale picks colours that are evenly spaced around the colour
wheel. Useful alternatives are the ColorBrewer scales which have been
hand tuned to work better for people with common types of colour
blindness. The two plots below look similar, but there is enough
difference in the shades of red and green that the dots on the right can
be distinguished even by people with red-green colour blindness.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{drv))}

\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{drv)) +}
\StringTok{  }\KeywordTok{scale_colour_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-22-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-22-2}

Don't forget simpler techniques. If there are just a few colours, you
can add a redundant shape mapping. This will also help ensure your plot
is interpretable in black and white.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{drv, }\DataTypeTok{shape =} \NormalTok{drv)) +}
\StringTok{  }\KeywordTok{scale_colour_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-23-1} \end{center}

The ColorBrewer scales are documented online at
\url{http://colorbrewer2.org/} and made available in R via the
\textbf{RColorBrewer} package, by Erich Neuwirth. Figure
\ref{fig:brewer} shows the complete list of all palettes. The sequential
(top) and diverging (bottom) palettes are particularly useful if your
categorical values are ordered, or have a ``middle''. This often arises
if you've used \texttt{cut()} to make a continuous variable into a
categorical variable.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/brewer-1} 

}

\caption{All ColourBrewer scales.}\label{fig:brewer}
\end{figure}

When you have a predefined mapping between values and colours, use
\texttt{scale\_colour\_manual()}. For example, if we map presidential
party to colour, we want to use the standard mapping of red for
Republicans and blue for Democrats:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{presidential %>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{id =} \DecValTok{33} \NormalTok{+}\StringTok{ }\KeywordTok{row_number}\NormalTok{()) %>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(start, id, }\DataTypeTok{colour =} \NormalTok{party)) +}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{    }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =} \NormalTok{end, }\DataTypeTok{yend =} \NormalTok{id)) +}
\StringTok{    }\KeywordTok{scale_colour_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\DataTypeTok{Republican =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{Democratic =} \StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-24-1} \end{center}

For continuous colour, you can use the built-in
\texttt{scale\_colour\_gradient()} or \texttt{scale\_fill\_gradient()}.
If you have a diverging scale, you can use
\texttt{scale\_colour\_gradient2()}. That allows you to give, for
example, positive and negative values different colours. That's
sometimes also useful if you want to distinguish points above or below
the mean.

Another option is \texttt{scale\_colour\_viridis()} provided by the
\textbf{viridis} package. It's a continuous analog of the categorical
ColorBrewer scales. The designers, Nathaniel Smith and Stéfan van der
Walt, carefully tailored a continuous colour scheme that has good
perceptual properties. Here's an example from the viridis vignette.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{),}
  \DataTypeTok{y =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{)}
\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(df, }\KeywordTok{aes}\NormalTok{(x, y)) +}
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{() +}
\StringTok{  }\KeywordTok{coord_fixed}\NormalTok{()}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}

\KeywordTok{ggplot}\NormalTok{(df, }\KeywordTok{aes}\NormalTok{(x, y)) +}
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{() +}
\StringTok{  }\NormalTok{viridis::}\KeywordTok{scale_fill_viridis}\NormalTok{() +}
\StringTok{  }\KeywordTok{coord_fixed}\NormalTok{()}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-25-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-25-2}

Note that all colour scales come in two variety:
\texttt{scale\_colour\_x()} and \texttt{scale\_fill\_x()} for the
\texttt{colour} and \texttt{fill} aesthetics respectively (the colour
scales are available in both UK and US spellings).

\subsection{Exercises}\label{exercises-73}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why doesn't the following code override the default scale?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(df, }\KeywordTok{aes}\NormalTok{(x, y)) +}
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{() +}
\StringTok{  }\KeywordTok{scale_colour_gradient}\NormalTok{(}\DataTypeTok{low =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{) +}
\StringTok{  }\KeywordTok{coord_fixed}\NormalTok{()}
\CommentTok{#> Warning: Computation failed in `stat_binhex()`:}
\CommentTok{#> Package `hexbin` required for `stat_binhex`.}
\CommentTok{#> Please install and try again.}
\end{Highlighting}
\end{Shaded}
\item
  What is the first argument to every scale? How does it compare to
  \texttt{labs()}?
\item
  Change the display of the presidential terms by:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Combining the two variants shown above.
  \item
    Improving the display of the y axis.
  \item
    Labelling each term with the name of the president.
  \item
    Adding informative plot labels.
  \item
    Placing breaks every 4 years (this is trickier than it seems!).
  \end{enumerate}
\item
  Use \texttt{override.aes} to make the legend on the following plot
  easier to see.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds, }\KeywordTok{aes}\NormalTok{(carat, price)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =} \NormalTok{cut), }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{/}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{center}\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-27-1} \end{center}
\end{enumerate}

\section{Zooming}\label{zooming}

There are three ways to control the plot limits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adjusting what data are plotted
\item
  Setting the limits in each scale
\item
  Setting \texttt{xlim} and \texttt{ylim} in \texttt{coord\_cartesian()}
\end{enumerate}

To zoom in on a region of the plot, it's generally best to use
\texttt{coord\_cartesian()}. Compare the following two plots:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{() +}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{), }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{30}\NormalTok{))}

\NormalTok{mpg %>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(displ >=}\StringTok{ }\DecValTok{5}\NormalTok{, displ <=}\StringTok{ }\DecValTok{7}\NormalTok{, hwy >=}\StringTok{ }\DecValTok{10}\NormalTok{, hwy <=}\StringTok{ }\DecValTok{30}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-28-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-28-2}

You can also set the \texttt{limits} on individual scales. Reducing the
limits is basically equivalent to subsetting the data. It is generally
more useful if you want \emph{expand} the limits, for example, to match
scales across different plots. For example, if we extract two classes of
cars and plot them separately, it's difficult to compare the plots
because all three scales (the x-axis, the y-axis, and the colour
aesthetic) have different ranges.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{suv <-}\StringTok{ }\NormalTok{mpg %>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(class ==}\StringTok{ "suv"}\NormalTok{)}
\NormalTok{compact <-}\StringTok{ }\NormalTok{mpg %>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(class ==}\StringTok{ "compact"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(suv, }\KeywordTok{aes}\NormalTok{(displ, hwy, }\DataTypeTok{colour =} \NormalTok{drv)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}

\KeywordTok{ggplot}\NormalTok{(compact, }\KeywordTok{aes}\NormalTok{(displ, hwy, }\DataTypeTok{colour =} \NormalTok{drv)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-29-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-29-2}

One way to overcome this problem is to share scales across multiple
plots, training the scales with the \texttt{limits} of the full data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x_scale <-}\StringTok{ }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{range}\NormalTok{(mpg$displ))}
\NormalTok{y_scale <-}\StringTok{ }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{range}\NormalTok{(mpg$hwy))}
\NormalTok{col_scale <-}\StringTok{ }\KeywordTok{scale_colour_discrete}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{unique}\NormalTok{(mpg$drv))}

\KeywordTok{ggplot}\NormalTok{(suv, }\KeywordTok{aes}\NormalTok{(displ, hwy, }\DataTypeTok{colour =} \NormalTok{drv)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\NormalTok{x_scale +}
\StringTok{  }\NormalTok{y_scale +}
\StringTok{  }\NormalTok{col_scale}

\KeywordTok{ggplot}\NormalTok{(compact, }\KeywordTok{aes}\NormalTok{(displ, hwy, }\DataTypeTok{colour =} \NormalTok{drv)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}
\StringTok{  }\NormalTok{x_scale +}
\StringTok{  }\NormalTok{y_scale +}
\StringTok{  }\NormalTok{col_scale}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-30-1}
\includegraphics[width=0.5\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-30-2}

In this particular case, you could have simply used faceting, but this
technique is useful more generally, if for instance, you want spread
plots over multiple pages of a report.

\section{Themes}\label{themes}

Finally, you can customise the non-data elements of your plot with a
theme:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \NormalTok{class)) +}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) +}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-31-1} \end{center}

ggplot2 includes eight themes by default, as shown in Figure
\ref{fig:themes}. Many more are included in add-on packages like
\textbf{ggthemes} (\url{https://github.com/jrnold/ggthemes}), by Jeffrey
Arnold.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/visualization-themes} 

}

\caption{The eight themes built-in to ggplot2.}\label{fig:themes}
\end{figure}

Many people wonder why the default theme has a grey background. This was
a deliberate choice because it puts the data forward while still making
the grid lines visible. The white grid lines are visible (which is
important because they significantly aid position judgements), but they
have little visual impact and we can easily tune them out. The grey
background gives the plot a similar typographic colour to the text,
ensuring that the graphics fit in with the flow of a document without
jumping out with a bright white background. Finally, the grey background
creates a continuous field of colour which ensures that the plot is
perceived as a single visual entity.

It's also possible to control individual components of each theme, like
the size and colour of the font used for the y axis. Unfortunately, this
level of detail is outside the scope of this book, so you'll need to
read the \href{https://amzn.com/331924275X}{ggplot2 book} for the full
details. You can also create your own themes, if you are trying to match
a particular corporate or journal style.

\hypertarget{saving-your-plots}{\section{Saving your
plots}\label{saving-your-plots}}

There are two main ways to get your plots out of R and into your final
write-up: \texttt{ggsave()} and knitr. \texttt{ggsave()} will save the
most recent plot to disk:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(mpg, }\KeywordTok{aes}\NormalTok{(displ, hwy)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggsave}\NormalTok{(}\StringTok{"my-plot.pdf"}\NormalTok{)}
\CommentTok{#> Saving 6 x 3.71 in image}
\end{Highlighting}
\end{Shaded}

If you don't specify the \texttt{width} and \texttt{height} they will be
taken from the dimensions of the current plotting device. For
reproducible code, you'll want to specify them.

Generally, however, I think you should be assembling your final reports
using R Markdown, so I want to focus on the important code chunk options
that you should know about for graphics. You can learn more about
\texttt{ggsave()} in the documentation.

\subsection{Figure sizing}\label{figure-sizing}

The biggest challenge of graphics in R Markdown is getting your figures
the right size and shape. There are five main options that control
figure sizing: \texttt{fig.width}, \texttt{fig.height},
\texttt{fig.asp}, \texttt{out.width} and \texttt{out.height}. Image
sizing is challenging because there are two sizes (the size of the
figure created by R and the size at which it is inserted in the output
document), and multiple ways of specifying the size (i.e., height,
width, and aspect ratio: pick two of three).

I only ever use three of the five options:

\begin{itemize}
\item
  I find it most aesthetically pleasing for plots to have a consistent
  width. To enforce this, I set \texttt{fig.width\ =\ 6} (6``) and
  \texttt{fig.asp\ =\ 0.618} (the golden ratio) in the defaults. Then in
  individual chunks, I only adjust \texttt{fig.asp}.
\item
  I control the output size with \texttt{out.width} and set it to a
  percentage of the line width). I default to
  \texttt{out.width\ =\ "70\%"} and \texttt{fig.align\ =\ "center"}.
  That give plots room to breathe, without taking up too much space.
\item
  To put multiple plots in a single row I set the \texttt{out.width} to
  \texttt{50\%} for two plots, \texttt{33\%} for 3 plots, or
  \texttt{25\%} to 4 plots, and set \texttt{fig.align\ =\ "default"}.
  Depending on what I'm trying to illustrate (e.g. show data or show
  plot variations), I'll also tweak \texttt{fig.width}, as discussed
  below.
\end{itemize}

If you find that you're having to squint to read the text in your plot,
you need to tweak \texttt{fig.width}. If \texttt{fig.width} is larger
than the size the figure is rendered in the final doc, the text will be
too small; if \texttt{fig.width} is smaller, the text will be too big.
You'll often need to do a little experimentation to figure out the right
ratio between the \texttt{fig.width} and the eventual width in your
document. To illustrate the principle, the following three plots have
\texttt{fig.width} of 4, 6, and 8 respectively:

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-35-1} \end{center}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-36-1} \end{center}

\begin{center}\includegraphics[width=0.7\linewidth]{_bookdown_files/communicate-plots_files/figure-latex/unnamed-chunk-37-1} \end{center}

If you want to make sure the font size is consistent across all your
figures, whenever you set \texttt{out.width}, you'll also need to adjust
\texttt{fig.width} to maintain the same ratio with your default
\texttt{out.width}. For example, if your default \texttt{fig.width} is 6
and \texttt{out.width} is 0.7, when you set
\texttt{out.width\ =\ "50\%"} you'll need to set \texttt{fig.width} to
4.3 (6 * 0.5 / 0.7).

\hypertarget{other-important-options}{\subsection{Other important
options}\label{other-important-options}}

When mingling code and text, like I do in this book, I recommend setting
\texttt{fig.show\ =\ "hold"} so that plots are shown after the code.
This has the pleasant side effect of forcing you to break up large
blocks of code with their explanations.

To add a caption to the plot, use \texttt{fig.cap}. In R Markdown this
will change the figure from inline to ``floating''.

If you're producing PDF output, the default graphics type is PDF. This
is a good default because PDFs are high quality vector graphics.
However, they can produce very large and slow plots if you are
displaying thousands of points. In that case, set \texttt{dev\ =\ "png"}
to force the use of PNGs. They are slightly lower quality, but will be
much more compact.

It's a good idea to name code chunks that produce figures, even if you
don't routinely label other chunks. The chunk label is used to generate
the file name of the graphic on disk, so naming your chunks makes it
much easier to pick out plots and reuse in other circumstances (i.e.~if
you want to quickly drop a single plot into an email or a tweet).

\section{Learning more}\label{learning-more-3}

The absolute best place to learn more is the ggplot2 book:
\href{https://amzn.com/331924275X}{\emph{ggplot2: Elegant graphics for
data analysis}}. It goes into much more depth about the underlying
theory, and has many more examples of how to combine the individual
pieces to solve practical problems. Unfortunately, the book is not
available online for free, although you can find the source code at
\url{https://github.com/hadley/ggplot2-book}.

Another great resource is the ggplot2 extensions guide
\url{http://www.ggplot2-exts.org/}. This site lists many of the packages
that extend ggplot2 with new geoms and scales. It's a great place to
start if you're trying to do something that seems hard with ggplot2.

\hypertarget{r-markdown-formats}{\chapter{R Markdown
formats}\label{r-markdown-formats}}

\section{Introduction}\label{introduction-20}

So far you've seen R Markdown used to produce HTML documents. This
chapter gives a brief overview of some of the many other types of output
you can produce with R Markdown. There are two ways to set the output of
a document:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Permanently, by modifying the YAML header:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{title:} \StringTok{"Viridis Demo"}
\FunctionTok{output:} \NormalTok{html_document}
\end{Highlighting}
\end{Shaded}
\item
  Transiently, by calling \texttt{rmarkdown::render()} by hand:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmarkdown::}\KeywordTok{render}\NormalTok{(}\StringTok{"diamond-sizes.Rmd"}\NormalTok{, }\DataTypeTok{output_format =} \StringTok{"word_document"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  This is useful if you want to programmatically produce multiple types
  of output.
\end{enumerate}

RStudio's knit button renders a file to the first format listed in its
\texttt{output} field. You can render to additional formats by clicking
the dropdown menu beside the knit button.

\begin{center}\includegraphics{screenshots/rmarkdown-knit} \end{center}

\section{Output options}\label{output-options}

Each output format is associated with an R function. You can either
write \texttt{foo} or \texttt{pkg::foo}. If you omit, \texttt{pkg} the
default is assumed to be rmarkdown. It's important to know the name of
the function that makes the output because that's where you get help.
For example, to figure out what parameters you can set with
\texttt{html\_document}, look at \texttt{?rmarkdown:html\_document()}

To override the default parameter values, you need to use an expanded
\texttt{output} field. For example, if you wanted to render an
\texttt{html\_document} with a floating table of contents, you'd use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{output:}
  \FunctionTok{html_document:}
    \FunctionTok{toc:} \NormalTok{true}
    \FunctionTok{toc_float:} \NormalTok{true}
\end{Highlighting}
\end{Shaded}

You can even render to multiple outputs by supplying a list of formats:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{output:}
  \FunctionTok{html_document:}
    \FunctionTok{toc:} \NormalTok{true}
    \FunctionTok{toc_float:} \NormalTok{true}
  \FunctionTok{pdf_document:} \NormalTok{default}
\end{Highlighting}
\end{Shaded}

Note the special syntax if you don't want to override any of the default
options.

\section{Documents}\label{documents}

The previous chapter focussed on the default \texttt{html\_document}
output. There are number of basic variations on that theme, generating
different types of documents:

\begin{itemize}
\item
  \texttt{pdf\_document} makes a PDF with LaTeX (an open source document
  layout system), which you'll need to install. RStudio will prompt you
  if you don't already have it.
\item
  \texttt{word\_document} for Microsoft Word documents (\texttt{.docx}).
\item
  \texttt{odt\_document} for OpenDocument Text documents
  (\texttt{.odt}).
\item
  \texttt{rtf\_document} for Rich Text Format (\texttt{.rtf}) documents.
\item
  \texttt{md\_document} for a Markdown document. This isn't typically
  useful by itself, but you might use it if, for example, your corporate
  CMS or lab wiki uses markdown.
\item
  \texttt{github\_document}: this is a tailored version of
  \texttt{md\_document} designed for sharing on GitHub.
\end{itemize}

Remember, when generating a document to share with decision makers, you
can turn off the default display of code by setting global options in
the setup chunk:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr::opts_chunk$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For \texttt{html\_document}s another option is to make the code chunks
hidden by default, but visible with a click:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{output:}
  \FunctionTok{html_document:}
    \FunctionTok{code_folding:} \NormalTok{hide}
\end{Highlighting}
\end{Shaded}

\section{Notebooks}\label{notebooks}

A notebook, \texttt{html\_notebook}, is a variation on a
\texttt{html\_document}. The rendered outputs are very similar, but the
purpose is different. A \texttt{html\_document} is focussed on
communicating with decision makers, while a notebook is focussed on
collaborating with other data scientists. These different purposes lead
to using the HTML output in different ways. Both HTML outputs will
contain the fully rendered output, but the notebook also contains the
full source code. That means you can use the \texttt{.nb.html} generated
by the notebook in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You can view it in a web browser, and see the rendered output. Unlike
  \texttt{html\_document}, this rendering always includes an embedded
  copy of the source code that generated it.
\item
  You can edit it in RStudio. When you open an \texttt{.nb.html} file,
  RStudio will automatically recreate \texttt{.Rmd} file that generated
  it. In the future, you will also be able include supporting files
  (e.g. \texttt{.csv} data files), which will be automatically extracted
  when needed.
\end{enumerate}

Emailing \texttt{.nb.html} files is a simple way to share analyses with
your colleagues. But things will get painful as soon as they want to
make changes. If this starts to happen, it's a good time to learn Git
and GitHub. Learning Git and GitHub is definitely painful at first, but
the collaboration payoff is huge. As mentioned earlier, Git and GitHub
are outside the scope of the book, but there's one tip that's useful if
you're already using them: use both \texttt{html\_notebook} and
\texttt{github\_document} outputs:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{output:}
  \FunctionTok{html_notebook:} \NormalTok{default}
  \FunctionTok{github_document:} \NormalTok{default}
\end{Highlighting}
\end{Shaded}

\texttt{html\_notebook} gives you a local preview, and a file that you
can share via email. \texttt{github\_document} creates a minimal md file
that you can check into git. You can easily see how the results of your
analysis (not just the code) change over time, and GitHub will render it
for you nicely online.

\section{Presentations}\label{presentations}

You can also use R Markdown to produce presentations. You get less
visual control than with a tool like Keynote or PowerPoint, but
automatically inserting the results of your R code into a presentation
can save a huge amount of time. Presentations work by dividing your
content into slides, with a new slide beginning at each first
(\texttt{\#}) or second (\texttt{\#\#}) level header. You can also
insert a horizontal rule (\texttt{***}) to create a new slide without a
header.

R Markdown comes with three presentations formats built-in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{ioslides\_presentation} - HTML presentation with ioslides
\item
  \texttt{slidy\_presentation} - HTML presentation with W3C Slidy
\item
  \texttt{beamer\_presentation} - PDF presentation with LaTeX Beamer.
\end{enumerate}

Two other popular formats are provided by packages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{revealjs::revealjs\_presentation} - HTML presentation with
  reveal.js. Requires the \textbf{revealjs} package.
\item
  \textbf{rmdshower}, \url{https://github.com/MangoTheCat/rmdshower},
  provides a wrapper around the \textbf{shower},
  \url{https://github.com/shower/shower}, presentation engine
\end{enumerate}

\section{Dashboards}\label{dashboards}

Dashboards are a useful way to communicate large amounts of information
visually and quickly. Flexdashboard makes it particularly easy to create
dashboards using R Markdown and a convention for how the headers affect
the layout:

\begin{itemize}
\tightlist
\item
  Each level 1 header (\texttt{\#}) begins a new page in the dashboard.
\item
  Each level 2 header (\texttt{\#\#}) begins a new column.
\item
  Each level 3 header (\texttt{\#\#\#}) begins a new row.
\end{itemize}

For example, you can produce this dashboard:

\begin{center}\includegraphics[width=0.75\linewidth]{screenshots/rmarkdown-flexdashboard} \end{center}

Using this code:

\begin{verbatim}
---
title: "Diamonds distribution dashboard"
output: flexdashboard::flex_dashboard
---

```{r setup, include = FALSE}
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(fig.width = 5, fig.asp = 1/3)
```

## Column 1

### Carat

```{r}
ggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.1)
```

### Cut

```{r}
ggplot(diamonds, aes(cut)) + geom_bar()
```

### Colour

```{r}
ggplot(diamonds, aes(color)) + geom_bar()
```

## Column 2

### The largest diamonds

```{r}
diamonds %>% 
  arrange(desc(carat)) %>% 
  head(100) %>% 
  select(carat, cut, color, price) %>% 
  DT::datatable()
```
\end{verbatim}

Flexdashboard also provides simple tools for creating sidebars, tabsets,
value boxes, and gauges. To learn more about flexdashboard visit
\url{http://rmarkdown.rstudio.com/flexdashboard/}.

\section{Interactivity}\label{interactivity}

Any HTML format (document, notebook, presentation, or dashboard) can
contain interactive components.

\subsection{htmlwidgets}\label{htmlwidgets}

HTML is an interactive format, and you can take advantage of that
interactivity with \textbf{htmlwidgets}, R functions that produce
interactive HTML visualisations. For example, take the \textbf{leaflet}
map below. If you're viewing this page on the web, you can drag the map
around, zoom in and out, etc. You obviously can't do that on a book, so
rmarkdown automatically inserts a static screenshot for you.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(leaflet)}
\KeywordTok{leaflet}\NormalTok{() %>%}
\StringTok{  }\KeywordTok{setView}\NormalTok{(}\FloatTok{174.764}\NormalTok{, -}\FloatTok{36.877}\NormalTok{, }\DataTypeTok{zoom =} \DecValTok{16}\NormalTok{) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{addTiles}\NormalTok{() %>%}
\StringTok{  }\KeywordTok{addMarkers}\NormalTok{(}\FloatTok{174.764}\NormalTok{, -}\FloatTok{36.877}\NormalTok{, }\DataTypeTok{popup =} \StringTok{"Maungawhau"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{center\}\includegraphics[width=0.7\linewidth]{_bookdown_files/rmarkdown-formats_files/figure-latex/unnamed-chunk-7-1}

The great thing about htmlwidgets is that you don't need to know
anything about HTML or JavaScript to use them. All the details are
wrapped inside the package, so you don't need to worry about it.

There are many packages that provide htmlwidgets, including:

\begin{itemize}
\item
  \textbf{dygraphs}, \url{http://rstudio.github.io/dygraphs/}, for
  interactive time series visualisations.
\item
  \textbf{DT}, \url{http://rstudio.github.io/DT/}, for interactive
  tables.
\item
  \textbf{threejs}, \url{https://github.com/bwlewis/rthreejs} for
  interactive 3d plots.
\item
  \textbf{DiagrammeR}, \url{http://rich-iannone.github.io/DiagrammeR/}
  for diagrams (like flow charts and simple node-link diagrams).
\end{itemize}

To learn more about htmlwidgets and see a more complete list of packages
that provide them visit \url{http://www.htmlwidgets.org/}.

\subsection{Shiny}\label{shiny}

htmlwidgets provide \textbf{client-side} interactivity --- all the
interactivity happens in the browser, independently of R. On one hand,
that's great because you can distribute the HTML file without any
connection to R. However, that fundamentally limits what you can do to
things that have been implemented in HTML and JavaScript. An alternative
approach is to use \textbf{shiny}, a package that allows you to create
interactivity using R code, not JavaScript.

To call Shiny code from an R Markdown document, add
\texttt{runtime:\ shiny} to the header:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{title:} \StringTok{"Shiny Web App"}
\FunctionTok{output:} \NormalTok{html_document}
\FunctionTok{runtime:} \NormalTok{shiny}
\end{Highlighting}
\end{Shaded}

Then you can use the ``input'' functions to add interactive components
to the document:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(shiny)}

\KeywordTok{textInput}\NormalTok{(}\StringTok{"name"}\NormalTok{, }\StringTok{"What is your name?"}\NormalTok{)}
\KeywordTok{numericInput}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"How old are you?"}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{150}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=9.92in]{screenshots/rmarkdown-shiny} \end{center}

You can then refer to the values with \texttt{input\$name} and
\texttt{input\$age}, and the code that uses them will be automatically
re-run whenever they change.

I can't show you a live shiny app here because shiny interactions occur
on the \textbf{server-side}. This means you can write interactive apps
without knowing JavaScript, but it means that you need a server to run
it on. This introduces a logistical issue: Shiny apps need a Shiny
server to be run online. When you run shiny apps on your own computer,
shiny automatically sets up a shiny server for you, but you need a
public facing shiny server if you want to publish this sort of
interactivity online. That's the fundamental trade-off of shiny: you can
do anything in a shiny document that you can do in R, but it that
requires someone to be running R.

Learn more about Shiny at \url{http://shiny.rstudio.com/}.

\section{Websites}\label{websites}

With a little additional infrastructure you can use R Markdown to
generate a complete website:

\begin{itemize}
\item
  Put your \texttt{.Rmd} files in a single directory. \texttt{index.Rmd}
  will become the home page.
\item
  Add a YAML file named \texttt{\_site.yml} provides the navigation for
  the site. For example:

\begin{verbatim}
name: "my-website"
navbar:
  title: "My Website"
  left:
    - text: "Home"
      href: index.html
    - text: "Viridis Colors"
      href: 1-example.html
    - text: "Terrain Colors"
      href: 3-inline.html
\end{verbatim}
\end{itemize}

Execute \texttt{rmarkdown::render\_site()} to build \texttt{\_site}, a
directory of files ready to deploy as a standalone static website, or if
you use an RStudio Project for your website directory. RStudio will add
a Build tab to the IDE that you can use to build and preview your site.

Read more at \url{http://rmarkdown.rstudio.com/rmarkdown_websites.html}.

\section{Other formats}\label{other-formats}

Other packages provide even more output formats:

\begin{itemize}
\item
  The \textbf{bookdown} package,
  \url{https://github.com/rstudio/bookdown}, makes it easy to write
  books, like this one. To learn more, read
  \href{https://bookdown.org/yihui/bookdown/}{\emph{Authoring Books with
  R Markdown}}, by Yihui Xie, which is, of course, written in bookdown.
  Visit \url{http://www.bookdown.org} to see other bookdown books
  written by the wider R community.
\item
  The \textbf{prettydoc} package,
  \url{https://github.com/yixuan/prettydoc/}, provides lightweight
  document formats with a range of attractive themes.
\item
  The \textbf{rticles} package,
  \url{https://github.com/rstudio/rticles}, compiles a selection of
  formats tailored for specific scientific journals.
\end{itemize}

See \url{http://rmarkdown.rstudio.com/formats.html} for a list of even
more formats. You can also create your own by following the instructions
at \url{http://rmarkdown.rstudio.com/developer_custom_formats.html}.

\section{Learning more}\label{learning-more-4}

To learn more about effective communication in these different formats I
recommend the following resources:

\begin{itemize}
\item
  To improve your presentation skills, I recommend
  \href{https://amzn.com/0321820800}{\emph{Presentation Patterns}}, by
  Neal Ford, Matthew McCollough, and Nathaniel Schutta. It provides a
  set of effective patterns (both low- and high-level) that you can
  apply to improve your presentations.
\item
  If you give academic talks, I recommend reading the
  \href{https://github.com/jtleek/talkguide}{\emph{Leek group guide to
  giving talks}}.
\item
  I haven't taken it myself, but I've heard good things about Matt
  McGarrity's online course on public speaking:
  \url{https://www.coursera.org/learn/public-speaking}.
\item
  If you are creating a lot of dashboards, make sure to read Stephen
  Few's \href{https://amzn.com/0596100167}{Information Dashboard Design:
  The Effective Visual Communication of Data}. It will help you create
  dashboards that are truly useful, not just pretty to look at.
\item
  Effectively communicating your ideas often benefits from some
  knowledge of graphic design.
  \href{http://amzn.com/0133966151}{\emph{The Non-Designer's Design
  Book}} is a great place to start.
\end{itemize}

\hypertarget{r-markdown-workflow}{\chapter{R Markdown
workflow}\label{r-markdown-workflow}}

Earlier, we discussed a basic workflow for capturing your R code where
you work interactively in the \emph{console}, then capture what works in
the \emph{script editor}. R Markdown brings together the console and the
script editor, blurring the lines between interactive exploration and
long-term code capture. You can rapidly iterate within a chunk, editing
and re-executing with Cmd/Ctrl + Shift + Enter. When you're happy, you
move on and start a new chunk.

R Markdown is also important because it so tightly integrates prose and
code. This makes it a great \textbf{analysis notebook} because it lets
you develop code and record your thoughts. An analysis notebook shares
many of the same goals as a classic lab notebook in the physical
sciences. It:

\begin{itemize}
\item
  Records what you did and why you did it. Regardless of how great your
  memory is, if you don't record what you do, there will come a time
  when you have forgotten important details. Write them down so you
  don't forget!
\item
  Supports rigorous thinking. You are more likely to come up with a
  strong analysis if you record your thoughts as you go, and continue to
  reflect on them. This also saves you time when you eventually write up
  your analysis to share with others.
\item
  Helps others understand your work. It is rare to do data analysis by
  yourself, and you'll often be working as part of a team. A lab
  notebook helps you share not only what you've done, but why you did it
  with your colleagues or lab mates.
\end{itemize}

Much of the good advice about using lab notebooks effectively can also
be translated to analysis notebooks. I've drawn on my own experiences
and Colin Purrington's advice on lab notebooks
(\url{http://colinpurrington.com/tips/lab-notebooks}) to come up with
the following tips:

\begin{itemize}
\item
  Ensure each notebook has a descriptive title, an evocative filename,
  and a first paragraph that briefly describes the aims of the analysis.
\item
  Use the YAML header date field to record the date you started working
  on the notebook:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{date:} \NormalTok{2016-08-23}
\end{Highlighting}
\end{Shaded}

  Use ISO8601 YYYY-MM-DD format so that's there no ambiguity. Use it
  even if you don't normally write dates that way!
\item
  If you spend a lot of time on an analysis idea and it turns out to be
  a dead end, don't delete it! Write up a brief note about why it failed
  and leave it in the notebook. That will help you avoid going down the
  same dead end when you come back to the analysis in the future.
\item
  Generally, you're better off doing data entry outside of R. But if you
  do need to record a small snippet of data, clearly lay it out using
  \texttt{tibble::tribble()}.
\item
  If you discover an error in a data file, never modify it directly, but
  instead write code to correct the value. Explain why you made the fix.
\item
  Before you finish for the day, make sure you can knit the notebook (if
  you're using caching, make sure to clear the caches). That will let
  you fix any problems while the code is still fresh in your mind.
\item
  If you want your code to be reproducible in the long-run (i.e.~so you
  can come back to run it next month or next year), you'll need to track
  the versions of the packages that your code uses. A rigorous approach
  is to use \textbf{packrat}, \url{http://rstudio.github.io/packrat/},
  which store packages in your project directory, or
  \textbf{checkpoint},
  \url{https://github.com/RevolutionAnalytics/checkpoint}, which will
  reinstall packages available on a specified date. A quick and dirty
  hack is to include a chunk that runs \texttt{sessionInfo()} --- that
  won't you let easily recreate your packages as they are today, but at
  least you'll know what they were.
\item
  You are going to create many, many, many analysis notebooks over the
  course of your career. How are you going to organise them so you can
  find them again in the future? I recommend storing them in individual
  projects, and coming up with a good naming scheme.
\end{itemize}


\end{document}
